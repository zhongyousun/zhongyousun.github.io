[
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/aspnetcore-blazor-lazy-loading/",
	  "title": "Image Lazy Loading in ASP.NET Core Blazor",
	  "summary": "\u003cp\u003eDocument the method of implementing lazy loading of images in the Blazor framework.\u003c/p\u003e",
	  "content": "Document the method of implementing lazy loading of images in the Blazor framework.\nWhy should to use Lazy Loading in website? Basically, the biggest reason is performance issues. If a page has 10 pictures, the user cannot actually see all the pictures at once in the operation window (screen), so if the page is loaded immediately, In order to load these 10 or more complete pictures, not only does the user have to wait for them all to be loaded, which is very time-consuming, but even worse is that the user feel stuck and don\u0026rsquo;t know how to do then choose close the website. If the pictures are placed on cloud services such as S3, the accumulated data transmission costs over a long period of time will also be considerable!!\nImplementation method There are actually many ways to implement the lazy loading of pictures. The simplest is of course to limit the number of pictures loaded. Only a few pictures are loaded at a time. If you want to view more, you can slide down or click the load more button. However, this method is not enough well. After all, if the number of images loaded each time is too small, it will be very troublesome for users. If it is too many, it will not achieve the desired effect of increasing performance and reducing transmission volume.\nThe browser supports lazy loading, which can be achieved through this syntax„ÄÇ\n\u0026lt;img src=\u0026#34;image.jpg\u0026#34; loading=\u0026#34;lazy\u0026#34;\u0026gt; But in the Blazor, it may be because all object screens have an extra layer rendered by JS. I am not sure about the actual principle, but it is invalid after testing.\nThe method I currently choose is to use IntersectionObserver (an API implemented by JavaScript that can monitor the status of elements in the window, that is, whether the element is within the window) to determine whether to load the image.\nCode Copy this code into the body of index.html so that every page of the website can use the lazy loading function.\n1\u0026lt;!-- lzay loading --\u0026gt; 2\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/intersection-observer@0.12.2/intersection-observer.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 3\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; 4 window.lazyLoading = function () { 5 const observer = new IntersectionObserver((entries, owner) =\u0026gt; { 6 for (let entry of entries) { 7 if (entry.isIntersecting) { 8 // Picture enters the screen and then loads the real photo 9 const img = entry.target; 10 img.setAttribute(\u0026#39;src\u0026#39;, img.dataset.src); 11 img.removeAttribute(\u0026#39;data-src\u0026#39;); 12 owner.unobserve(img); 13 } 14 } 15 }); 16 const images = document.querySelectorAll(\u0026#39;img.lazy\u0026#39;) 17 for (let image of images) { 18 observer.observe(image); 19 } 20 } 21\u0026lt;/script\u0026gt; The main purpose is to use IntersectionObserver to determine whether the image has entered the window. If it has entered, the original src will be removed and the content of data-src will be displayed.\nWebsite Page that require lazy loading of images (.razor):\n@inject IJSRuntime js \u0026lt;img src=\u0026#34;@placeholderImage\u0026#34; data-src=\u0026#34;@o.ImageUrl\u0026#34; class=\u0026#34;lazy\u0026#34;\u0026gt; src is the display screen when the image is not loaded, which can be a low-resolution pixel image.\ndata-src is the actual image file URL to be displayed.\nThe part of @code is:\nprotected override void OnAfterRender(bool firstRender) { if (firstRender) { js.InvokeVoidAsync(\u0026#34;lazyLoading\u0026#34;); } } Used to load IntersectionObserver on first load.\nConclusion The actual browsing effect is as follows: You can see that when the image enters the window, the actual image will be loaded from the original default low-resolution pixel image to achieve a delayed loading effect.\n",
	  "pubDate": "2023-12-25T07:44:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/049en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/aspnetcore-blazor-wasm-prerender/",
	  "title": "Blazor WASM SEO - Use pre-rendering to solve the problem that SPA web SEO (Prerender.io \u0026 Cloudflare Workers tutorial)",
	  "summary": "\u003cp\u003eDocument how to use Prerender.io and Cloudflare Workers to solve the problem of SPA websites not being able to crawl content in search engines.\u003c/p\u003e",
	  "content": "Document how to use Prerender.io and Cloudflare Workers to solve the problem of SPA websites not being able to crawl content in search engines.\nBlazor WASM Preparatory work (can be skipped when using front-end frameworks such as Vue and React) Basically, in order for search engines to obtain data correctly, Open Graph must be added to the \u0026lt;head\u0026gt; of each page.\nFor detailed information, please refer to The Open Graph protocol, The implementation method of Blazor WASM is provided directly here. For easier access, the entire Open Graph can be made into an object.\nAdd a Razor component in Client \u0026gt; Shared with the following code (can be changed according to needs):\n1\u0026lt;PageTitle\u0026gt;@title\u0026lt;/PageTitle\u0026gt; 2\u0026lt;HeadContent\u0026gt; 3 \u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;@title\u0026#34;\u0026gt; 4 \u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34;\u0026gt; 5 \u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;@url\u0026#34;\u0026gt; 6 \u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;@description\u0026#34;\u0026gt; 7 \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;@image\u0026#34;\u0026gt; 8 \u0026lt;meta property=\u0026#34;og:image:alt\u0026#34; content=\u0026#34;Shop\u0026#34;\u0026gt; 9 10 \u0026lt;meta name=\u0026#34;twitter:card\u0026#34; content=\u0026#34;summary_large_image\u0026#34;\u0026gt; 11 \u0026lt;meta name=\u0026#34;twitter:site\u0026#34; content=\u0026#34;@your twitter\u0026#34;\u0026gt; 12 \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;alvin\u0026#34;\u0026gt; 13 14 \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;@description\u0026#34;\u0026gt; 15 \u0026lt;meta name=\u0026#34;locale\u0026#34; content=\u0026#34;zh-tw\u0026#34;\u0026gt; 16 \u0026lt;meta name=\u0026#34;scope\u0026#34; content=\u0026#34;Accessories\u0026#34;\u0026gt; 17 18\u0026lt;/HeadContent\u0026gt; 19@code { 20 [Parameter] public string title { get; set; } = string.Empty; 21 [Parameter] public string url { get; set; } = string.Empty; 22 [Parameter] public string description { get; set; } = string.Empty; 23 [Parameter] public string image { get; set; } = string.Empty; 24} After the construction is completed, just add this object to the page you want to be searched (the parameters change according to different pages).\n\u0026lt;C_SEO title=\u0026#34;@pageTitle\u0026#34; description=\u0026#34;@product.Description\u0026#34; image=\u0026#34;@product.ImageUrl\u0026#34; url=\u0026#34;@NavigationManager.Uri\u0026#34; /\u0026gt; After viewing the source code on the page, you can see the customized Open Graph in \u0026lt;head\u0026gt;. I originally thought this would solve the problem, but if I search through a browser or share a link through social media, I find that the crawler cannot find the custom Open Graph!?\nProblem SPA (Single Page Application) websites built using front-end frameworks such as Vue and React. Compared with traditional static websites, it uses JavaScript to dynamically render content. This allows users to not need to re-enter the website every time. Load the entire page, but only update the required information and graphics when the page switches. This mechanism not only improves performance, but also allows users to interact more fluidly between pages, as page transitions are faster and smoother. Single-page applications effectively optimize the user experience by dynamically updating content on the client side.\nHowever, this method cannot display the actual content of the website because the crawler does not wait for the dynamically rendered content to appear when the browser is searching. If you use Blazor WASM to build a website, no matter what screen you see on the search engine, you will see the content of index.html. as follows:\n1\u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt;Loading...\u0026lt;/div\u0026gt; 2 3\u0026lt;div id=\u0026#34;blazor-error-ui\u0026#34;\u0026gt; 4 An unhandled error has occurred. 5 \u0026lt;a href=\u0026#34;\u0026#34; class=\u0026#34;reload\u0026#34;\u0026gt;Reload\u0026lt;/a\u0026gt; 6 \u0026lt;a class=\u0026#34;dismiss\u0026#34;\u0026gt;üóô\u0026lt;/a\u0026gt; 7\u0026lt;/div\u0026gt; Solution To solve this problem so that the crawler can find the correct information, you have to use pre-rendering technology, which mainly stores the information of each screen in advance and displays it directly when the crawler robot crawls the website.\nPre-rendering can use the service provided by Prerender.io to cache the actual content of the website in advance, and then use Middleware to determine whether the person reading the website is a general user or a crawler robot. Middleware can builted by Cloudflare Workers.\nSystem diagram:\nPrerender Solution Architecture\rCloudflare Workers Cloudflare Workers is a service provided by Cloudflare that allows developers to execute and deploy JavaScript or WebAssembly code on Cloudflare\u0026rsquo;s globally distributed edge network.\nAnd we can use Cloudflare Workers to implement a Middleware to determine whether the request sent is a crawler robot or a general user.\nIf you want to use Cloudflare Workers, you must first let Cloudflare host your website domain name. The current project originally used Godaddy to purchase the domain name. You need to go to Cloudflare to add a new website first (Choose the free plan). If your website already has a DNS record in Godaddy, that is It will be automatically added. After the addition is successful, you can get the customized name server URL, and then go to the name server of Godaddy DNS to modify it.\nDeploy Prerender worker at Cloudflare Click on the left function bar (Workers \u0026amp; Pages) from the home page.\nCreate a Worker and publish it. The first time you build the Worker, it will provide an example of Hello world. You can create it directly first, and then can modify the content inside after.\nNext, you can directly copy and paste the code that Prerender.io provides Cloudflare as Middleware, and click Save and Deploy to complete. (Prerender.io selects Cloudflare in the connection method to view this code)\n1// User agents handled by Prerender 2const BOT_AGENTS = [ 3 \u0026#34;googlebot\u0026#34;, 4 \u0026#34;yahoo! slurp\u0026#34;, 5 \u0026#34;bingbot\u0026#34;, 6 \u0026#34;yandex\u0026#34;, 7 \u0026#34;baiduspider\u0026#34;, 8 \u0026#34;facebookexternalhit\u0026#34;, 9 \u0026#34;twitterbot\u0026#34;, 10 \u0026#34;rogerbot\u0026#34;, 11 \u0026#34;linkedinbot\u0026#34;, 12 \u0026#34;embedly\u0026#34;, 13 \u0026#34;quora link preview\u0026#34;, 14 \u0026#34;showyoubot\u0026#34;, 15 \u0026#34;outbrain\u0026#34;, 16 \u0026#34;pinterest/0.\u0026#34;, 17 \u0026#34;developers.google.com/+/web/snippet\u0026#34;, 18 \u0026#34;slackbot\u0026#34;, 19 \u0026#34;vkshare\u0026#34;, 20 \u0026#34;w3c_validator\u0026#34;, 21 \u0026#34;redditbot\u0026#34;, 22 \u0026#34;applebot\u0026#34;, 23 \u0026#34;whatsapp\u0026#34;, 24 \u0026#34;flipboard\u0026#34;, 25 \u0026#34;tumblr\u0026#34;, 26 \u0026#34;bitlybot\u0026#34;, 27 \u0026#34;skypeuripreview\u0026#34;, 28 \u0026#34;nuzzel\u0026#34;, 29 \u0026#34;discordbot\u0026#34;, 30 \u0026#34;google page speed\u0026#34;, 31 \u0026#34;qwantify\u0026#34;, 32 \u0026#34;pinterestbot\u0026#34;, 33 \u0026#34;bitrix link preview\u0026#34;, 34 \u0026#34;xing-contenttabreceiver\u0026#34;, 35 \u0026#34;chrome-lighthouse\u0026#34;, 36 \u0026#34;telegrambot\u0026#34;, 37 \u0026#34;integration-test\u0026#34;, // Integration testing 38 \u0026#34;google-inspectiontool\u0026#34; 39]; 40 41// These are the extensions that the worker will skip prerendering 42// even if any other conditions pass. 43const IGNORE_EXTENSIONS = [ 44 \u0026#34;.js\u0026#34;, 45 \u0026#34;.css\u0026#34;, 46 \u0026#34;.xml\u0026#34;, 47 \u0026#34;.less\u0026#34;, 48 \u0026#34;.png\u0026#34;, 49 \u0026#34;.jpg\u0026#34;, 50 \u0026#34;.jpeg\u0026#34;, 51 \u0026#34;.gif\u0026#34;, 52 \u0026#34;.pdf\u0026#34;, 53 \u0026#34;.doc\u0026#34;, 54 \u0026#34;.txt\u0026#34;, 55 \u0026#34;.ico\u0026#34;, 56 \u0026#34;.rss\u0026#34;, 57 \u0026#34;.zip\u0026#34;, 58 \u0026#34;.mp3\u0026#34;, 59 \u0026#34;.rar\u0026#34;, 60 \u0026#34;.exe\u0026#34;, 61 \u0026#34;.wmv\u0026#34;, 62 \u0026#34;.doc\u0026#34;, 63 \u0026#34;.avi\u0026#34;, 64 \u0026#34;.ppt\u0026#34;, 65 \u0026#34;.mpg\u0026#34;, 66 \u0026#34;.mpeg\u0026#34;, 67 \u0026#34;.tif\u0026#34;, 68 \u0026#34;.wav\u0026#34;, 69 \u0026#34;.mov\u0026#34;, 70 \u0026#34;.psd\u0026#34;, 71 \u0026#34;.ai\u0026#34;, 72 \u0026#34;.xls\u0026#34;, 73 \u0026#34;.mp4\u0026#34;, 74 \u0026#34;.m4a\u0026#34;, 75 \u0026#34;.swf\u0026#34;, 76 \u0026#34;.dat\u0026#34;, 77 \u0026#34;.dmg\u0026#34;, 78 \u0026#34;.iso\u0026#34;, 79 \u0026#34;.flv\u0026#34;, 80 \u0026#34;.m4v\u0026#34;, 81 \u0026#34;.torrent\u0026#34;, 82 \u0026#34;.woff\u0026#34;, 83 \u0026#34;.ttf\u0026#34;, 84 \u0026#34;.svg\u0026#34;, 85 \u0026#34;.webmanifest\u0026#34;, 86]; 87 88export default { 89 /** 90 * Hooks into the request, and changes origin if needed 91 */ 92 async fetch(request, env) { 93 return await handleRequest(request, env).catch( 94 (err) =\u0026gt; new Response(err.stack, { status: 500 }) 95 ); 96 }, 97}; 98 99/** 100 * @param {Request} request 101 * @param {any} env 102 * @returns {Promise\u0026lt;Response\u0026gt;} 103 */ 104async function handleRequest(request, env) { 105 const url = new URL(request.url); 106 const userAgent = request.headers.get(\u0026#34;User-Agent\u0026#34;)?.toLowerCase() || \u0026#34;\u0026#34;; 107 const isPrerender = request.headers.get(\u0026#34;X-Prerender\u0026#34;); 108 const pathName = url.pathname.toLowerCase(); 109 const extension = pathName 110 .substring(pathName.lastIndexOf(\u0026#34;.\u0026#34;) || pathName.length) 111 ?.toLowerCase(); 112 113 // Prerender loop protection 114 // Non robot user agent 115 // Ignore extensions 116 if ( 117 isPrerender || 118 !BOT_AGENTS.some((bot) =\u0026gt; userAgent.includes(bot)) || 119 (extension.length \u0026amp;\u0026amp; IGNORE_EXTENSIONS.includes(extension)) 120 ) { 121 return fetch(request); 122 } 123 124 // Build Prerender request 125 const newURL = `https://service.prerender.io/${request.url}`; 126 const newHeaders = new Headers(request.headers); 127 128 newHeaders.set(\u0026#34;X-Prerender-Token\u0026#34;, env.PRERENDER_TOKEN); 129 130 return fetch(new Request(newURL, { 131 headers: newHeaders, 132 redirect: \u0026#34;manual\u0026#34;, 133 })); 134} The code is mainly a service that handles HTTP requests. Use the User-Agent and file extensions to determine whether the Prerender service should be used.\nNext, return to the preview screen of this worker, click Triggers, click Add route below, and fill in the route of the website such as *your-domain.com/* and which website the Zone belongs to, and finally Click Add route to save. Triggers\rThe next step is to add the Prerender Token to the environment variables. Click Settings \u0026gt; Variables \u0026gt; Add Variable. And copy and paste the Token provided by Prerender.io. Prerender Token\rAdd a Variable\r!!The variable name must be PRERENDER_TOKEN.\nAfter pressing save, the Prerender worker will run smoothly.\nAfter completing the above steps, you can upload the Sitemap to Prerender.io for crawlers to crawl and then cache.\nConclusion The current free limit of Prerender.io is 1,000 pre-renderings per month, which is enough for small or infrequently changed websites. It is also relatively easy to use this method to solve the problems encountered in SPA web SEO. , after all, there is no need to modify the original project code. However, after this experience, the next development project will have to consider the selected framework and method based on whether there is a need for SEO. After all, if there are too many projects and the service starts charging, it will lead to unaffordable costüò∂.\nReference\nBlazor WebAssembly Client Side SEO Pre-renderingÔºå\nDeploy your Prerender worker on CloudflareÔºå\nCloudflare WorkersÔºå\nPrerender.ioÂ¶Ç‰ΩïÂπ´Âä©CSRÁöÑSEO - ÂéüÁêÜ‰ªãÁ¥πËàá‰ΩøÁî®ÊïôÂ≠∏Ôºå\nBlazor WASM SEO - You have a broken website according to Google!Ôºå\n",
	  "pubDate": "2023-12-20T07:44:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/048en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/aspnetcore-sitemap/",
	  "title": "Create a Sitemap Using ASP.NET Core API \u0026 Blazor",
	  "summary": "\u003cp\u003eDocument the process of using Blazor WebAssembly Hybrid to generate a site map.\u003c/p\u003e",
	  "content": "Document the process of using Blazor WebAssembly Hybrid to generate a site map.\nWhat is Sitemap?\nSitemap is like a list or directory of websites. This list lists all the accessible pages, articles, images, etc. on the website, making it easier for search engines and website crawlers (such as Googlebot, Bingbot, etc.) to understand and explore how the website is organized. It helps search engines find and record content on your website more quickly and accurately, so users can more easily find what they need when they search.\nIn short, sitemap is a way for search engines to better understand and record the structure of website content, so that users can more easily find the information they want.\nForeword Currently, I have experience using Hugo (a static web page generator written in Go language) to build a website. Basically, after compilation, this framework can automatically generate Sitemap.xml for use, and web crawlers can also use it because each web page is a static website. Easily obtain website information.\nHowever, for websites built using Blazor WebAssembly Hybrid or using front-end frameworks such as Vue and React, because they are SPA (Single Page Application), they always encounter many problems in website exposure, SEO, etc.\nThis article mainly records the creation method of website map required by SPA type websites in order to facilitate related services such as Prerender.io and crawlers when doing pre-rendering.\nRegarding SPA type websites, how to use pre-rendering to solve the problem that Google crawlers cannot crawl website actual data, you can refer to this article: Blazor WASM SEO - Use pre-rendering to solve the problem that SPA web SEO (Prerender.io \u0026amp; Cloudflare Workers tutorial) (Prerender.io \u0026amp; Cloudflare Workers)\nSolution Basically there is no quick way, you can only generate a sitemap according to your own website structure. Regarding the use of Blazor WebAssembly Hybrid, I was originally thinking about whether it can be automatically generated by the server, so that if the website data changes, the Sitemap can be modified immediately, but real-time means that all APIs for data changes must be linked, which seems complicated. So use an independent API instead, and then generate Sitemap according to needs.\nAs for the Hybrid type framework, after the file is generated, it is located in the root directory, which is the server side. Currently, when deploying the website, there is an extra step to move the sitemap.xml of the server side to the client side. In this way, the file can be obtained through https://your domain/sitemap.xml when website lunched.\nCode First refer to the examples provided by Google\n1\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; 2\u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34;\u0026gt; 3 \u0026lt;url\u0026gt; 4 \u0026lt;loc\u0026gt;https://www.example.com/foo.html\u0026lt;/loc\u0026gt; 5 \u0026lt;lastmod\u0026gt;2022-06-04\u0026lt;/lastmod\u0026gt; 6 \u0026lt;/url\u0026gt; 7\u0026lt;/urlset\u0026gt; Contains \u0026lt;loc\u0026gt; URL, \u0026lt;lastmod\u0026gt; last modification time, and can also increase \u0026lt;priority\u0026gt; web page weight and \u0026lt;changefreq\u0026gt; update frequency. So we need to use API to assemble files in the same format. The following is an example:\nBasic parameters for Sitemap 1public class Page 2{ 3 public string Url { get; set; } 4 public DateTime LastModified { get; set; } 5 public string ChangeFrequency { get; set; } 6 public double Priority { get; set; } 7} API 1[HttpGet] 2public async Task\u0026lt;bool\u0026gt; GetSiteMap() 3{ 4 XNamespace xmlns = \u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34;; // Define namespace 5 6 // Create XML document 7 XDocument sitemap = new XDocument( 8 new XDeclaration(\u0026#34;1.0\u0026#34;, \u0026#34;utf-8\u0026#34;, \u0026#34;yes\u0026#34;), 9 new XElement(xmlns + \u0026#34;urlset\u0026#34; // Using XNamespace 10 ) 11 ); 12 13 string baseUrl = \u0026#34;https://your domain/\u0026#34;; 14 15 // Create List\u0026lt;Page\u0026gt; object 16 var pages = new List\u0026lt;Page\u0026gt; 17 { 18 new Page { Url = baseUrl, LastModified = DateTime.Now, ChangeFrequency = \u0026#34;daily\u0026#34;, Priority = 1.0 } 19 }; 20 //Add pages according to your website structure 21 ServiceResponse\u0026lt;List\u0026lt;Category\u0026gt;\u0026gt; result = await _categoryService.GetCategories(); 22 List\u0026lt;Product\u0026gt; products = await _productService.GetAvailableProducts(); 23 foreach (Category category in result.Data) 24 { 25 category.Product = products.Where(o =\u0026gt; o.CategoryId == category.Id \u0026amp;\u0026amp; o.IsImage == true).OrderBy(o =\u0026gt; o.Order).ToList(); 26 pages.Add(new Page { Url = baseUrl + \u0026#34;category/\u0026#34; + category.Url, LastModified = DateTime.Now, ChangeFrequency = \u0026#34;daily\u0026#34;, Priority = 1.0 }); 27 28 foreach (Product product in products) 29 { 30 pages.Add(new Page { Url = baseUrl + \u0026#34;product/\u0026#34; + product.Url, LastModified = DateTime.Now, ChangeFrequency = \u0026#34;daily\u0026#34;, Priority = 1.0 }); 31 } 32 } 33 34 // Add each page to the Sitemap 35 foreach (var page in pages) 36 { 37 XElement urlElement = new XElement(xmlns + \u0026#34;url\u0026#34;, 38 new XElement(xmlns + \u0026#34;loc\u0026#34;, page.Url), 39 new XElement(xmlns + \u0026#34;lastmod\u0026#34;, page.LastModified.ToString(\u0026#34;yyyy-MM-dd\u0026#34;)), 40 new XElement(xmlns + \u0026#34;changefreq\u0026#34;, page.ChangeFrequency), 41 new XElement(xmlns + \u0026#34;priority\u0026#34;, page.Priority.ToString(\u0026#34;0.0\u0026#34;, System.Globalization.CultureInfo.InvariantCulture)) 42 ); 43 sitemap.Root.Add(urlElement); 44 } 45 46 // Save XML 47 string filePath = \u0026#34;sitemap.xml\u0026#34;; 48 sitemap.Save(filePath); 49 return true; 50} Conclusion The above is the method for creating Sitemap.xml. Any website that wants to improve SEO or analyze traffic sources through GA4 requires the provision of Sitemap. Next will explain how SPA-type websites solve the problem of web crawlers not being able to crawl website data.\nReference\nBuild and submit a sitemapÔºå ",
	  "pubDate": "2023-12-18T07:44:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/047en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/api-with-synology-nas/",
	  "title": "Use API to connect to Synology NAS services",
	  "summary": "\u003cp\u003eDocumenting the problem encountered while integrating with Synology NAS services via API.\u003c/p\u003e",
	  "content": "Documenting the problem encountered while integrating with Synology NAS services via API.\nForeword Recently, in order to overcome with the large amount of data storage, the company\u0026rsquo;s system has chosen to store data on NAS, so let\u0026rsquo;s study how to connect this service via API.\nBasically, you can refer to the official documentation for how to use it. Synology_File_Station_API_Guide, This article mainly records several APIs used and problems encountered.\nLogin To use each API of this service, you basically need to log in to obtain permissions. Each parameter is entrained in the form of a query string.\nAPI Request:\nGet\rhttp://myds.com:port/webapi/auth.cgi?api=SYNO.API.Auth\u0026amp;version=3\u0026amp;method=login\u0026amp;account=admin\u0026amp;passwd=12345\u0026amp;session=FileStation\u0026amp;format=cookie URLÔºöhttp://{nasurl}:5000/webapi/auth.cgi\nThe port for http is 5000 and for https is 5001.\nQuery string:\napi=SYNO.API.Auth\nversion=3\nmethod=login\naccount=your account\npasswd=your passwd\nsession=FileStation\nformat=cookie\nResponse:\n{\r\u0026#34;data\u0026#34;: {\r\u0026#34;did\u0026#34;: \u0026#34;WeoBfHknZ4EllXoN4Jjds8ukcgyWyVJfQvoz60xTTpzMSpEaD8GU_yvrvjxK10h1Z4eMUhaxtyH5OphCCGl5Hw\u0026#34;,\r\u0026#34;sid\u0026#34;: \u0026#34;2ZLVB8UbL6EHnkI4uCO730lqNgJyzjBfhDcodeqNETcG_pMHr5yjt-gTYI0OtpzbmXAxlmZMXi3PyHkEyZHI0E\u0026#34;\r},\r\u0026#34;success\u0026#34;: true\r} did is the ID of the nas service, and sid is the permission certificate obtained after logging in. When calling any API later, you can choose to carry it in the form of a cookie or query string.\nThis article uses the query string to carry sid, and the parameter is _sid=sid.\nLogout API Request:\nGet\rhttp://myds.com:5000/webapi/auth.cgi?api=SYNO.API.Auth\u0026amp;version=1\u0026amp;method=logout\u0026amp;session=FileStation Query string:\napi=SYNO.API.Auth\nversion=1\nmethod=logout\nsession=FileStation\n_sid=sid\nResponse:\n{\r\u0026#34;success\u0026#34;: true\r} After testing the sid\u0026rsquo;s permissions, the validity period is quite long, so there is no need to log in and out frequently. My current approach involves checking the viability of the sid before each NAS operation. If the sid is invalid, I trigger a login action to acquire a new sid.\nCreate new folder API Request:\nGet\rhttp://myds.com:5000/webapi/entry.cgi?api=SYNO.FileStation.CreateFolder\u0026amp;version=2\u0026amp;method=create\u0026amp;folder_path=%5B%22%2Fvideo%22%5D\u0026amp;name=%5B%22test%22%5D Query string:\napi=SYNO.FileStation.CreateFolder\nversion=2\nmethod=create\nfolder_path=/path\nname=folderName\n_sid=sid\nResponse:\n{\r\u0026#34;data\u0026#34;: {\r\u0026#34;folders\u0026#34;: [\r{\r\u0026#34;isdir\u0026#34;: true,\r\u0026#34;name\u0026#34;: \u0026#34;XDS\u0026#34;,\r\u0026#34;path\u0026#34;: \u0026#34;/SPM/XDS\u0026#34;\r}\r]\r},\r\u0026#34;success\u0026#34;: true\r} Delete API Request:\nGet\rhttp://myds.com:5000/webapi/entry.cgi?api=SYNO.FileStation.Delete\u0026amp;version=2\u0026amp;method=start\u0026amp;path=%22%2Fvideo%2Fdel_folder%22 Query string:\napi=SYNO.FileStation.Delete\nversion=2\nmethod=start\npath=/filePath\nname=folderName\n_sid=sid\nResponse:\n{\r\u0026#34;data\u0026#34;: {\r\u0026#34;taskid\u0026#34;: \u0026#34;FileStation_657A5A3DF1C925ED\u0026#34;\r},\r\u0026#34;success\u0026#34;: true\r} Executing deletion will return a taskid, which represents an action to perform deletion. To check whether the deletion is successful, you need to call another API.\nAPI Request:\nGet\rhttp://myds.com:5000/webapi/entry.cgi?api=SYNO.FileStation.Delete\u0026amp;version=2\u0026amp;method=status\u0026amp;taskid=%22FileStation_51CEC9C979340E5A%22 Query string:\napi=SYNO.FileStation.Delete\nversion=2\nmethod=status\ntaskid=FileStation_657A5A483B021703\n_sid=sid\nResponse:\n{\r\u0026#34;data\u0026#34;: {\r\u0026#34;finished\u0026#34;: true,\r\u0026#34;path\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;processed_num\u0026#34;: 0,\r\u0026#34;processing_path\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;progress\u0026#34;: 1,\r\u0026#34;total\u0026#34;: -1\r},\r\u0026#34;success\u0026#34;: true\r} You can confirm whether the file or folder was deleted successfully.\nUpload File API Request:\nPost\rhttp://myds.com:5000/webapi/entry.cgi?api=SYNO.FileStation.Upload\u0026amp;version=2\u0026amp;method=upload\u0026amp;path=/SPM/XD\u0026amp;create_parents=true\u0026amp;_sid=AgQzS8bUN7bJcfwfwHj4n2ZT_pjmC5PQAiKV_Xt27AgTO8B9uySZqhK9uQg8UosfMvBLUKrtYqrZtQKp1fJVqU Query string:\napi=SYNO.FileStation.Upload\nversion=2\nmethod=upload\npath=/folderPath\ncreate_parents=true\n_sid=sid\nThe Body part needs to use the form-data format:\nPostman form-data\rResponse:\n{\r\u0026#34;data\u0026#34;: {\r\u0026#34;blSkip\u0026#34;: false,\r\u0026#34;file\u0026#34;: \u0026#34;unnamed.png\u0026#34;,\r\u0026#34;pid\u0026#34;: 30385,\r\u0026#34;progress\u0026#34;: 1\r},\r\u0026#34;success\u0026#34;: true\r} Official document for file upload are not clear, and I am not sure whether the parameters must be placed in the query string or form-data. As a result, the file upload was successful when the file was placed on both sides\u0026hellip;\nFile Download API Request:\nGet\rhttp://myds.com:5000/webapi/entry.cgi?api=SYNO.FileStation.Download\u0026amp;version=2\u0026amp;method=download\u0026amp;path=%5B%22%2Ftest%2FITEMA_20445972-0.mp3%22%5D\u0026amp;mode=%22open%22 Query string:\napi=SYNO.FileStation.Download\nversion=2\nmethod=download\npath=/filePath\nmode=open or download\nTo open a file or folder using open, the Content-Type of the HTTP header can be set to the MIME type corresponding to the file.\nUse download to download a file or folder, set the Content-Type of the HTTP header to application/octet-stream, and set the Content-Disposition to attachment, which tells the browser to treat it as an attachment and prompt for download.\n_sid=sid\nConclusion What needs to be noted is that for the file upload part, while the parameters are carried using query strings, they also need to be passed using form-data to be successful. I have tried this for a long time\u0026hellip;\nThis is the first time I have connected to a NAS service. I usually provide APIs for people to connect to. After this experience, I realized that the API documentation must be well written, otherwise it will be really troublesome to rely on psychics when encountering problems.\n",
	  "pubDate": "2023-12-14T07:44:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/046en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/url-querystring-encoding-decoding/",
	  "title": "Understanding URL Query Strings and Encoding/Decoding",
	  "summary": "\u003cp\u003eWhen passing parameters in URL query strings, use encoding and decoding and some details that need to be attention.\u003c/p\u003e",
	  "content": "When passing parameters in URL query strings, use encoding and decoding and some details that need to be attention.\nProblem After working for a long time, I recently encountered an issue where some files couldn\u0026rsquo;t be downloaded successfully from a service. The system logic involves allowing file uploads and downloads. When a customer uploads a file, a UUID is generated, and the customer\u0026rsquo;s information is written into the database along with storing the file. During the download process, a JWT (JSON Web Token) containing time-limited information is generated in real-time based on the user\u0026rsquo;s information and the file\u0026rsquo;s UUID, to distribute a one-time download URL.\nHowever, the JWT is carried by the frontend as a query string when calling the file download API. In the user interface, there are often download errors occurring, while directly calling the API using Swagger results in successful downloads.\nReason for the issue After a long search, I discovered that the issue was caused by passing URL parameters containing special characters. Special characters in a URL typically carry different meanings.\nHere are a few examples:\n?Ôºö Commonly found at the beginning of a URL or between the path and query string, it serves to separate the primary components of a URL from query parameters. In a URL, the part after \u0026lsquo;?\u0026rsquo; is typically the query string, used to pass parameters and data to the server. For exampleÔºöhttps://www.example.com/search?query=example\nIn the above URL, the \u0026lsquo;query=example\u0026rsquo; after the \u0026lsquo;?\u0026rsquo; is the query string used to pass search parameters.\n\u0026amp;Ôºö In the URL\u0026rsquo;s query string, it serves to separate different query parameters, allowing the transmission of multiple parameters within the same URL. For exampleÔºöhttps://www.example.com/search?query=example\u0026amp;page=2\nIn this example, \u0026lsquo;\u0026amp;\u0026rsquo; is used to separate the query parameters \u0026lsquo;query=example\u0026rsquo; and \u0026lsquo;page=2\u0026rsquo;.\n=Ôºö In the URL\u0026rsquo;s query string, it serves to separate parameter names and values, linking each parameter\u0026rsquo;s name with its corresponding value. For exampleÔºöhttps://www.example.com/search?query=example\nHere, \u0026lsquo;=\u0026rsquo; separates \u0026lsquo;query\u0026rsquo; (parameter name) from \u0026rsquo;example\u0026rsquo; (parameter value).\nIf the query string carries parameters containing such special characters, browsers generally encounter issues while parsing them.\nThere are also other characters such as \u0026lsquo;+\u0026rsquo; and space that require careful handling when passing in a URL.\nEncoding and decoding So, if you insist on passing information containing such special characters through a query string, you can address this by using URL encoding and decoding.\nIn the situation I encountered, it\u0026rsquo;s necessary to pass a JWT through the query string. As JWTs contain numerous special characters, generating the JWT on the backend, encoding it, and then returning it to the frontend allows for proper API file downloads.\nExample :\nIf the parameter to be carried is: =hello+world.\nWithout encoding, using the query string to carry parameters will lead to misinterpretation by the browser.\nAfter encoding, it becomes: %3Dhello%2Bworld.\nWhen returned to the frontend, there won\u0026rsquo;t be any issues when calling.\nAs for the backend, most backend frameworks or languages\u0026rsquo; HTTP request handlers automatically decode URL parameters. However, it\u0026rsquo;s still advisable to verify during usage to prevent potential errors that might require debugging later on.\nConclusion It\u0026rsquo;s a very basic and simple concept, so I haven\u0026rsquo;t bothered to figure it out before. This lack of understanding took me half an hour to debug it, so I\u0026rsquo;ll record it.üòé\n",
	  "pubDate": "2023-12-07T07:44:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/045en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/aspnetcore-6.0-aws-s3/",
	  "title": "Operating Amazon S3 with ASP.NET Core Web API and Blazor",
	  "summary": "\u003cp\u003eRecording the Process of Integrating Amazon S3 with ASP.NET Core.\u003c/p\u003e",
	  "content": "Recording the Process of Integrating Amazon S3 with ASP.NET Core.\nForeword The project structure for this article is Blazor WebAssembly Hybrid.Combining Blazor WebAssembly (Frontend) with Server API (Backend). Apart from documenting the usage of Web API for S3 integration, it also involves recording how Blazor WebAssembly calls the API to transfer files.\nEnvironment Blazor Webassembly Hybrid .Net 6 In a typical Web API project, the structure is relatively straightforward. However, when utilizing the Blazor WebAssembly Hybrid approach, the project structure is divided into Client, Server, and Shared components. The Server part can essentially be treated similarly to a regular Web API project, encompassing structures like Controllers.\nBlazor WebAssembly Hybrid Structure\rPre-process Install the corresponding version of AWSSDK.S3 Nuget package AWSSDK.S3\nThen you can perform operations on S3 using the AWS SDK and .NET applications.\nGet Access Key This approach involves accessing AWS S3 through a third-party (Web API) and requires securely controlling access to AWS resources via AWS Identity and Access Management (IAM). After get Access Key that can implement S3 operation via API. (For details on acquiring these keys, please refer to the official website!)\nSet key to project C# projects need to go to appsettings.json to set up. The setting structure of this project is as follows:\n1{ 2 \u0026#34;AWS\u0026#34;: { 3 \u0026#34;AccessKey\u0026#34;: \u0026#34;Your AccessKey\u0026#34;, 4 \u0026#34;SecretKey\u0026#34;: \u0026#34;Your SecretKey\u0026#34; 5 } 6} Create S3 operation method Basic Settings 1using Amazon.Runtime; 2using Amazon.S3.Transfer; 3using Amazon.S3; 4using Amazon.S3.Model; 5using System.Web; 6 7private IConfiguration configuration; 8private readonly AwsCredentials awsCredentialsValues; 9private readonly BasicAWSCredentials credentials; 10private readonly AmazonS3Config config; 11 12public StorageService() 13{ 14 // Initializing configuration using appsettings.json file 15 configuration = new ConfigurationBuilder() 16 .AddJsonFile(\u0026#34;appsettings.json\u0026#34;) 17 .Build(); 18 19 // Retrieving AWS section from the configuration 20 var awsConfig = configuration.GetSection(\u0026#34;AWS\u0026#34;); 21 22 // Retrieving AccessKey and SecretKey from the AWS section 23 string accessKey = awsConfig[\u0026#34;AccessKey\u0026#34;]; 24 string secretKey = awsConfig[\u0026#34;SecretKey\u0026#34;]; 25 26 // Creating AWS credentials object 27 awsCredentialsValues = new AwsCredentials() 28 { 29 AccessKey = accessKey, 30 SecretKey = secretKey 31 }; 32 33 // Creating BasicAWSCredentials object using AccessKey and SecretKey 34 credentials = new BasicAWSCredentials(awsCredentialsValues.AccessKey, awsCredentialsValues.SecretKey); 35 36 // Configuring Amazon S3 region endpoint to APNortheast1 37 config = new AmazonS3Config() 38 { 39 RegionEndpoint = Amazon.RegionEndpoint.APNortheast1 40 }; 41} RegionEndpointNeed to be modified according to the service area.\nCreate Folder 1public async Task\u0026lt;bool\u0026gt; CreateFolder(string folderPath) 2{ 3 folderPath = HttpUtility.UrlDecode(folderPath); 4 var s3client = new AmazonS3Client(credentials, config); 5 6 PutObjectRequest request = new PutObjectRequest() 7 { 8 BucketName = \u0026#34;Your BucketName\u0026#34;, 9 Key = folderPath // in S3 key represents a path 10 }; 11 12 PutObjectResponse response = await s3client.PutObjectAsync(request); 13 14 if (response.HttpStatusCode == System.Net.HttpStatusCode.NoContent) 15 { 16 return true; 17 } 18 19 return false; 20} Uploaded object structure (can be set according to requirements)\n1 public class S3Obj 2 { 3 public string Name { get; set; } = null!; 4 public MemoryStream InputStream { get; set; } = null!; 5 public string BucketName { get; set; } = null!; 6 } Upload File 1public async Task\u0026lt;S3ResponseDto\u0026gt; UploadFileAsync(S3Obj obj) 2{ 3 var response = new S3ResponseDto(); 4 try 5 { 6 var uploadRequest = new TransferUtilityUploadRequest() 7 { 8 InputStream = obj.InputStream, 9 Key = obj.Name, 10 BucketName = obj.BucketName, 11 CannedACL = S3CannedACL.NoACL 12 }; 13 14 // Initialize client 15 using var client = new AmazonS3Client(credentials, config); 16 17 // Initialize the transfer/upload tools 18 var transferUtility = new TransferUtility(client); 19 20 // Initiate the file upload 21 await transferUtility.UploadAsync(uploadRequest); 22 23 response.StatusCode = 201; 24 response.Message = $\u0026#34;{obj.Name}\u0026#34;; 25 } 26 catch (AmazonS3Exception s3Ex) 27 { 28 response.StatusCode = (int)s3Ex.StatusCode; 29 response.Message = s3Ex.Message; 30 } 31 catch (Exception ex) 32 { 33 response.StatusCode = 500; 34 response.Message = ex.Message; 35 } 36 return response; 37} Returned file object (can be set according to requirements)\n1public class S3FileInfo 2 { 3 public string Url { get; set; } = string.Empty; 4 public byte[]? File { get; set; } = null; 5 public string Name { get; set; } = string.Empty; 6 public string str_Id { get; set; } = string.Empty; 7 public int Order { get; set; } = 0; 8 } Due to project requirements, need to get file URL and then displayed on the front end. To prevent leakage, you can use the pre-signed URL (to prevent the file from being freely downloaded and passed on), through GetPreSignedURL to obtain the pre-signed URL (Pre-signed URL) when retrieving the file.\nPre-signed URLs are a mechanism provided by Amazon S3 (Simple Storage Service) that allows access to specific resources in a bucket to be assigned in a secure and limited manner. This is a time-created URL that configures temporary access to a specific S3 resource (such as an object or file).\nGet Files 1public async Task\u0026lt;List\u0026lt;S3FileInfo\u0026gt;\u0026gt; GetFileAsync(ListObjectsV2Request request) 2{ 3 // Initialize Amazon S3 client 4 var s3client = new AmazonS3Client(credentials, config); 5 6 // Get objects list based on the provided request 7 var response = await s3client.ListObjectsV2Async(request); 8 List\u0026lt;S3FileInfo\u0026gt; result = new List\u0026lt;S3FileInfo\u0026gt;(); 9 10 foreach (var obj in response.S3Objects) 11 { 12 // Skip if the object is a folder (ends with \u0026#34;/\u0026#34;) 13 if (obj.Key.EndsWith(\u0026#34;/\u0026#34;)) continue; 14 15 // Generate a pre-signed URL for the object 16 var presignRequest = new GetPreSignedUrlRequest() 17 { 18 BucketName = \u0026#34;Your BucketName\u0026#34;, 19 Key = obj.Key, 20 Expires = DateTime.UtcNow.AddSeconds(86400), 21 }; 22 var presignedUrlResponse = s3client.GetPreSignedURL(presignRequest); 23 24 // The following content depends 25 //on the file\u0026#39;s naming convention 26 //logic and the format of the returned file 27 // Modify this logic according to your file 28 //naming conventions and desired file format 29 30 // Extract file information 31 string name = Path.GetFileName(obj.Key); 32 string[] info = name.Split(\u0026#39;.\u0026#39;); 33 string str_ID = string.Empty; 34 if (info.Length \u0026gt; 0) str_ID = info[0]; 35 36 // Add S3 file info to the result list 37 result.Add(new S3FileInfo 38 { 39 Url = presignedUrlResponse, 40 Name = obj.Key, 41 str_Id = str_ID 42 }); 43 } 44 return result; 45} Delete files 1public async Task\u0026lt;bool\u0026gt; DeleteFileAsync(string key) 2{ 3 key = HttpUtility.UrlDecode(key); 4 var s3client = new AmazonS3Client(credentials, config); 5 var response = await s3client.DeleteObjectAsync(\u0026#34;YourBucketName\u0026#34;, key); 6 if (response.HttpStatusCode == System.Net.HttpStatusCode.NoContent) 7 { 8 return true; 9 } 10 return false; 11} The key is the path of the actual file. If you want to delete the folder, the key is ForderName/. If there are files under the folder, this method cannot delete the folder. You need to clear all the data advance that you can delete the folder.\nThe above process mainly implements several methods based on the current project requirements, including creating folders, uploading files, obtaining file URLs in a pre-signed manner, and deleting files.\nThe following step involves documenting the API code and how the frontend interacts with the API.\nBlazor Server Side(Web API) Get file 1[HttpGet(\u0026#34;product/{file}\u0026#34;)] 2public async Task\u0026lt;List\u0026lt;S3FileInfo\u0026gt;\u0026gt; GetProductFile(string file) 3{ 4 var request = new ListObjectsV2Request() 5 { 6 BucketName = \u0026#34;YourBucketName\u0026#34;, 7 Prefix = $\u0026#34;YourFolderName/{file}\u0026#34; 8 }; 9 10 List\u0026lt;S3FileInfo\u0026gt; FileInfo = await _storageService.GetFileAsync(request); 11 return FileInfo; 12} Upload file 1[HttpPost(\u0026#34;product/{FileId}\u0026#34;)] 2public async Task\u0026lt;S3ResponseDto\u0026gt; UploadProductFile(IFormFile file, string FileId) 3{ 4 //Prevent unauthorized users from uploading 5 var authorizationHeader = HttpContext.Request.Headers[\u0026#34;Authorization\u0026#34;]; 6 bool IsAuthentication = _authService.CheckToken(authorizationHeader); 7 var result = new S3ResponseDto(); 8 9 if (IsAuthentication) 10 { 11 // Process file 12 await using var memoryStream = new MemoryStream(); 13 await file.CopyToAsync(memoryStream); 14 15 var fileExt = Path.GetExtension(file.FileName); 16 var docName = $\u0026#34;{FileId}{fileExt}\u0026#34;; 17 18 // Call server 19 var s3Obj = new S3Obj() 20 { 21 BucketName = \u0026#34;YourBucketName\u0026#34;, 22 InputStream = memoryStream, 23 Name = \u0026#34;YourFolderName/\u0026#34; + docName 24 }; 25 26 result = await _storageService.UploadFileAsync(s3Obj); 27 } 28 return result; 29} Delete files 1[HttpDelete(\u0026#34;{key}\u0026#34;)] 2public async Task\u0026lt;bool\u0026gt; Delete(string key) 3{ 4 //Prevent unauthorized users from deleting 5 var authorizationHeader = HttpContext.Request.Headers[\u0026#34;Authorization\u0026#34;]; 6 bool IsAuthentication = _authService.CheckToken(authorizationHeader); 7 8 if (IsAuthentication) 9 { 10 return await _storageService.DeleteFileAsync(key); 11 } 12 else 13 { 14 return false; 15 } 16} Blazor Client Side Register a StorageService service, corresponding to API functions such as getting, uploading and deleting files.\nCode:\n1public class StorageService : IStorageService 2{ 3 private readonly HttpClient _http; 4 5 public StorageService(HttpClient http) 6 { 7 _http = http; 8 } 9 10 public async Task\u0026lt;List\u0026lt;S3FileInfo\u0026gt;\u0026gt; GetProductFile(string file) 11 { 12 var result = await _http.GetFromJsonAsync\u0026lt;List\u0026lt;S3FileInfo\u0026gt;\u0026gt;($\u0026#34;api/Storage/product/{file}\u0026#34;); 13 return result; 14 } 15 16 public async Task\u0026lt;string\u0026gt; UploadFile(string FileId, string fileName, MemoryStream InputStream) 17 { 18 var response = await _http.PostAsync($\u0026#34;api/Storage/product/{FileId}\u0026#34;, 19 new MultipartFormDataContent { 20 { 21 new StreamContent(InputStream), \u0026#34;file\u0026#34;, fileName 22 } 23 }); 24 25 var newFileKey = (await response.Content.ReadFromJsonAsync\u0026lt;ServiceResponse\u0026lt;S3ResponseDto\u0026gt;\u0026gt;()).Message; 26 return newFileKey; 27 } 28 29 public async Task DeleteProductImage(string key) 30 { 31 key = $\u0026#34;YourFolderName/{key}\u0026#34;; 32 string encodedKey = Uri.EscapeDataString(key); 33 var result = await _http.DeleteAsync($\u0026#34;api/Storage/{encodedKey}\u0026#34;); 34 } 35} Conclusion This article mainly records how to operate S3 through AWS SDK and .NET. The actual API settings and client connection can actually be more optimized to avoid the risk of Bucket structure leakage. Because the code is already the result of simplification of the original project, feel free to raise any questions or errors.\nReference\nWorking with AWS S3 using ASP.NET Core ‚Äì Upload, Download \u0026amp; Delete Files ‚Äì Simplified Configuring AWS Credentials for .NET Applications ‚Äì Detailed Guide Amazon S3 For the .NET Developer: How to Easily Get Started ",
	  "pubDate": "2023-12-05T11:05:34+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/044en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/aspnetcore-6.0-blazor-globalization/",
	  "title": "Implementing Multilingual Support in ASP.NET Core Blazor",
	  "summary": "\u003cp\u003eDocumenting the process of developing a multilingual web application using ASP.NET Core Blazor.\u003c/p\u003e",
	  "content": "Documenting the process of developing a multilingual web application using ASP.NET Core Blazor.\nForeword Because during my side projects, I often like to mix both Chinese and English for displaying website information. However, my partner criticized that not everyone understands English. Therefore, I thought about implementing multilingual support, allowing users to choose freely. Hence, I\u0026rsquo;m documenting the process of implementation\nPrinciple To implement multilingual functionality on a website, the primary task is to predefine the corresponding translation data. This data can be obtained via JavaScript based on the user\u0026rsquo;s preferred language, and then displayed on the screen accordingly. Fortunately, Blazor has already provided this feature, and it can be implemented in just a few steps.\nEnvironment Blazor Webassembly .Net 6 Step 1-Registration Service Install the corresponding version of the package\nMicrosoft.Extensions.Localization\nRegister the service in Program.cs AddÔºö\n1builder.Services.AddLocalization(); and\n1var host = builder.Build(); 2CultureInfo culture; 3var js = host.Services.GetRequiredService\u0026lt;IJSRuntime\u0026gt;(); 4var result = await js.InvokeAsync\u0026lt;string\u0026gt;(\u0026#34;blazorCulture.get\u0026#34;); 5if (result != null) 6{ 7 culture = new CultureInfo(result); 8} 9else 10{ 11 culture = new CultureInfo(\u0026#34;zh-TW\u0026#34;); 12 await js.InvokeVoidAsync(\u0026#34;blazorCulture.set\u0026#34;, \u0026#34;zh-TW\u0026#34;); 13} 14CultureInfo.DefaultThreadCurrentCulture = culture; 15CultureInfo.DefaultThreadCurrentUICulture = culture; Mainly through JS to obtain the local storage language parameters, if not, the default is Traditional Chinese.\nAnd rewrite await builder.Build().RunAsync(); as await host.RunAsync();\nThe complete code is:\n1builder.Services.AddLocalization(); 2 3var host = builder.Build(); 4CultureInfo culture; 5var js = host.Services.GetRequiredService\u0026lt;IJSRuntime\u0026gt;(); 6var result = await js.InvokeAsync\u0026lt;string\u0026gt;(\u0026#34;blazorCulture.get\u0026#34;); 7if (result != null) 8{ 9 culture = new CultureInfo(result); 10} 11else 12{ 13 culture = new CultureInfo(\u0026#34;zh-TW\u0026#34;); 14 await js.InvokeVoidAsync(\u0026#34;blazorCulture.set\u0026#34;, \u0026#34;zh-TW\u0026#34;); 15} 16CultureInfo.DefaultThreadCurrentCulture = culture; 17CultureInfo.DefaultThreadCurrentUICulture = culture; 18 19//await builder.Build().RunAsync(); 20await host.RunAsync(); Then you need to add this item to the project file. Click Your Project.Client\n\u0026lt;PropertyGroup\u0026gt;\r\u0026lt;BlazorWebAssemblyLoadAllGlobalizationData\u0026gt;true\u0026lt;/BlazorWebAssemblyLoadAllGlobalizationData\u0026gt;\r\u0026lt;/PropertyGroup\u0026gt; It will actually look like this\n\u0026lt;PropertyGroup\u0026gt;\r\u0026lt;TargetFramework\u0026gt;net6.0\u0026lt;/TargetFramework\u0026gt;\r\u0026lt;BlazorWebAssemblyLoadAllGlobalizationData\u0026gt;true\u0026lt;/BlazorWebAssemblyLoadAllGlobalizationData\u0026gt;\r\u0026lt;Nullable\u0026gt;enable\u0026lt;/Nullable\u0026gt;\r\u0026lt;ImplicitUsings\u0026gt;enable\u0026lt;/ImplicitUsings\u0026gt;\r\u0026lt;/PropertyGroup\u0026gt; The JS to obtain the local storage parameter is (added to wwwroot/index.html):\n1\u0026lt;script\u0026gt; 2 window.blazorCulture = { 3 get: () =\u0026gt; window.localStorage[\u0026#39;BlazorCulture\u0026#39;], 4 set: (value) =\u0026gt; window.localStorage[\u0026#39;BlazorCulture\u0026#39;] = value 5 }; 6\u0026lt;/script\u0026gt; Step 2-Create corresponding translation information Create resource files in your project.Client/Shared/ResourceFiles\nCreate resource files\rWith resource files in other languages, an additional English language structure is created here as follows:\nProject structure\rThen you can establish the text data corresponding to the actual language of the key.\nStep 3-Globalization Next, just use the resource file and injection service in the razor file corresponding to the desired screen to achieve the effect of changing the language.\n1@inject Microsoft.Extensions.Localization.IStringLocalizer\u0026lt;Resource\u0026gt; localizer 2@using ShowcaseBlazorApp.Client.Shared.ResourceFiles For strings that need to be converted into multiple languages,\n1@localizer[\u0026#34;key\u0026#34;] can be used to successfully obtain the value of the resource file and display it.\nHere I found that if the corresponding resource file cannot be found, the key will be displayed directly, so I directly use the Chinese version of the translation as the key, so that I only need to add the value corresponding to the English language.\nStep 4-Add language selection menu The example on the official website is to add a simple drop-down menu object CultureSelector.razor\n1@using System.Globalization 2@inject IJSRuntime JS 3@inject NavigationManager Navigation 4 5\u0026lt;p\u0026gt; 6 \u0026lt;label\u0026gt; 7 Select your locale: 8 \u0026lt;select @bind=\u0026#34;Culture\u0026#34;\u0026gt; 9 @foreach (var culture in supportedCultures) 10 { 11 \u0026lt;option value=\u0026#34;@culture\u0026#34;\u0026gt;@culture.DisplayName\u0026lt;/option\u0026gt; 12 } 13 \u0026lt;/select\u0026gt; 14 \u0026lt;/label\u0026gt; 15\u0026lt;/p\u0026gt; 16 17@code 18{ 19 private CultureInfo[] supportedCultures = new[] 20 { 21 new CultureInfo(\u0026#34;en-US\u0026#34;), 22 new CultureInfo(\u0026#34;zh-TW\u0026#34;), 23 }; 24 25 private CultureInfo Culture 26 { 27 get =\u0026gt; CultureInfo.CurrentCulture; 28 set 29 { 30 if (CultureInfo.CurrentCulture != value) 31 { 32 var js = (IJSInProcessRuntime)JS; 33 js.InvokeVoid(\u0026#34;blazorCulture.set\u0026#34;, value.Name); 34 35 Navigation.NavigateTo(Navigation.Uri, forceLoad: true); 36 } 37 } 38 } 39} Add a drop-down menu in MainLayout.razor to allow users to switch languages ‚Äã‚Äãat any time.\n1\u0026lt;article class=\u0026#34;bottom-row px-4\u0026#34;\u0026gt; 2 \u0026lt;CultureSelector /\u0026gt; 3\u0026lt;/article\u0026gt; Drop down language selection menu\rThe above is the implementation process for adding multiple languages ‚Äã‚Äã‚Äã‚Äãto Blazor Webassembly. In fact, there is still some content such as automatically obtaining the user\u0026rsquo;s language. I haven\u0026rsquo;t search it yet, but using the menu to change the language is enough to personal needs, so that\u0026rsquo;s all.\nReference\nASP.NET Core Blazor globalization and localization [Blazor Webassembly] ‰∏ãÊãâÈÅ∏ÂñÆÈÅ∏ÊìáÂ§öÂúãË™ûÁ≥ª ",
	  "pubDate": "2023-11-16T11:05:34+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/043en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/book/just-keep-buying/",
	  "title": "The Best Investment Strategy - Just Keep Buying (Book Recommendation)",
	  "summary": "\u003cp\u003eJust Keep Buying: Proven Ways to Save Money And Build Your Wealth - Book Review and Insights\u003c/p\u003e",
	  "content": "Just Keep Buying: Proven Ways to Save Money And Build Your Wealth - Book Review and Insights\nForeword I\u0026rsquo;ve been exploring investment and finance for quite some time, from technical analysis to value investing, iterating through various concepts. I consider myself well-versed in investment principles and believe that with the right concepts, wealth accumulation is just a matter of time. However, when discussing investment concepts with family and friends, I always find it challenging to clearly articulate the concepts and principles. Perhaps it\u0026rsquo;s due to the lack of historical data for validation and substantiation.\nGrandpa Buffett said\n\u0026ldquo;In the business world, the rear view mirror is always clearer than the windshield.\u0026rdquo; - Warren Buffett\nWith a background in data analysis, I understand that the future may not necessarily follow the trends of historical data. In reality, one cannot drive solely by looking in the rearview mirror. However, using historical data as a basis does allow us to simulate or predict the probabilities of what might happen based on past historical trajectories, enabling us to anticipate or mitigate risks. Therefore, stumbling upon this book with its significant title grounded in data science, substantiating correct investment methods, truly intrigued me to read.\nSummary - Savings The book is mainly divided into two parts: savings and investments, each further divided into multiple subsections. In the savings section, it includes discussions on how much you should save, how to save more, whether to take on debt, whether to rent or buy a house, when you can retire, and more chapters. I believe those with a foundational understanding of financial management probably have a general idea. However, for investment novices, I believe it is helpful in establishing basic concepts.\nHere, I\u0026rsquo;ll mainly share a few parts that I consider to be new concepts in my understanding.\nHow to Spend Money Without Guilt? Actually, my original idea about saving was that not spending if it\u0026rsquo;s not necessary, the money naturally accumulates. However, distinguishing between what is necessary and what is not, in this rapidly changing era, is quite challenging. Also, holding onto this perspective, sometimes I find myself just surviving instead of truly living. Having more money, yet living a more difficult life.\nSo, the book does elaborate on how to spend money, and I believe it\u0026rsquo;s beneficial not only for individuals like me and people who tend to spend all the money what they earn but also aspire to save.\nHow much lifestyle inflation is harmless? This part is also something I find quite important. Nowadays, everyone has at least some notion of financial management. People tend to live frugally in their daily lives, but often, when their circumstances improve, such as through promotions or salary raises, they tend to engage in more retaliatory spending, causing a situation where even though they earn more, they end up living poorer than before. The book explains the proportion of spending and empirical calculations, which can clearly guide individuals on how to earn and spend more without shrinking their assets.\nSummary - Investment The next section covers more advanced aspects of savings, including why you should invest, what you should invest in, why you shouldn\u0026rsquo;t buy individual stocks, how early you should start investing, why you shouldn\u0026rsquo;t wait for a market downturn to buy, how luck is involved in investing, why you shouldn\u0026rsquo;t fear market fluctuations, how to buy during times of crisis, and when you should sell.\nAlso, there\u0026rsquo;s a section that I consider a kind of life philosophy, including chapters on \u0026lsquo;Why you never feel rich?\u0026rsquo; and \u0026lsquo;The Most Important Asset\u0026rsquo;.\nThe book provides empirical data for all the investment-related aspects discussed above, allowing you to have a more precise understanding of the data theories behind your investment concepts. I believe that as long as one follows these principles by excluding speculative mentality and ignorance, everyone can gradually accumulate wealth.\nBut the most important are the last few chapters, such as The Most Important Asset, where the book provides examples\nI bet if all of Buffett\u0026rsquo;s assets were offered to you, not a single person would be willing to exchange places with him right now. Conversely, if Buffett could live for another twenty years, he would also be willing to be penniless at this moment.\nThe most important asset for a person is actually time. Earning money, saving, and investing are all about having more of one\u0026rsquo;s own time. Balancing between earning, saving, investing, and living a good life within the short decades of life depends on one\u0026rsquo;s clear understanding of what they want and what they desire.\nConclusion I used to think that investment-related books were all the same, but books that substantiate correct investment concepts with data-based evidence are indeed rare. The conclusion drawn towards the end of the book about time being the most important asset stands out. It\u0026rsquo;s a book suitable for both investment beginners and veterans alike. I hope that beginners in investment can deepen their understanding of correct investment concepts through this book. Furthermore, I aspire to strike a balance where I pursue wealth while also savoring life without getting priorities mixed up.\n",
	  "pubDate": "2023-11-15T07:44:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/042en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/life/nasal-septum-inferior-turbinate-hypertrophy-surgery/",
	  "title": "Nasal Septum Straightening \u0026 Submucosal Turbinate Reduction Surgery for Nasal Septal Deviation and Inferior Turbinate Hypertrophy",
	  "summary": "\u003cp\u003eSurgical Procedure and Precautions for Allergic-Induced Deviated Nasal Septum and Inferior Turbinate Hypertrophy\u003c/p\u003e",
	  "content": "Surgical Procedure and Precautions for Allergic-Induced Deviated Nasal Septum and Inferior Turbinate Hypertrophy\nForeword Having suffered from allergies since childhood, while my allergy symptoms have somewhat improved as I\u0026rsquo;ve grown older, I still experience breathing difficulties due to a deviated nasal septum and chronic allergic-induced inferior turbinate hypertrophy. When I get sick or my immune system is weaker, my allergy symptoms worsen, often leading to nasal congestion and shallow sleep. After researching information about the potential health risks of poor sleep, including increased cardiovascular disease and diabetes risk, I am reluctant to rely on medication and am considering undergoing surgery to see if it can improve my condition\nHow to Choose a Doctor? Initially, I visited a clinic near my home for a checkup and medication. I noticed that this doctor performed surgeries to reduce inferior turbinate hypertrophy and claimed that they were relatively bloodless, requiring no hospitalization, and involving no actual cutting. I inquired about the procedure but was informed that my condition, characterized by a severe deviated nasal septum, would not significantly benefit from merely reducing inferior turbinate size, and my request was declined.\nSo, I decided to seek medical care at a major hospital, and eventually chose Dr. Wei-Chih Chen at Kaohsiung Chang Gung Memorial Hospital for the surgery. When registering for the appointment, it\u0026rsquo;s important to pay attention to the expertise of the attending physician. My first appointment was with a different doctor, who, despite being an otolaryngologist, primarily specialized in throat-related issues. During the appointment, the doctor conducted blood tests to measure allergy levels and concluded that my allergy levels were too high, rendering medication ineffective. I was advised not to proceed.\nUndeterred, I made a second appointment, this time specifically with Dr. Wei-Chih Chen, a specialist in rhinology. During the appointment, Dr. Chen examined the structure of my nasal passages, informed me about the severity of the nasal septal deviation, and presented the option of undergoing surgery. He explained the associated risks and inquired about elective procedures that would require additional fees, including Nasal Septum Straightening + Submucosal Turbinate Reduction Surgery (priced at 30,000 NTD). After confirming my decision, we scheduled a date for the surgical procedure.\nSurgical Procedure The surgery was scheduled for a Friday, so I needed to check in and be admitted on Thursday afternoon. There, a pre-operative assessment was conducted to evaluate the anesthesia risks. The anesthesiologist asked if I wanted to add any self-funded items, one of which was a monitoring device to assess the depth of anesthesia. This device, priced at 1500 NTD, would provide the anesthesiologist with real-time information about the depth of anesthesia, making the procedure potentially safer. I opted to pay for this additional monitoring. Following the assessment, I was directed to my hospital room, which was a three-person ward covered by national health insurance. As the surgery was scheduled for the next day, I was prohibited to eat or drink anything, including water, after midnight.\nAt 5 a.m., due to fasting requirements, a nurse administered a glucose solution via an IV. From this point on, I would remain connected to the IV until discharge, receiving essential medications, including antibiotics, intravenously. The doctor came to check on me around 7 a.m., but unfortunately, due to the high number of patients scheduled for surgery with Dr. Wei-Chih Chen that day, the order of procedures was based on factors like age. As a result, I had to wait until after 2 p.m. before my surgery could take place. Thankfully, the intravenous drip kept me comfortable, and the only inconvenience was the long hours of waiting.\nThe entire surgical process, including the waiting and entering the operating room, was surprisingly comfortable. The nurses were exceptionally kind and attentive. Upon arriving at the operating table, the nurses even asked if I felt cold. In addition to covering me with a warm blanket, they also provided a heater that was connected directly to my bedding. As a result, when I woke up after the surgery, even my feet felt warm.üòé\nAt the start of the anesthesia, the initial dose was relatively mild, resulting in a slight dizziness in my head. The anesthesiologist then began providing some post-anesthesia instructions, although I found it interesting that this was done at this moment. As I continued to listen to the instructions, I gradually drifted off to sleep\u0026hellip; and then woke up again.\nAfter waking up, I felt incredibly comfortable. It could be due to the additional self-funded items I had opted for, I experienced no dizziness or nausea whatsoever. What\u0026rsquo;s more, my nasal passages were completely clear! I was overjoyed. I then spent some time in the recovery room before being wheeled back to my hospital room.\nPostoperative Recovery However, as the anesthesia wore off, I began experiencing severe nasal congestion, which was quite challenging to endure. People who have had chronic nasal congestion may be familiar with occasional stuffiness, but this was a complete blockage. I believe it was due to the significant swelling of the surgical site. It was so severe that even swallowing saliva was a struggle. Throughout the night, nurses came in periodically to check on my intravenous drip and antibiotic dosage, and dealing with the extremely congested nose made it incredibly difficult to sleep.\nI had the surgery on Friday, and early on Saturday, I had to return for a follow-up appointment with the doctor to have the wound examined. This mostly involved a bit of cleaning and ensuring there was no sign of infection. After that, the hospital offered the option to purchase a nasal irrigator for 1,200 NTD as an additional expense. However, I later discovered more cost-effective alternatives online. At the time, I relied on the advice of family members and nurses and ended up purchasing the one offered by the hospital. If you already have a nasal irrigation routine, this additional purchase might not be necessary.\nI was discharged from the hospital around noon on Sunday, so my total stay in the hospital was approximately three full days.\nAfter returning home, I carefully reviewed the instructions for the nasal irrigator and realized that it should not be used when experiencing severe nasal congestion. This explained why, when I attempted to use it in the hospital, the water jet kept causing intense discomfort in my ears. However, both the nurses and the doctor had advised me to start nasal irrigation and recommended doing it four times a day. It was ultimately a matter of personal judgment, so I decided to temporarily pause the nasal irrigation, as I was concerned it might lead to ear discomfort or potentially develop into otitis media.\nThe following days were quite challenging as my severe nasal congestion persisted, and it was the kind that felt completely blocked. There wasn\u0026rsquo;t much I could do except focus on nutrition and try to get more sleep in the hope of a quicker recovery through this phase.\nMy next follow-up appointment was scheduled for Wednesday, which marked five days since the surgery. During the appointment, the doctor not only examined the surgical site but also helped clear the blood clots that had been causing the severe nasal congestion. This was the primary cause of the congestion. The process was relatively painless, and after the clearing, my nose instantly became clear. It seems that frequent nasal irrigation might help prevent these blood clots from forming.\nHowever, after a brief period of relief, the congestion returned. I wasn\u0026rsquo;t sure if it was due to new blood clots or if my nasal passages were still experiencing heightened sensitivity due to allergies. After a few more days, the symptoms of nasal congestion finally began to subside. I could finally get a good night\u0026rsquo;s sleep, and there was less blood in my mucus. The flow of water during nasal irrigation also became smoother. It seems that this surgical chapter has come to a close.\nConclusion The doctor scheduled another follow-up appointment for a month later. I think that if my recovery is progressing well by then, I might not need to return for the appointment, considering the additional cost of the registration fee. I anticipate that there shouldn\u0026rsquo;t be any significant issues at that point!üòÖ\nThe main thing to keep in mind is to exercise discretion when it comes to the self-funded options. For instance, there are two pricing options for the submucosal turbinate reduction surgery, as the doctor explained that the difference in temperature results in varying post-operative bleeding and recovery rates. Additionally, it\u0026rsquo;s crucial not to blow your nose after the surgery, as it could disrupt wound healing and lead to further bleeding. I followed these guidelines diligently, and personally, I didn\u0026rsquo;t experience significant post-operative bleeding, and my nose didn\u0026rsquo;t have much discomfort (possibly due to the pain medication!).\nIt\u0026rsquo;s been approximately two weeks since the surgery, and my nose is currently quite clear. There\u0026rsquo;s only a slight congestion remaining on the left side, where the primary surgical correction was made (due to the deviated nasal septum towards the left). However, compared to how it was before, it\u0026rsquo;s much improved. I consider the surgery a success and hope for even better sleep quality in the future. I\u0026rsquo;m very grateful to the doctor and the nurses. A visit to the hospital made me realize how demanding the work of healthcare professionals is. I hope that Taiwan\u0026rsquo;s healthcare environment continues to improve! üëç\n",
	  "pubDate": "2023-10-06T19:44:38+08:00",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/hugo-auto-update-last-modified/",
	  "title": "Automatically Updating the Last Modified Date of Hugo Posts: Streamlining Management Steps and Enhancing SEO",
	  "summary": "\u003cp\u003eSwitching from setting the last modified date in Front Matter to using the file modification date as the default post update date in Hugo simplifies the tedious process and boosts SEO ranking.\u003c/p\u003e",
	  "content": "Switching from setting the last modified date in Front Matter to using the file modification date as the default post update date in Hugo simplifies the tedious process and boosts SEO ranking.\nForeword There are often situations where posts need to be edited or updated that show updated time on the post, usually to improve SEO or inform readers that the content is the latest. Originally, the Front Matter was used, and within each post\u0026rsquo;s content, the lastmod field along with a date would be added to display the last editing time of the post. However, this process was quite tedious as it required editing the field every time an post was modified. With a large number of posts, this procedure became intricate. Upon researching, it was discovered that it\u0026rsquo;s possible to directly add a default Front Matter reference order in the config.toml file. This allows automatic insertion of any desired value for lastmod in cases where it\u0026rsquo;s not explicitly used in the post. This achieves the desired effect of default lastmod display.\nSolution Simply by adding the following syntax to config.toml:\n1[frontmatter] 2lastmod = [\u0026#34;lastmod\u0026#34; ,\u0026#39;:fileModTime\u0026#39;, \u0026#34;:git\u0026#34;, \u0026#34;date\u0026#34;] :fileModTime : The last modification time of the file.\nlastmod : The original lastmod date.\n:git : It is said to be possible to use the commit time from git as the display date, but further research has not been conducted here.\ndate: The publication date of the post.\nThis method involves modifying the default data for lastmod (prioritizing the one listed first in the order; if not found, moving on to the next) to be displayed.\nSo, the syntax can be as simple as this.\n1[frontmatter] 2lastmod = [\u0026#34;lastmod\u0026#34; ,\u0026#39;:fileModTime\u0026#39;] If an post doesn\u0026rsquo;t have the lastmod field, then the default behavior is to use :fileModTime as the last modification time of the post.\nHowever, this approach would result in each post displaying the last modification time, since typically there is a difference between the post\u0026rsquo;s date and :fileModTime.\nMy approach involves modifying root\\themes\\{your theme}\\layouts\\partials\\post_meta\\date.html to achieve the effect of hiding the modification date for certain posts.\nCode:\n1{{- if not .Date.IsZero }} 2\u0026lt;div class=\u0026#34;meta__item-datetime meta__item\u0026#34;\u0026gt; 3\t{{ partial \u0026#34;svg/calendar.svg\u0026#34; (dict \u0026#34;class\u0026#34; \u0026#34;meta__icon\u0026#34;) -}} 4\t\u0026lt;time class=\u0026#34;meta__text\u0026#34; datetime=\u0026#34;{{ .Date.Format \u0026#34;2006-01-02T15:04:05Z07:00\u0026#34; }}\u0026#34;\u0026gt; 5\t{{- .Date.Format (.Site.Params.dateformat | default \u0026#34;January 02, 2006\u0026#34;) -}} 6\t\u0026lt;/time\u0026gt; 7\t{{- $dateDiff := .Lastmod.Sub .Date -}} 8\t{{- if ge $dateDiff.Hours 100 }} 9\t\u0026lt;time class=\u0026#34;meta__text\u0026#34; datetime=\u0026#34;{{ .Lastmod.Format \u0026#34;2006-01-02T15:04:05Z07:00\u0026#34; }}\u0026#34;\u0026gt;( 10\t{{- T \u0026#34;meta_lastmod\u0026#34; }}: {{ .Lastmod.Format (.Site.Params.dateformat | default \u0026#34;January 02, 2006\u0026#34;) -}} 11\t)\u0026lt;/time\u0026gt; 12\t{{- end -}} 13\u0026lt;/div\u0026gt; 14{{- end }} The modification involves the following changes, with the logic being to calculate the time difference between post date.Date and modification date.Lastmod.\nIf the time difference is greater than 100 hours, then the modification date is displayed. (This value can be adjusted according to personal preference. Since I\u0026rsquo;m accustomed to the timeframe between writing an post and publishing it being usually less than a week, as long as the modification date is more than a week after the post date, it indicates content update and the modification date is shown.)\n1{{- $dateDiff := .Lastmod.Sub .Date -}} 2{{- if ge $dateDiff.Hours 100 }} 3\u0026lt;time class=\u0026#34;meta__text\u0026#34; datetime=\u0026#34;{{ .Lastmod.Format \u0026#34;2006-01-02T15:04:05Z07:00\u0026#34; }}\u0026#34;\u0026gt;( 4\t{{- T \u0026#34;meta_lastmod\u0026#34; }}: {{ .Lastmod.Format (.Site.Params.dateformat | default \u0026#34;January 02, 2006\u0026#34;) -}} 5\t)\u0026lt;/time\u0026gt; 6{{- end -}} Conclusion By employing this method, it\u0026rsquo;s possible to efficiently manage modification dates and even use only the latest modification date as the publication date. This creates a perception for readers that the content is freshly published and entirely new. As for how browsers might interpret this and the impact on SEO ranking, that\u0026rsquo;s a bit uncertain üòé.\n",
	  "pubDate": "2023-08-21T10:51:46+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/040en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/life/mk870-hand-rest-unboxing/",
	  "title": "Unboxing the Fuling MK870 Mechanical Keyboard with African Rosewood Wrist Rest",
	  "summary": "\u003cp\u003eThe User Experience of the Fuling MK870 Mechanical Keyboard~\u003c/p\u003e",
	  "content": "The User Experience of the Fuling MK870 Mechanical Keyboard~\nForeword As a coder, the keyboard is perhaps the most frequently used tool throughout the day. However, being tight on budget, I\u0026rsquo;ve always held onto the mindset of making things last as long as they work. This time, I\u0026rsquo;ve finally made up my mind to treat myself. Despite not having much knowledge about mechanical keyboards and usually using scissor-switch keyboards, I decided to indulge myself a bit. The experience turned out to be quite pleasant, so I thought I\u0026rsquo;d share my thoughts on the Fuling MK870 mechanical keyboard. Without any professional critique, the exterior exudes a premium quality, and the typing experience feels refreshing. That\u0026rsquo;s it.üòé„ÄÇ (Taobao is truly a delightful place to browse. After placing an order for the keyboard, I came across wrist rests that even offer free engraving services. Without hesitation, I immediately added it to my cart ‚ùó)\nThe actual keyboard I purchased a three-mode mechanical keyboard, primarily using wireless Bluetooth to connect to my computer. Currently, the connection is smooth without any latency issues. The keyboard\u0026rsquo;s RGB lighting can be customized to preferred styles. During actual testing, I used the wired mode to connect to the computer in order to effectively access and adjust the keyboard\u0026rsquo;s RGB lighting driver. Once adjustments are made, I can switch back to the wireless mode seamlessly.\nside-engraved keycaps\rMK870 Mechanical Keyboard with African Rosewood Wrist Rest\rBe More Than Just a Code Monkey.\nAspire to improve myself, whether in life or programming.\n",
	  "pubDate": "2023-08-09T08:29:23+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/039.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/mongodb-atlas-crud/",
	  "title": "Performing MongoDB Atlas CRUD Operations with .NET 6",
	  "summary": "\u003cp\u003eRecording Common .NET Core 6 Syntax for MongoDB Operations.\u003c/p\u003e",
	  "content": "Recording Common .NET Core 6 Syntax for MongoDB Operations.\nForeword While using C# with Microsoft\u0026rsquo;s MSSQL remains the most convenient and user-friendly option, I\u0026rsquo;m still willing to give it a try and take the opportunity to learn NoSQL. I\u0026rsquo;ll attempt to use C# to perform CRUD operations on MongoDB.\nOnce you\u0026rsquo;ve set up the database and collections in MongoDB Atlas, you can start practicing CRUD operations. You can also directly use the example databases provided by the official documentation for hands-on experience.\nYou can refer to the previous article for related content. MongoDB Atlas Basic Concepts \u0026amp; Using Compass to Connect MongoDB„ÄÇ\nDatabase Connection On the official website, under \u0026ldquo;Database Deployments,\u0026rdquo; click on Connect \u0026gt; Drivers, and it will display the connection syntax based on your selected language.\nFirst, you need to install MongoDB.Driver\ndotnet add package MongoDB.Driver Here is the connection string for C#, version 2.13 or later:\n1const string connectionString = \u0026#34;mongodb+srv://\u0026lt;name\u0026gt;:\u0026lt;password\u0026gt;@cluster0.swvkd15.mongodb.net/?retryWrites=true\u0026amp;w=majority\u0026#34;; Establishing Connection\n1var client = new MongoClient(connectionString); 2var collection = client.GetDatabase(\u0026#34;sample_analytics\u0026#34;).GetCollection\u0026lt;BsonDocument\u0026gt;(\u0026#34;customers\u0026#34;); Fetching the customers collection from the sample_analytics database allows subsequent operations such as reading and writing to be performed using the collection.\nBsonDocument BsonDocument is a data type in MongoDB used to represent documents in the BSON (Binary JSON) format. BSON is a lightweight binary serialization format designed for storing and exchanging data within MongoDB. BsonDocument can be thought of as a dynamic, schema-less document, similar to documents in NoSQL databases.\nWhen storing and exchanging data in MongoDB, it\u0026rsquo;s necessary to convert it to the BsonDocument format.\nHere are some characteristics and uses of BsonDocument:\nDynamic Structure: BsonDocument has a dynamic structure that doesn\u0026rsquo;t require defining fields or schemas beforehand, making it highly flexible for handling diverse data.\nStoring MongoDB Documents: BsonDocument can represent individual documents within MongoDB collections. It contains key-value pairs where keys are field names, and values can be various BSON types (e.g., strings, numbers, dates).\nUsed for Operations: In MongoDB\u0026rsquo;s .NET driver, BsonDocument is often used for executing operations like querying, inserting, updating, and deleting.\nSerialization Capabilities: BsonDocument can be easily serialized into the BSON format and passed between different applications and languages.\nWidely Applied: BsonDocument is extensively used in MongoDB development, especially when dealing with dynamic data of varying structures.\nNext, is the basic CRUD operation syntax.\nQuery Retrieve All Data\n1var documents = collection.Find(new BsonDocument()).ToList(); Limit Retrieval to the First 10 Records\n1var documents = collection.Find(new BsonDocument()).Limit(10).ToList(); Retrieve Sorted Data, Below is the data retrieved after sorting by the Date field in descending order.\n1var sort = Builders\u0026lt;BsonDocument\u0026gt;.Sort.Descending(\u0026#34;Date\u0026#34;); 2var sortedDocuments = collection.Find(new BsonDocument()).Sort(sort).ToList(); You can first define the query criteria and then execute the query using the Find method.\nQuery all data where the name field is \u0026quot;Alice\u0026quot; and the age field is greater than or equal to 25.\n1var filter = Builders\u0026lt;BsonDocument\u0026gt;.Filter.And( 2 Builders\u0026lt;BsonDocument\u0026gt;.Filter.Eq(\u0026#34;name\u0026#34;, \u0026#34;Alice\u0026#34;), 3 Builders\u0026lt;BsonDocument\u0026gt;.Filter.Gte(\u0026#34;age\u0026#34;, 25) 4 ); Query all data where the name field is \u0026quot;Alice\u0026quot; and the age field is either greater than 30 or less than 10.\n1var filter = Builders\u0026lt;BsonDocument\u0026gt;.Filter.And( 2 Builders\u0026lt;BsonDocument\u0026gt;.Filter.Eq(\u0026#34;name\u0026#34;, \u0026#34;Alice\u0026#34;), 3 Builders\u0026lt;BsonDocument\u0026gt;.Filter.Or( 4 Builders\u0026lt;BsonDocument\u0026gt;.Filter.Gt(\u0026#34;age\u0026#34;, 30), 5 Builders\u0026lt;BsonDocument\u0026gt;.Filter.Lt(\u0026#34;age\u0026#34;, 10) 6 ) 7 ); Once the query criteria are defined, execute the query using the Find method.\n1var queryResult = collection.Find(filter).ToList(); Query to Retrieve Specific Parameters Example Data:\n1using MongoDB.Bson; 2 3BsonDocument document = new BsonDocument 4{ 5 { \u0026#34;name\u0026#34;, \u0026#34;Alice\u0026#34; }, 6 { \u0026#34;age\u0026#34;, 30 }, 7 { \u0026#34;email\u0026#34;, \u0026#34;alice@example.com\u0026#34; } 8}; Next, I will provide three methods to retrieve values from the document:\nRetrieve Values Using Indexing 1string name = document[\u0026#34;name\u0026#34;].AsString; 2int age = document[\u0026#34;age\u0026#34;].AsInt32; 3string email = document[\u0026#34;email\u0026#34;].AsString; Retrieve Values Using the GetValue Method, which can prevent situations where the value does not exist 1BsonValue nameValue; 2if (document.TryGetValue(\u0026#34;name\u0026#34;, out nameValue)) 3{ 4 string name = nameValue.AsString; 5} Retrieve Values Using the TryGetElement Method, which allows you to obtain both the key and its corresponding value 1BsonElement nameElement; 2if (document.TryGetElement(\u0026#34;name\u0026#34;, out nameElement)) 3{ 4 string name = nameElement.Value.AsString; 5} Convert bsonDocument to List\u0026lt;\u0026gt;\n1List\u0026lt;People\u0026gt; result = documents.Select(bsonDocument =\u0026gt; new People 2 { 3 name = bsonDocument.GetValue(\u0026#34;name\u0026#34;).AsString, 4 age = bsonDocument.GetValue(\u0026#34;age\u0026#34;).AsInt32, 5 email = bsonDocument.GetValue(\u0026#34;email\u0026#34;).AsString, 6 // Map other properties as needed 7 }).ToList(); Insert JSON-formatted objects can be directly converted into BsonDocument and stored in the database.\n1string jsonString = \u0026#34;{ \\\u0026#34;name\\\u0026#34;: \\\u0026#34;Alice\\\u0026#34;, \\\u0026#34;age\\\u0026#34;: 30, \\\u0026#34;email\\\u0026#34;: \\\u0026#34;alice@example.com\\\u0026#34; }\u0026#34;; 2BsonDocument document = BsonDocument.Parse(jsonString); 3collection.InsertOne(document); If you want to store multiple records simultaneously, you can use method collection.InsertMany(Multiple);\nYou can also directly create BsonDocument objects and store them in the database.\nExample of Storing Multiple Objects in the Database\n1BsonDocument newDocument1 = new BsonDocument 2{ 3 { \u0026#34;name\u0026#34;, \u0026#34;Alice\u0026#34; }, 4 { \u0026#34;age\u0026#34;, 30 } 5}; 6BsonDocument newDocument2 = new BsonDocument 7{ 8 { \u0026#34;name\u0026#34;, \u0026#34;Alvin\u0026#34; }, 9 { \u0026#34;age\u0026#34;, 29 } 10}; 11 List\u0026lt;BsonDocument\u0026gt; Multiple = new List\u0026lt;BsonDocument\u0026gt;(); 12 Multiple.Add(newDocument1); 13 Multiple.Add(newDocument2); 14 collection.InsertMany(Multiple); Update Define the query criteria to find documents where the name is \u0026ldquo;Alice\u0026rdquo;.\n1var filter = Builders\u0026lt;BsonDocument\u0026gt;.Filter.Eq(\u0026#34;name\u0026#34;, \u0026#34;Alice\u0026#34;); Define the operation for updating, where the age field is updated to 31, and the email field is updated to a new value.\n1var update = Builders\u0026lt;BsonDocument\u0026gt;.Update 2 .Set(\u0026#34;age\u0026#34;, 31) 3 .Set(\u0026#34;email\u0026#34;, \u0026#34;newemail@example.com\u0026#34;); Use the UpdateOne method for updating, which will only update the first data that matches the defined criteria.\n1var result = collection.UpdateOne(filter, update); Use the UpdateMany method for updating, which will update all data that matches the defined criteria.\n1var result = collection.UpdateMany(filter, update); Delete Deletion, similar to Update, involves first defining the query criteria and then proceeding with the deletion.\nUse the DeleteOne method for deletion, which will delete the first data that matches the defined criteria.\n1var result = collection.DeleteOne(filter); Use the DeleteMany method for deletion, which will delete all data that matches the defined criteria.\n1var result = collection.DeleteMany(filter); ",
	  "pubDate": "2023-08-08T11:24:03+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/038en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/cte-lazy-loading-infinite-data-structure/",
	  "title": "Using CTE (Common Table Expression) to achieve lazy loading and querying infinite hierarchical data structure",
	  "summary": "\u003cp\u003eRecording the backend implementation of Lazy loading querying infinite hierarchical data structure.\u003c/p\u003e",
	  "content": "Recording the backend implementation of Lazy loading querying infinite hierarchical data structure.\nForeword This article focuses on querying and implementing lazy loading for an infinite hierarchical data structure, which is commonly encountered in social media platforms that offer comment functionalities. Users can reply to specific comments, generating multi-level comments. If the system does not have restrictions, it may lead to an infinite depth of comments. This article records how to query and implement lazy loading for such infinite hierarchical data structure.\nInfinite hierarchical data structure Field Name Description comment_id comment ID parent_id parent comment ID, if null, represents the first level. This structure can store comments and their hierarchical relationships. The comment_id is used to identify each comment, while the parent_id indicates which parent comment the comment is replying to.\nImplementing lazy loading Lazy loading is implemented on the frontend, while the backend is responsible for sequentially providing corresponding data. By utilizing the OFFSET-FETCH pagination technique, the backend can provide the desired data in the specified order.\ne.g.\n1 OFFSET 0 ROWS FETCH NEXT 2 ROWS ONLY To retrieve data starting from the 0th record and fetching the next two records is equivalent to obtaining the 1st and 2nd records (modifiable as per requirements).\nUsing CTE to implement reading infinite hierarchical data structure When sorting the comments based on the creation time, we can retrieve data in the correct order to resolve the issue of lazy loading. However, a problem arises with the creation time of child comments not always being in chronological order. This means that the oldest comment at the first level might have newer child comments. If we strictly sort based on creation time and return the latest 10 comments, the frontend might receive the newest child comment without getting the corresponding parent comment from the first level, causing display issues on the interface.\nTo address this issue, the data retrieval approach involves pagination and sorting based on the creation time of the first-level comments. It fetches the latest 10 first-level comments along with all their corresponding child comments. This ensures that there will be no mismatch between child comments and their respective parent comments, eliminating any problems arising from child comments not corresponding to their parent comments.\nHowever, due to the possibility of having an infinite number of child comments, it is necessary to use CTE (Common Table Expression) to ensure that data from each level is accurately retrieved.\nAssuming the table name is comment_history\ne.g.\n1WITH CommentHierarchy AS ( 2 -- Anchor member definition 3 SELECT 4 comment_id, 5 parent_id, 6 message, 7 created_time, 8 created_user, 9 1 AS Level 10 FROM 11 comment_history 12 WHERE 13 parent_id IS NULL -- Fetching top-level comments 14\tand comment_id in ( 15 select comment_id from comment_history 16 where parent_id is null„ÄÄ17 order by created_time desc 18 OFFSET 0 ROWS FETCH NEXT 10 ROWS ONLY) 19\t20 21 UNION ALL 22 23 -- Recursive member definition 24 SELECT 25 c.comment_id, 26 c.parent_id, 27 c.message, 28 c.created_time, 29 c.created_user, 30 ch.Level + 1 AS Level 31 FROM 32 comment_history c 33 INNER JOIN 34 CommentHierarchy ch ON c.parent_id = ch.comment_id 35) 36SELECT 37 comment_id, 38 parent_id, 39 message, 40 created_time, 41 created_user, 42\tLevel 43FROM 44 CommentHierarchy 45ORDER BY 46 created_time DESC Syntax Explanation:\nCreate the CommentHierarchy, first fetching the original data of comments where the condition is specified.\nparent_id IS NULL (It represents the first level.) and\ncomment_id in (\rselect comment_id from comment_history\rwhere parent_id is null„ÄÄorder by created_time desc OFFSET 0 ROWS FETCH NEXT 10 ROWS ONLY) In conjunction with lazy loading, fetch the latest 10 comments and create a new column \u0026ldquo;Level\u0026rdquo; with a value of 1 (representing first-level comments).\nAfter obtaining the first level, perform a JOIN between the original data and CommentHierarchy, binding the relationship where the parent_id in the original data matches the comment_id in the CommentHierarchy.(Retrieve the second-level child comments using the comment_id from the first level.)\n-- Recursive member definition\rSELECT\rc.comment_id,\rc.parent_id,\rc.message,\rc.created_time,\rc.created_user,\rch.Level + 1 AS Level\rFROM\rcomment_history c\rINNER JOIN\rCommentHierarchy ch ON c.parent_id = ch.comment_id Continuously use the CommentHierarchy\u0026rsquo;s comment_id to determine if there are child comments, obtaining each level of child comments in a recursive manner, and finally merging all the data together.\nConclusion With the above approach, you can obtain all child comments of parent comments and label the level of each comment.\ncomment_id parent_id message created_time created_user Level 138808E8-5FFA-4F26-927D-3E06654079BC 82D25141-29D3-4135-A162-A588B61AA6EF 2233 2023-07-25 11:55:44.443 Alvin 2 0D46466C-3725-4FBA-9676-C1D364ED0D16 82D25141-29D3-4135-A162-A588B61AA6EF 1112 2023-07-25 11:16:55.977 OKA 2 548439B1-7D6C-4268-88CA-A52570DC5409 30C7A6F8-4E69-4F3C-836B-04AEBAE46F84 test 2023-07-25 11:10:43.610 David 3 30C7A6F8-4E69-4F3C-836B-04AEBAE46F84 82D25141-29D3-4135-A162-A588B61AA6EF XD 2023-07-25 11:10:17.560 Alvin 2 82D25141-29D3-4135-A162-A588B61AA6EF NULL so tired 2023-07-25 11:09:52.187 OKA 1 6722E047-5547-4EE7-8959-938A10534858 B6C20E72-2F7A-4891-B8A3-301DBBF72CC6 cool~ 2023-07-25 11:02:23.717 Alvin 2 2281BDB4-7B21-442F-84C5-A0736BE7DD9A B6C20E72-2F7A-4891-B8A3-301DBBF72CC6 wow new system 2023-07-25 11:02:19.607 Alvin 2 B6C20E72-2F7A-4891-B8A3-301DBBF72CC6 NULL what is this? 2023-07-25 11:02:09.060 JSON 1 ",
	  "pubDate": "2023-07-25T14:00:18+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/037en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/aspnetcore-6.0-signalr-websocket-2/",
	  "title": "Implementing Real-Time Communication with WebSocket using SignalR and .NET 6 Web API - Part 2",
	  "summary": "\u003cp\u003eUsing .NET 6 Web API to establish WebSocket communication, this record simulates the test process of mapping users to connection IDs and storing and sending messages.\u003c/p\u003e",
	  "content": "Using .NET 6 Web API to establish WebSocket communication, this record simulates the test process of mapping users to connection IDs and storing and sending messages.\nForeword The previous article has introduced how to use full push. As long as there is a connection, users can receive the message. Next, we will simulate the function of pushing to a specific user.\nPrinciple WebSocket communication, as long as the user connects, a connection ID will be automatically generated. If the program logic is that when the user performs an operation, it is very easy to push a message to it. Just send a message for the current connection ID directly. But in fact, the logic may be more complicated. Usually, user A performs an operation, or wants to directly send a message to another user B. In this case, a specific user ID and connection ID must be bound. In this way, when pushing, the connection ID can be obtained and pushed through the known user ID.\nMapping Users to Connections ID Typically, when a user connects, it is necessary to store both the user ID and the connection ID. This allows for querying the connection ID associated with a specific user when broadcasting messages. There are two main methods for storing this information: in-memory storage and database storage.\nStoring the information in memory offers better performance advantages. However, if the server restarts, the related information will be lost. On the other hand, storing the information in a database allows for permanent data storage but may result in slightly lower performance.\nThe choice between these methods depends on the specific application requirements and considerations.\nDetailed information can be found in the tutorial: Mapping SignalR Users to Connections\nTest By default, when a user connects, they need to enter a username, and when they click \u0026ldquo;login\u0026rdquo;, the username will be sent to the backend, which immediately binds the username to this connection ID. Later, if you want to send information to a specific user, you can simply query the connection ID corresponding to the user from the stored data, and then send the information.\nThe process is roughly as follows:\nEnter username ‚ûú Bind username and connection ID and save\nUser B sends a message to A ‚ûú Search the stored user name A ‚ûú Get the connection ID and send\nUser offline ‚ûú Delete saved connection ID\nAdd two interfaces in Hub/IMessageHub.cs:\n1namespace SignalR_Example.Hub 2{ 3 public interface IMessageHub 4 { 5 Task sendToAllConnections(List\u0026lt;string\u0026gt; message); 6 Task JsonDataTransfer(dynamic message); 7 Task StringDataTransfer(string message); 8 } 9} JsonDataTransferis used to pass JSON object\nStringDataTransferis used to pass string\nAdd several functions in Hub/MessageHub.cs as follows:\npublic static Dictionary\u0026lt;string, string\u0026gt; userInfoDict = new Dictionary\u0026lt;string, string\u0026gt;(); Because it is only a simulation, the storage of data only uses memory storage.\nThe key of userInfoDict is the user name, and the value is the connection ID.\npublic async Task LoadUserInfo(dynamic message) { dynamic dynParam = JsonConvert.DeserializeObject(Convert.ToString(message)); string userID = dynParam.userId; var ID = Context.ConnectionId; userInfoDict[userID] = ID; await Clients.Client(ID).StringDataTransfer(\u0026#34;Login successfully.\u0026#34;); } Simulating the frontend, when a user logs in, it sends a JSON object to the backend. After storing the data in the userInfoDict, it will call StringDataTransfer to pass the message \u0026ldquo;Login successful\u0026rdquo; back to the frontend.\nContext.ConnectionId is a property in the SignalR package used to retrieve the Connection ID of the current connection.\npublic async Task SendToConnection(string userID, string message) { if (userInfoDict.ContainsKey(userID)) { await Clients.Client(userInfoDict[userID]).StringDataTransfer(message); } } Simulating the frontend, when a user sends a message to another user, it queries the userInfoDict using the userID to check if there is a corresponding connection ID. If a connection ID is found, it uses StringDataTransfer to pass the string message.\n/// \u0026lt;summary\u0026gt; /// Automatically obtaining the connection ID /// \u0026lt;/summary\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public override Task OnConnectedAsync() { //string userId = Context.User.Identity.Name; string connectionId = Context.ConnectionId; return base.OnConnectedAsync(); } This is a built-in method in SignalR, which is triggered when a user establishes a connection. Since this simulation assumes that the user needs to input a userID beforehand, it is not tested in this particular section.\n/// \u0026lt;summary\u0026gt; /// Disconnecting and automatically removing the connection ID /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;exception\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; public override Task OnDisconnectedAsync(Exception exception) { string ID = Context.ConnectionId; string userID = string.Empty; if (userInfoDict.ContainsValue(ID)) { string key = userInfoDict.FirstOrDefault(x =\u0026gt; x.Value == ID).Key; userInfoDict.Remove(key); } return base.OnDisconnectedAsync(exception); } This is a built-in method in SignalR, which is triggered when a user disconnects. In this simulation, when a user goes offline, it removes the connection data stored in the userInfoDict.\nThe above two built-in methods can be tested with breakpoints by themselves.\nHub/MessageHub.cs Complete Code:\n1namespace SignalR_Example.Hub 2{ 3 public class MessageHub: Hub\u0026lt;IMessageHub\u0026gt; 4 { 5 public async Task sendToAllConnections(List\u0026lt;string\u0026gt; message) 6 { 7 await Clients.All.sendToAllConnections(message); 8 } 9 10 public static Dictionary\u0026lt;string, string\u0026gt; userInfoDict = new Dictionary\u0026lt;string, string\u0026gt;(); 11 public async Task LoadUserInfo(dynamic message) 12 { 13 dynamic dynParam = JsonConvert.DeserializeObject(Convert.ToString(message)); 14 string userID = dynParam.userId; 15 var ID = Context.ConnectionId; 16 userInfoDict[userID] = ID; 17 await Clients.Client(ID).StringDataTransfer(\u0026#34;Login successfully.\u0026#34;); 18 } 19 public async Task SendToConnection(string userID, string message) 20 { 21 if (userInfoDict.ContainsKey(userID)) 22 { 23 await Clients.Client(userInfoDict[userID]).StringDataTransfer(message); 24 } 25 } 26 27 /// \u0026lt;summary\u0026gt; 28 /// Automatically obtaining the connection ID 29 /// \u0026lt;/summary\u0026gt; 30 /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; 31 public override Task OnConnectedAsync() 32 { 33 //string userId = Context.User.Identity.Name; 34 string connectionId = Context.ConnectionId; 35 return base.OnConnectedAsync(); 36 } 37 38 /// \u0026lt;summary\u0026gt; 39 /// Disconnecting and automatically removing the connection ID 40 /// \u0026lt;/summary\u0026gt; 41 /// \u0026lt;param name=\u0026#34;exception\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; 42 /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; 43 public override Task OnDisconnectedAsync(Exception exception) 44 { 45 string ID = Context.ConnectionId; 46 string userID = string.Empty; 47 if (userInfoDict.ContainsValue(ID)) 48 { 49 string key = userInfoDict.FirstOrDefault(x =\u0026gt; x.Value == ID).Key; 50 userInfoDict.Remove(key); 51 } 52 return base.OnDisconnectedAsync(exception); 53 } 54 } 55} In Controllers/MsgController.cs, add an API for sending information to a specific user, simulating instant messaging through the API.\n1[HttpPost] 2[Route(\u0026#34;toUser\u0026#34;)] 3public string toUser([FromBody] JsonElement jobj) 4{ 5 var userID = jobj.GetProperty(\u0026#34;userID\u0026#34;).GetString(); 6 var Msg = jobj.GetProperty(\u0026#34;msg\u0026#34;).GetString(); 7 if (MessageHub.userInfoDict.ContainsKey(userID)) 8 { 9 messageHub.Clients.Client(MessageHub.userInfoDict[userID]).StringDataTransfer(Msg); 10 return \u0026#34;Msg sent successfully to user!\u0026#34;; 11 } 12 else return \u0026#34;Msg sent failed to user!\u0026#34;; 13 14} Front-end Code Next, is the screen display of the front end, because it is mainly used to test the connection function, and the messages received by the front end will be displayed on the console for convenience.\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; 5 \u0026lt;title\u0026gt;SignalR TEST \u0026lt;/title\u0026gt; 6 \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@microsoft/signalr@5.0.7/dist/browser/signalr.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 7 \u0026lt;script\u0026gt; 8 // Establish SignalR Hub connection 9 const hubConnection = new signalR.HubConnectionBuilder() 10 .withUrl(\u0026#34;https://localhost:7013/messageHub/\u0026#34;) 11 .build(); 12 13 hubConnection.start() 14 .then(() =\u0026gt; { 15 console.log(\u0026#34;Connection started\u0026#34;); 16 }); 17 18 19 20 // Handle the Login button click event 21 function onLoginClick() { 22 // Get the user ID entered by the user 23 const userId = document.getElementById(\u0026#34;userIdInput\u0026#34;).value; 24 25 // Wrap the user ID in JSON format 26 const jsonData = { 27 \u0026#34;userId\u0026#34;: userId 28 }; 29 30 // Use the LoadUserInfo method of SignalR Hub to send the JSON data to the backend 31 hubConnection.invoke(\u0026#34;LoadUserInfo\u0026#34;, jsonData) 32 .then(() =\u0026gt; { 33 console.log(\u0026#34;Data sent successfully!\u0026#34;); 34 }) 35 .catch((error) =\u0026gt; { 36 console.error(error); 37 }); 38 } 39 40 41 // Handle the Send button click event 42 function onSendClick() { 43 // Get the user ID entered by the user 44 const userId = document.getElementById(\u0026#34;msgUserIdInput\u0026#34;).value; 45 // Get the message entered by the user 46 const msg = document.getElementById(\u0026#34;msgInput\u0026#34;).value; 47 48 // Use the SendToConnection method of SignalR Hub to send the data to another user 49 hubConnection.invoke(\u0026#34;SendToConnection\u0026#34;, userId, msg) 50 .then(() =\u0026gt; { 51 console.log(\u0026#34;Msg sent successfully!\u0026#34;); 52 }) 53 .catch((error) =\u0026gt; { 54 console.error(error); 55 }); 56 } 57 58 // Register events of the MessageHub 59 hubConnection.on(\u0026#34;sendToAllConnections\u0026#34;, function (msgs) { 60 console.log(\u0026#34;To All Connections:\u0026#34;, msgs); 61 }); 62 63 hubConnection.on(\u0026#34;StringDataTransfer\u0026#34;, (response) =\u0026gt; { 64 console.log(\u0026#34;Received Msg:\u0026#34;, response); 65 }); 66 67 68 \u0026lt;/script\u0026gt; 69\u0026lt;/head\u0026gt; 70\u0026lt;body\u0026gt; 71 SignalR TEST 72 \u0026lt;hr\u0026gt; 73 \u0026lt;label for=\u0026#34;userIdInput\u0026#34;\u0026gt;Please enter user IDÔºö\u0026lt;/label\u0026gt; 74 \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;userIdInput\u0026#34;\u0026gt; 75 \u0026lt;button onclick=\u0026#34;onLoginClick()\u0026#34;\u0026gt;Login\u0026lt;/button\u0026gt; 76 \u0026lt;hr\u0026gt; 77 \u0026lt;label for=\u0026#34;msgUserIdInput\u0026#34;\u0026gt;User IDÔºö\u0026lt;/label\u0026gt; 78 \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;msgUserIdInput\u0026#34;\u0026gt; 79 \u0026lt;label for=\u0026#34;msgInput\u0026#34;\u0026gt;msg:\u0026lt;/label\u0026gt; 80 \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;msgInput\u0026#34;\u0026gt; 81 \u0026lt;button onclick=\u0026#34;onSendClick()\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; 82 83\u0026lt;/body\u0026gt; 84\u0026lt;/html\u0026gt; Test Results and Conclusion During the test, you can open multiple web pages and use different userIDs when logging in. You can set breakpoint during the process to view the execution process.\nUser B logs in and sends a message to User A\rUser A\u0026#39;s interface, acknowledging receipt of the message\rThe above is a simple way to implement the instant messaging function using Web API. The process includes binding the user, connection ID and storing, and providing an API for instant messaging to a specific user. Although the front-end screen is a bit crude, it is mainly through testing to understand the principle. Then implement any instant function easily.\nThe project has been uploaded to Github.\n",
	  "pubDate": "2023-06-16T11:05:34+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/034en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/aspnetcore-6.0-signalr-websocket-1/",
	  "title": "Implementing Real-Time Communication with WebSocket using SignalR and .NET 6 Web API - Part 1",
	  "summary": "\u003cp\u003eCreating WebSocket Communication with .NET 6 Web API, This article documents examples of simulating broadcasting, group broadcasting, and individual broadcasting using .NET 6 Web API with a simple frontend.\u003c/p\u003e",
	  "content": "Creating WebSocket Communication with .NET 6 Web API, This article documents examples of simulating broadcasting, group broadcasting, and individual broadcasting using .NET 6 Web API with a simple frontend.\nForeword In our company\u0026rsquo;s projects, we used to integrate with Flask-SocketIO which developed by other colleagues, for real-time communication. However, our backend team primarily uses C#, and our manager has been wanting us to develop our own C# WebSocket communication for quite some time. As someone who is junior backend engineer, understanding the entire communication process (including the frontend) has been quite challenging. So, I decided to document my testing process.\nPrinciple WebSocket is a protocol that differs from the traditional API used for communication between front-end and back-end. It enables the establishment of a persistent, bi-directional connection. In traditional API requests, every time a client needs to fetch data from the server, it sends an HTTP request and waits for the server\u0026rsquo;s response. Such requests and responses are stateless, meaning they are independent of each other.\nWebSocket, on the other hand, allows for ongoing communication once the connection is established. Clients and servers can directly send data to each other without needing to reestablish the connection. This significantly reduces communication overhead, improves communication efficiency, and supports applications with high real-time requirements, such as multiplayer games, chat rooms, and real-time monitoring.\nSignalR SignalR is a library for the ASP.NET Core framework used to build real-time web applications. It allows server-side code to push new information to the WebSocket connections established with clients, enabling clients to receive this information in real-time. While ASP.NET Core also provides a WebSocket library, Microsoft recommends choosing SignalR for implementing real-time communication. This is because SignalR builds upon WebSocket connections and offers advanced features such as automatic reconnection, grouping, broadcasting, serialization, and more. With SignalR, it becomes easier to create efficient real-time web applications and reduces development complexity. Hence, SignalR is the chosen framework for this project.\nTest When I was initially assigned the task of implementing real-time communication, I must admit that it was quite challenging. After all, it operates differently from the traditional data transmission model. As a backend engineer, if you want to test APIs, you can use Swagger for testing. However, for real-time communication, testing involves checking the connection, message transmission, and disconnection. Since the frontend team is quite busy, I had to take on the testing myself. üòó\nIn addition to connection and disconnection testing, in terms of message transmission, it is crucial to note that all connections are made to the same server Hub. (A Hub is an abstract concept that describes a logical connection point between clients and the server.) Typically, connections with the same functionality are made to the same Hub. Therefore, it is vital to ensure that messages are sent from the same Hub to the specified clients. Sending the wrong message can be quite embarrassing. üòÇ\nNext, the message transmission will be tested for broadcasting to all clients, group broadcasting, and targeted message transmission.\nCreate Web API Project Use Visual studio to click Create a new project \u0026gt; ASP.NET Core Web API \u0026gt; choose a good project name \u0026gt; select .Net 6.0 \u0026gt; Create\nThe project structure is as follows: SignalR-Example/\r‚îú‚îÄ‚îÄ Connected Services/\r‚îú‚îÄ‚îÄ Dependencies/\r‚îú‚îÄ‚îÄ Properties/\r‚îÇ ‚îú‚îÄ‚îÄ launchSettings.json\r‚îú‚îÄ‚îÄ Controllers/\r‚îÇ ‚îú‚îÄ‚îÄ WeatherForecastController.cs\r‚îú‚îÄ‚îÄ appsettings.json\r‚îÇ ‚îú‚îÄ‚îÄ appsettings.Development.json\r‚îú‚îÄ‚îÄ Program.cs\r‚îú‚îÄ‚îÄ WeatherForecast.cs The next step is to add the SignalR library\nClick Tools above Visual studio \u0026gt; NuGet Package Manager \u0026gt; Manage NuGet Package for Solution\nDownload the SignalR NuGet package Create SignalR Hub Add Hub folder \u0026gt; Add IMessageHub.cs interface under Hub \u0026gt; Add a sendToAllConnections method to send messages to all connected users\n1namespace SignalR_Example.Hub 2{ 3 public interface IMessageHub 4 { 5 Task sendToAllConnections(List\u0026lt;string\u0026gt; message); 6 } 7} Add MessageHub.cs under the Hub folder\n1using Microsoft.AspNetCore.SignalR; 2namespace SignalR_Example.Hub 3{ 4 public class MessageHub: Hub\u0026lt;IMessageHub\u0026gt; 5 { 6 public async Task sendToAllConnections(List\u0026lt;string\u0026gt; message) 7 { 8 await Clients.All.sendToAllConnections(message); 9 } 10 } 11} Next, add a Controller that provides the push function, and select Add API Controller - Empty :\nMsgController.cs\n1using Microsoft.AspNetCore.Http; 2using Microsoft.AspNetCore.Mvc; 3using Microsoft.AspNetCore.SignalR; 4using SignalR_Example.Hub; 5 6namespace SignalR_Example.Controllers 7{ 8 [Route(\u0026#34;api/[controller]\u0026#34;)] 9 [ApiController] 10 public class MsgController : ControllerBase 11 { 12 private IHubContext\u0026lt;MessageHub, IMessageHub\u0026gt; messageHub; 13 public MsgController(IHubContext\u0026lt;MessageHub, IMessageHub\u0026gt; _messageHub) 14 { 15 messageHub = _messageHub; 16 } 17 [HttpPost] 18 [Route(\u0026#34;toAll\u0026#34;)] 19 public string ToAll() 20 { 21 List\u0026lt;string\u0026gt; msgs = new List\u0026lt;string\u0026gt;(); 22 msgs.Add(\u0026#34;Don\u0026#39;t forget, the deadline for submitting your expense reports is this Friday.\u0026#34;); 23 msgs.Add(\u0026#34;Friendly reminder, please refrain from using the conference room for personal calls or meetings without prior approval.\u0026#34;); 24 messageHub.Clients.All.sendToAllConnections(msgs); 25 return \u0026#34;Msg sent successfully to all users!\u0026#34;; 26 } 27 } 28} Added an API and injects the IHubContext included in SignalR to provide the function of sending messages to everyone.\nIn order to confirm whether the front end is connected to the Hub and the subsequent test message transmission, it is necessary to add an html page to the project (it can also directly avoid CORS):\nAdd wwwroot under the project, and add an html file under it.\nwwwroot is a special directory in ASP.NET Core for storing static resource files such as HTML, CSS, JavaScript, images, etc. In an ASP.NET Core application, users can access the files in this directory by directly entering the URL from the browser, because these files can be loaded directly from the web page request.\nAdd htmlpage.html, because it is mainly to realize the back-end function, the communicate information just displayed on the console.\n1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; 5 \u0026lt;title\u0026gt;SignalR TEST \u0026lt;/title\u0026gt; 6 \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/@microsoft/signalr@5.0.7/dist/browser/signalr.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 7 \u0026lt;script\u0026gt; 8 // Establish SignalR Hub connection 9 const hubConnection = new signalR.HubConnectionBuilder() 10 .withUrl(\u0026#34;https://localhost:7013/messageHub/\u0026#34;) 11 .build(); 12 13 hubConnection.start() 14 .then(() =\u0026gt; { 15 console.log(\u0026#34;Connection started\u0026#34;); 16 }); 17 // Register the \u0026#34;sendToAllConnections\u0026#34; event of the MessageHub 18 hubConnection.on(\u0026#34;sendToAllConnections\u0026#34;, function (msgs) { 19 console.log(msgs); 20 }); 21 \u0026lt;/script\u0026gt; 22\u0026lt;/head\u0026gt; 23\u0026lt;body\u0026gt; 24 SignalR TEST 25\u0026lt;/body\u0026gt; 26\u0026lt;/html\u0026gt; Next, we need to adjust the SignalR and CORS policy settings, as well as make some configurations for reading static data in Program.cs. Annotated items are newly added items\n1using SignalR_Example.Hub; 2 3var builder = WebApplication.CreateBuilder(args); 4 5// Add services to the container. 6builder.Services.AddSignalR(); // Register SignalR services 7 8builder.Services.AddCors(options =\u0026gt; // Register CORS services to allow cross-origin requests 9{ 10 options.AddPolicy(\u0026#34;CorsPolicy\u0026#34;, builder =\u0026gt; 11 { 12 builder.AllowAnyOrigin() 13 .AllowAnyHeader() 14 .AllowAnyMethod(); 15 }); 16}); 17builder.Services.AddControllers(); 18builder.Services.AddEndpointsApiExplorer(); 19builder.Services.AddSwaggerGen(); 20 21var app = builder.Build(); 22 23// Configure the HTTP request pipeline. 24if (app.Environment.IsDevelopment()) 25{ 26 app.UseSwagger(); 27 app.UseSwaggerUI(); 28} 29app.UseCors(\u0026#34;CORSPolicy\u0026#34;); // Use the defined CORS policy 30app.UseHttpsRedirection(); 31 32app.UseAuthorization(); 33 34app.MapControllers(); 35app.UseFileServer(); // Use built-in middleware to serve static files 36app.MapHub\u0026lt;MessageHub\u0026gt;(\u0026#34;/messageHub\u0026#34;); // Map SignalR Hub to \u0026#34;/messageHub\u0026#34; (connection string on the frontend) 37app.Run(); Final project structure\rConclusion Start the project, open swagger and https://localhost:7013/htmlpage.html to confirm whether the front end has received the message when calling the API.\nAfter entering the website, it will pop out that the connection has been successful.\rAfter calling toALL, a message will be displayed, and you can open multiple screens to confirm that you will receive the message.\rThe above implements the function of sending messages to all connected users.\nReference\nSignalR introduction and implementation using the.NET Core 6 Web API and Angular 14 ",
	  "pubDate": "2023-06-15T16:08:13+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/029en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/database-optimization-clustered-index-nonclustered-index/",
	  "title": "Database Query Performance Optimization Tips : Index Concepts, Clustered, Non-Clustered",
	  "summary": "\u003cp\u003eRecording tips for optimizing query performance through indexing, differences between clustered and non-clustered indexes.\u003c/p\u003e",
	  "content": "Recording tips for optimizing query performance through indexing, differences between clustered and non-clustered indexes.\nForeword When a table contains a large amount of data, even if the TSQL syntax is correct, there may be abnormal slowness during CRUD operations. This is closely related to the configuration of indexes. In fact, I believe that setting the right indexes from the beginning when designing the schema is essential and it\u0026rsquo;s important to cultivate good habits in this regard.\nIndex An index is a data structure used to accelerate query operations in a database. It is typically created on one or more columns of a table to facilitate the quick locating and retrieval of specific rows or data that meet certain conditions. In simple terms, it trades space for time. If there are well-defined indexes, when querying data, it can quickly retrieve the desired information through a B-tree structure, instead of scanning the entire table.\nClicking on the index of a table that can see the list of indexes.\rThe main types of indexes are Clustered Index, Non-Clustered Index, Unique Index, Primary Key Index, Clustered Index, and Full-Text Index.\nThis article primarily discusses:\nClustered Index: A clustered index determines the physical sorting order of data in a database. Each database table can have only one clustered index, which determines the physical storage order of the table.\nNon-Clustered Index: A non-clustered index is created outside of the clustered index. It consists of index keys and pointers to data pages, allowing for fast data retrieval. A database table can have multiple non-clustered indexes.\nClarification on a few questions. Is it better to have more indexes on a table? While having more indexes may improve query efficiency, it also increases disk space usage and can potentially slow down data modifications on the table.\nWhat type of index is PRIMARY KEY? When creating a PRIMARY KEY, if there is no clustered index already present, a clustered index will be automatically created. Therefore, further evaluation is necessary to determine if it is suitable.\nWhat type of index is FOREIGN KEY? Creating a FOREIGN KEY does not automatically create any index. If frequent JOIN operations are expected, it is recommended to create an index.\nIs a Clustered Index the same as a UNIQUE Index? A Clustered Index is not necessarily a UNIQUE Index.\nPrinciples of Index Selection The principle of index selection is to choose the most frequently queried and less repetitive (higher uniqueness) columns for indexing.\nCreating a Clustered Index 1CREATE CLUSTERED INDEX IX_ClusteredIndexName 2ON YourTableName (ColumnName); A clustered index affects the sorting order of data when it is physically stored in a table. The suitable fields for a clustered index include:\nFields used in WHERE conditions that involve searching for a large number of duplicate values, e.g., WHERE city=\u0026lsquo;Taipei\u0026rsquo;. Fields frequently used in ORDER BY clauses. Fields used in range queries, e.g., WHERE [time] BETWEEN \u0026lsquo;20230101\u0026rsquo; AND \u0026lsquo;20231231\u0026rsquo;. Fields commonly used in join clauses. In SQL Server, when creating a primary key index, it is by default a clustered index. When defining a primary key for a table, if the index type is not explicitly specified, SQL Server automatically creates the primary key index as a clustered index.\nUsing a GUID as a primary key can lead to performance issues due to the inherent randomness and lack of continuity in GUID values. Even if data is written sequentially, the GUID values for later-written data may appear before earlier ones. As a result, the physical storage location, governed by the characteristics of a clustered index, may require constant adjustments and reordering of existing data, leading to index fragmentation. This fragmentation can significantly impact both write and query performance. For more information, please refer to the provided resource What are the best practices for using a GUID as a primary key, specifically regarding performance?\nCreating a Non-Clustered Index 1CREATE INDEX index_name 2ON table_name (column_name); Creating a composite index with multiple columns.\n1CREATE INDEX idx_column1_column2 2ON your_table (column1, column2); Non-clustered indexes do not affect the sorting order of data when it is physically stored in a table.\nDisplay estimated execution plan(Ctrl + L) Regardless of whether indexes are created or not, when writing complex TSQL queries, you can also view the execution plan to identify which part of the query consumes the majority of the query performance.\nClicking this button will allow you to view the execution plan for each query.\rAfterward, you can click on the execution plan to view it in each query result.\rActually, I\u0026rsquo;m still studying and don\u0026rsquo;t fully understand highly complex queries either. However, in general, I can clearly determine whether the query is using an index seek or an index scan, which helps to provide a basic understanding of the query execution.\nClustered Index Seek and Clustered Index Scan Clustered Index SeekÔºö Searching with a clustered index is based on the key value of the index. The search operation utilizes the B-tree structure of the index, quickly navigating to the node containing the specific key value and directly accessing the required data pages. Clustered index search is typically an efficient access method, especially when the query conditions can effectively utilize the key value for filtering.\nIn general, if you see a clustered index seek, it indicates that the clustered index you have created is being used by the current query. However, if the query conditions do not align with the key value sorting order of the clustered index, the database engine may not be able to effectively utilize the clustered index and might resort to scanning the entire table or using other indexes (such as non-clustered indexes) to execute the query.\nClustered Index ScanÔºö Scanning with a clustered index involves traversing the entire clustered index to find the data that matches the query conditions. The scanning operation requires traversing the entire index structure, so it may take longer to execute, especially for larger tables or indexes containing a large amount of data. Clustered index scanning typically occurs in the following situations: when the query lacks effective filtering conditions, when retrieval of the entire table is needed, or when the index covers all the columns required for the query.\nIn summary, clustered index searching involves quickly locating specific data based on the key value, while clustered index scanning involves traversing the entire index to find the data that matches the conditions. Searching operations are usually more efficient, while scanning operations take longer to execute.If scanning operations consume a significant portion of query time, it may be necessary to use other indexes or redesign the indexing strategy to improve query performance.\nConclusion Index design is a challenge that every backend engineer is likely to encounter. While some companies have dedicated database administrators, it is still important for engineers to study and understand index design. I\u0026rsquo;m also making efforts to improve my knowledge in this area. Otherwise, it can be frustrating when your code seems perfectly written, yet the performance is unexpectedly slow, only to realize that it\u0026rsquo;s actually a database issue.ü§£\n",
	  "pubDate": "2023-06-13T13:21:06+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/036en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cryptocurrency/exchange-recommendations-value-investing-insights/",
	  "title": "Cryptocurrency Exchanges : User Experience, Value Investing, Diversification of Risk Perspective",
	  "summary": "\u003cp\u003eShare some cryptocurrency exchanges (Binance„ÄÅBybit„ÄÅPionex„ÄÅOKX„ÄÅBitget) that I have personally used, along with some investment insights, to help beginners avoid pitfalls.\u003c/p\u003e",
	  "content": "Share some cryptocurrency exchanges (Binance„ÄÅBybit„ÄÅPionex„ÄÅOKX„ÄÅBitget) that I have personally used, along with some investment insights, to help beginners avoid pitfalls.\nQuick tour\n‚ûû Exchange User Experience and Recommendations\n‚ûû Recommended Taiwan exchanges: TWD deposit channels\nForeword I\u0026rsquo;ve been in the cryptocurrency space for nearly two years now and have experienced the USDT delisting and the bankruptcy of FTX, resulting in a loss of one-third of my assetsüòì. As a software engineer, my faith in this industry remains unchanged, and I continue to use cryptocurrency exchanges for investment purposes. In this article, I will share my experiences with the exchanges I have used and discuss some of the lessons and insights I have gained from learning to invest.\nInvestment concept 1. Value Investing: The Difference Between Investment and Speculation\nFirst of all, let me explain that I adhere to the philosophy of value investing. In my view, \u0026ldquo;investment\u0026rdquo; means believing in the future growth of an industry, company, or individual and providing financial support to them while aligning our fates together.\nOf course, in reality, it is not as simplistic as symbiotically sharing life and death. When an investment project is on the brink of failure, there are rumors of a company\u0026rsquo;s lack of competitiveness, or when the overall situation becomes unstable, it is common to take actions to redeem funds. This is what I consider arbitrage or speculation.\nTo increase one\u0026rsquo;s own assets, some level of speculation is inevitable. However, I do not recommend engaging in excessive speculation, such as leveraging or short-term trading (although it can be entertaining to dabble in them). After all, the purpose of my investments is to have something to do in my leisure time, rather than taking on an additional job or making it my primary occupation (e.g., sneaking glances at the market during work and continuing to monitor it after work or be a professional trader).\n2. Investment Method : DCA (Dollar Cost Averaging)\nCurrently, in the cryptocurrency field, I consider the strategy of Dollar-Cost Averaging (DCA). If you\u0026rsquo;re unfamiliar with this term, you can search for it yourself as I won\u0026rsquo;t provide a detailed tutorial here. üòé Essentially, it\u0026rsquo;s akin to long-term investing in the stock market. However, due to the cyclical nature of the cryptocurrency market, while I continue with DCA, I also engage in arbitrage (speculation) by selling at high points.\n3. Diversifying Risk: How to Spread Your Investments\nActually, I consider myself a novice investor as well. Initially, I didn\u0026rsquo;t see the need to diversify risks. Like many young people, I was full of confidence and would go all-in on whatever project caught my attention. This was a grave mistake. Often, people have to experience losses before they learn their lesson. That\u0026rsquo;s why I recommend starting to invest at a young age when potential losses are relatively smaller. Gaining valuable experience with a smaller amount of money is far better than sailing smoothly through life. There are some people only to realize the importance of risk diversification at the age of retirement, when they encountered Black Swan and all retirement funds are wiped out.\nUsing the example of the FTX bankruptcy incident, I had a basic understanding of risk diversification. At that time, I had spread my assets across three different exchanges, which ultimately resulted in a loss of only one-third of my investments. Currently, I have my funds distributed across approximately five different platforms, including a cold wallet. (XD)\nCold wallet (Hardware wallet) Currently, the safest way to store cryptocurrencies is through a cold wallet. As for the detailed principles behind it, I recommend conducting your own search.\nSimply put, owning a cold wallet for your cryptocurrencies is the only true ownership. Storing your assets on an exchange means they are actually held in the exchange\u0026rsquo;s own cold wallets. If the exchange were to go bankrupt or run away, you would not be able to retrieve your funds.\nUsing the real-world example of banks, the money deposited in a bank is not entirely yours until you withdraw and keep it in your own pocket. The perceived sense of security comes from the fact that governments provide guarantees, assuring you that you can redeem your money at any time. However, it\u0026rsquo;s important to carefully consider why the government is willing to provide that guarantee. If a situation arises where the government doesn\u0026rsquo;t benefit from honoring the guarantee (e.g., during times of war or bankruptcy), do you think you would still be able to retrieve your money?\nHowever, due to the high transaction fees involved in transferring funds from exchanges to cold wallets, I, as someone with limited assets, do not frequently transfer cryptocurrencies. Instead, I keep them stored in exchanges to earn some interest. (After all, you wouldn\u0026rsquo;t constantly incur withdrawal fees from your bank, would you?)\nOn the other hand, for whales (wealthy individuals) whose assets far exceed the transaction fees, and whose profits from each trade greatly outweigh the interest and fees, they typically transfer funds from their cold wallets to exchanges only when they intend to make a trade, and then transfer them back to their cold wallets once the trade is completed.\nFeel free to click on the cold wallet link to make a purchase if you\u0026rsquo;re interested. !!It is essential to purchase a cold wallet through the official website and have it delivered directly from the official source. There are cybersecurity risks associated with purchasing from third-party platforms.\nBy clicking on the following link, both of us will receive an equivalent of $10 worth of BTC upon purchase. Home of the first and only certified Hardware wallets\rLedger offers certified crypto asset hardware wallets bringing optimal protection level to your bitcoins, ethereums, XRP and more - without sacrificing usability or control.\nshop.ledger.com\rPersonally, I believe purchasing the LEDGER NANO S PLUS, the basic version, is enough. It includes all the necessary features, except for Bluetooth functionality.\nExchange selection method CoinMarketCap is a globally recognized cryptocurrency market information platform that provides comprehensive and accurate data on cryptocurrency markets, prices, market capitalization, and trading volumes. Its website enables users to track market dynamics of various cryptocurrencies, view price trend charts, market rankings, and trading pairs. Additionally, CoinMarketCap offers detailed information pages, news articles, and educational resources about cryptocurrencies to help users understand market trends, technical specifications, and investment strategies.\nSo, when you hear about an exchange or a new cryptocurrency, you can often start by checking CoinMarketCap - Top Cryptocurrency Spot Exchanges for relevant information. The choice of exchange is evaluated based on factors such as spot trading volume, derivatives trading, lending, and other rankings provided on the website.\nLet me explain my selection criteria. Basically, I consider factors such as scale, trading volume, and company\u0026rsquo;s tenure. A longer tenure signifies experience gained over time, while a large trading volume indicates a company that is making profits. Although a large scale, high trading volume, and long tenure do not guarantee absolute safety since the money is still entrusted to others, the underlying concept is that profitable companies usually have better systems, higher salaries, and skilled engineers, which significantly reduces security risks. However, exceptions can be made for exchanges like FTX, where the CEO\u0026rsquo;s actions have been questionable. In general, there is usually no compelling reason to choose a small, relatively unknown exchange.\nExchange User Experience and Recommendations Next, I recommend several exchanges that I am still using.\nBinance Friends Referral Program\rGet a 100 USDT cashback voucher when you refer a friend. T\u0026amp;C apply.\nwww.binance.com\rBinance, founded by Changpeng Zhao in 2017, is the world\u0026rsquo;s largest cryptocurrency exchange platform. It provides a platform that combines digital asset trading, investing, and financial services, allowing users to easily buy, sell, and exchange various cryptocurrencies.\nBinance currently holds the top position in terms of spot trading and derivatives trading volume. It also weathered the panic and run on assets following the bankruptcy of FTX, demonstrating that it is indeed a resilient company.\nUser Experience :\nBinance was the exchange I started using after entering the cryptocurrency space, and compared to others, its interface and operations are smooth. It offers a variety of financial products, and the process of staking and earning interest is quite straightforward. Recently, they even introduced features like a dollar-cost averaging bot and products related to index investing, which can be considered at the forefront of the industry. It seems that not using Binance in the crypto world might surprise some people.ü§£\nBybit https://www.bybit.com/invite?ref=WNMQMX\rBybit is a globally renowned cryptocurrency derivatives trading platform, established in 2018. It focuses on providing derivative trading services for Bitcoin and other cryptocurrencies, including perpetual contracts (which are contracts without an expiration date) and futures contracts.\nUser Experience :\nIt is evident that Bybit is an exchange that initially focused on futures contract trading. It holds the third position in derivatives rankings on CoinMarketCap. Experienced traders who engage in futures contracts generally find the trading interface and smoothness of Bybit satisfactory. Although its spot trading ranking may not be as high, the trading volume of derivatives remains at a certain level. Personally, I have invested in BitDAO, a project sponsored and promoted by this platform, and have staked Bit (soon to be renamed Mantle) on Bybit. Aside from contract trading, I believe that the spot financial services on Bybit are not as smooth as those on platforms like Binance and OKEx. Improvements in this aspect could potentially attract more retail investors rather than solely catering to contract traders.\nPionex Bitcoin Ethereum Auto buy low and sell high\rPionex is the best crypto trading bot currently available, 24/7 trading automatically in the cloud. Easy to use, powerful, and extremely safe. Trade your cryptocurrency now with Pionex trading robot, the automated crypto trading bot.\nwww.pionex.com\rPionex offers a variety of features and tools to cater to the investment needs of different users. The platform provides an intuitive user interface and user-friendly functionalities, making it easy for both novice and experienced traders to engage in trading activities. Security is a top priority for Pionex, and it implements advanced security technologies and measures to safeguard user assets and personal information. Additionally, Pionex offers automated trading bots to assist users in executing trading strategies automatically, enhancing trading efficiency. Whether you are an investor or a trader, Pionex provides a comprehensive and reliable platform that allows you to participate in the global digital asset market and achieve your investment goals.\nUser Experience :\nPionex is considered the pioneer of quantitative trading exchanges, offering a wide range of quantitative trading bots. It is suitable for individuals without a professional background in trading who wish to engage in quantitative trading. It also caters to those who aim to mitigate the risks associated with emotional trading, such as panic selling or chasing price highs.\nPersonally, I believe that quantitative trading is a profitable endeavor. After all, if we compare making money or losing money to gambling, the more times you gamble, the closer your win rate will be to 50%. If you can rely on certain indicators to make judgments, as long as your win rate exceeds 50% in the long run, you will make money. Currently, I have participated in a Haze 800-grid ETH/BTC spot trading strategy on Pionex, and I am curious to see how much profit it will generate when the market enters a bullish phase.üòé\nOKX Register in OKX\rSign up and log in OKX App to claim a Mystery Box up to $10,000.\nwww.okx.com\rOKEx is a globally leading digital asset exchange platform established in 2017, operated as a subsidiary of OKCoin. The platform provides cryptocurrency and digital asset trading services worldwide and continuously strives for security, stability, and innovation. CEO Jay Hao plays a crucial role in driving the development of OKEx.\nAs a significant participant in the cryptocurrency field, OKEx offers trading services for a wide range of cryptocurrencies globally. It also provides advanced features such as contract trading and options trading, enabling users to implement various investment strategies. OKEx prioritizes security and implements multiple layers of identity verification and state-of-the-art security technologies to protect user assets and personal information. Whether you are a novice or an experienced trader, OKEx offers a reliable and convenient platform to participate in the global digital asset market.\nUser Experience :\nConsidering its establishment time, OKEx has also proven to be a seasoned exchange that has withstood the test of time. However, it was only after using Binance, Bybit, and Pionex that I decided to give OKEx a try. From my experience, I find OKEx to have the smoothest user interface among the exchanges I\u0026rsquo;ve used. From an engineering and UI/UX perspective, it is truly impeccable. Therefore, recently, I have started allocating some of my assets to OKEx as well.\nBitget Sign up with Bitget and earn 5,005 USDT rewards\rCreate an account with the world\u0026#39;s largest crypto copy trading platform and enjoy 5,005 USDT rewards.\nhttps://www.bitget.com/en/referral/register?from=referral\u0026amp;clacCode=NNQX2LFB\rBitget offers a variety of cryptocurrency and digital asset trading services, including derivatives trading. As an innovative blockchain financial platform, Bitget provides a secure, efficient, and innovative trading environment, allowing users to easily participate in the global digital asset market. The platform supports trading for multiple cryptocurrencies and offers advanced tools such as contract trading, options trading, and leverage trading to cater to the diverse investment needs of users. Bitget prioritizes user experience by providing an intuitive and user-friendly interface. It continuously optimizes and enhances its user interface and functionalities to ensure users can conveniently and swiftly carry out their trading operations.\nUser Experience :\nBitget is considered one of the emerging exchanges that focuses on copy-trading systems. This system allows users to effortlessly profit by replicating trades made by professional traders. While personally, I prefer not to entrust my trades to others, I have observed that the platform provides various preventive measures and implements a reward-penalty mechanism for the copy-trading system. I believe that by investing more effort in finding skilled traders, it is possible to make profits. However, it is important to note that copy-trading is not a foolproof way to make money, and having strong risk management abilities is crucial.\nSometimes, it\u0026rsquo;s fun to invest a small amount of money in copy-trading or automated trading bots, and I believe it\u0026rsquo;s something that people in the cryptocurrency community are inclined to try. Currently, I\u0026rsquo;m also impressed by Bitget\u0026rsquo;s rapid growth in derivatives trading volume, placing it among the top five in a short period. I believe there is great potential, so I have invested some of my assets in Bitget\u0026rsquo;s platform token, BGB, to earn additional rewards through staking on the platform.\nRecommended Taiwan exchanges: TWD deposit channels The most cost-effective way to deposit funds in Taiwan currently is by registering with a local exchange, linking it to a bank account for TWD transfers, and then converting the TWD to USDT using the TWD/USDT trading pair on the exchange. Due to liquidity concerns, users typically transfer the USDT to large foreign exchanges for trading purposes. They do not engage in cryptocurrency trading on the local exchange.\nCurrently, there are several local exchanges in Taiwan such as MAX, ACE, BitoPoint, and Rybit that are being used and considered user-friendly. It is recommended for beginners to register on these platforms simultaneously so that they can compare the exchange rates offered when purchasing USDT. They can then choose the exchange that offers the best rate at the time of their purchase.( !! Please note that there may be transfer fees when sending USDT to foreign exchanges, and the discounts offered by each exchange may vary.)\nMAX max.maicoin.com\rACE Âè∞ÁÅ£È¶ñÂÆ∂ÂêàÊ≥ïÂêàË¶èËôõÊì¨Ë≤®Âπ£‰∫§ÊòìÊâÄ\rACE ÁéãÁâåËôõÊì¨Ë≤®Âπ£‰∫§ÊòìÊâÄÊèê‰æõÊñ∞Âè∞Âπ£Ë≥ºË≤∑ËôõÊì¨Ë≤®Âπ£Ôºå‰æãÂ¶ÇÊØîÁâπÂπ£ÔºàBTCÔºâ„ÄÅ‰ª•Â§™Âπ£ÔºàETHÔºâ„ÄÅUSDTÔºàÊ≥∞ÈÅîÂπ£ÔºâÁ≠â‰∏ªÊµÅÂπ£Á®ÆÔºå24Â∞èÊôÇÈö®ÊôÇÂèØ‰ª•Ë≤∑Ë≥£ÔºåÂä†‰∏äÂá±Âü∫ÈäÄË°åÊñ∞Âè∞Âπ£ÂÉπÈáë‰ø°Ë®óÔºåÂÆâÂÖ®Êúâ‰øùÈöú\nace.io\rUser Experience :\nThere\u0026rsquo;s not much else to say. Registering is a must for depositing Taiwanese dollars, and I mostly use MAX exchange for deposits. The experience has shown that MAX tends to have a slightly faster transfer speed when it comes to transferring funds.\nConclusion The emphasis on user experience while discussing various exchanges is because, despite how impressive or secure the exchanges may be, it is important to acknowledge that cryptocurrencies are primarily built on decentralized principles. Centralization poses risks, so storing assets on exchanges carries inherent risks. Currently, the safest method is to store one\u0026rsquo;s assets in a cold wallet.\nFeel free to click on the cold wallet link to make a purchase if you\u0026rsquo;re interested. !!It is essential to purchase a cold wallet through the official website and have it delivered directly from the official source. There are cybersecurity risks associated with purchasing from third-party platforms.\nBy clicking on the following link, both of us will receive an equivalent of $10 worth of BTC upon purchase. Home of the first and only certified Hardware wallets\rLedger offers certified crypto asset hardware wallets bringing optimal protection level to your bitcoins, ethereums, XRP and more - without sacrificing usability or control.\nshop.ledger.com\rPersonally, I believe purchasing the LEDGER NANO S PLUS, the basic version, is enough. It includes all the necessary features, except for Bluetooth functionality.\nHowever, in practice, it is not always possible to achieve perfection, and there is always a trade-off between convenience and security. Additionally, it is now common for investors to hold platform tokens (e.g., BNB, BGB, BIT) as they are integral to the survival of the platforms themselves. These platform tokens are typically stored on various platforms since they are intertwined with the platforms\u0026rsquo; activities. Keeping them on an exchange allows for participation in platform events and potential earnings. However, if an exchange goes bankrupt, even if the tokens are withdrawn, they may not be of much use. Moreover, individual retail investors may not be able to afford transfer fees for every transaction and instead tend to transfer to cold wallets once they have accumulated a certain amount. Hence, the choice of exchange becomes crucial.\nDiversifying assets across several exchanges that you consider to be promising and transferring long-term holdings to a cold wallet is currently the most reliable approach in my opinion. If there comes a day when all exchanges indeed collapse, then I would have to accept that reality as well.üòé\nThe previous generation relied on real estate, and it seems that this generation will have to rely on cryptocurrencies. I wish everyone success in achieving financial freedom at an early stage.\n",
	  "pubDate": "2023-05-29T14:17:01+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/033en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/hokkaido/sapporo-dormyinn-kanihonke/",
	  "title": "Hokkaido-Sapporo Travel Diary: Dormy Inn Accommodation and Sapporo Kani Honke Experience",
	  "summary": "\u003cp\u003eMy travel diary in Hokkaido-Sapporo included a two-night stay at Dormy Inn, exploring the underground shopping streets, visiting Nakajima Park and Odori Park, indulging in Sapporo Kani Honke experience, and unfortunately missing out on Genghis Khan barbecue.\u003c/p\u003e",
	  "content": "My travel diary in Hokkaido-Sapporo included a two-night stay at Dormy Inn, exploring the underground shopping streets, visiting Nakajima Park and Odori Park, indulging in Sapporo Kani Honke experience, and unfortunately missing out on Genghis Khan barbecue.\nForeword After leaving Hakodate, I headed to Sapporo with the main purpose of shopping and experiencing local restaurants and the everyday areas where locals hang out. I didn\u0026rsquo;t include any city landmarks or observation decks in my itinerary, as I wanted to focus on a more casual exploration. Since my trip was quite spontaneous and I didn\u0026rsquo;t do much planning in advance, I asked my Japanese friends from language class for some recommendations on must-try foods in Sapporo. They suggested Genghis Khan grilled lamb, but unfortunately, I missed out on it because I didn\u0026rsquo;t make a reservation. I realized I was being silly as even in Taiwan, there are many restaurants where you need reservations. How could I have thought Japan would be any different\u0026hellip;\nReturning the car, JR New Chitose Airport Station ‚ûû Sapporo Station First, after leaving Hakodate, I returned the rental car at the original rental location, which was about a 15-minute drive from New Chitose Airport. After that, it was quite convenient to travel from New Chitose Airport to Sapporo using the JR railway. By taking the JR Rapid Airport train, it only took 40 minutes to reach Sapporo Station.\nAt the beginning, there were relatively few people, but as we got closer to Sapporo, the number of people increased.\nTanukikoji Shopping Street and Dormy Inn Sapporo ANNEX accommodation After arriving in Sapporo, I headed to my accommodation, Dormy Inn Sapporo ANNEX, located in the 6th block of Tanukikoji Shopping Street. The hotel is conveniently located, with Sapporo Station just a 10-minute walk away. There are also nearby bus and subway stations, making it easy to access various attractions and commercial areas within the city.\nIn addition to the essential facility of hot springs, the hotel also provides complimentary popsicles and Yakult. The popsicle flavors were quite to my liking, even though the temperature in early May in Hokkaido was only around 10 degrees Celsius üòÇ. As for Yakult, it\u0026rsquo;s the same type that you can find in Taiwan, but the bottles in Japan always have labels written in Japanese, mentioning various beneficial bacteria and cultivation. It\u0026rsquo;s particularly intriguing and makes me want to give it a try, haha.\nThe Yakult provided by the hotel.\rThe breakfast at the hotel has been consistently good, and based on my experiences in Noboribetsu and Hakodate accommodations, I highly recommend this hotel in terms of quality.\nBreakfast\rStrolling through the underground shopping street, Nakajima Park, and Odori Park. I started my day by mingling with the locals in the underground shopping street! üòé I must say, the temperature outside was really chilly for someone like me from southern Taiwan üòÇ. If I wanted to get to a specific destination, I definitely opted for the underground street as it provided shelter from the cold.\nWell, Everyone was walking quite fast.\rNakajima Park\rNakajima Park\rOdori Park\rI suggest that you visit Sapporo TV Tower, where you can enjoy a panoramic view of Odori Park.\rThese attractions are mainly for spending time with family, strolling around, taking leisurely walks, and having conversations.\nSapporo Kani Honke Next, it was time to try the famous local crab restaurant. Personally, I wasn\u0026rsquo;t particularly interested in crab, but my older brother is a big fan. He had heard great recommendations from his colleagues and insisted on making an early reservation for us to try it out. I must say, I found the restaurant\u0026rsquo;s decor and architecture to be incredibly grand and elegant, with a distinct Japanese style.\nAs for the portion size, I found it to be a bit small, and my mom kept comparing it to seafood restaurants in Tainan, Taiwan üòÇ. However, what stood out was the service of having Japanese staff dressed in kimono, delicately arranging and cooking the dishes. I personally enjoyed the ambiance and experience, but if we consider the value for money, it might be helpful to refer to the opinions of those who truly appreciate crab cuisine. After all, it\u0026rsquo;s possible that the ingredients were of top-notch quality, but I just couldn\u0026rsquo;t discern it myself üòé.\nHokkaido University After having a satisfying meal, I made a special visit to Hokkaido University. Established in 1876, it is one of the oldest universities in the Hokkaido region and one of the seven earliest imperial universities in Japan. I happened to visit during the afternoon when the first classes were starting, and I saw many university students riding bicycles to make it on time for their classes.\nObserving the scenery of these boreal coniferous forests, I couldn\u0026rsquo;t help but wonder if students at Hokkaido University ever skip classes because of the extreme cold during winter üòÇ.\nI initially thought about trying to join the university students for their cafeteria meals, but in the end, I decided to go and try other delicious food instead.\nThe missed opportunity to visit the original Genghis Khan restaurant, \u0026ldquo;Sapporo Jingisukan Honten.\u0026rdquo; Sapporo Genghis Khan is a renowned restaurant famous for its grilled lamb dishes. This restaurant is widely popular for its authentic Genghis Khan-style barbecue.\nGenghis Khan barbecue is a grilled dish primarily made with thinly sliced lamb. At the restaurant, you can sit in front of a unique charcoal grill and enjoy the experience of grilling the lamb yourself. This cooking method helps maintain the tenderness and texture of the meat while allowing the flavors to fully infuse, resulting in a rich and delicious taste.\nThis particular restaurant is one I personally wanted to try, as it came highly recommended by my Japanese friends. In Taiwan, I haven\u0026rsquo;t had the chance to try a lamb-focused barbecue restaurant. However, the downside is that popular restaurants like this usually require reservations, as they tend to be fully booked. One thing I found not quite satisfactory was the ventilation system, which seemed inadequate and resulted in smoke and fumes from the grilling. It could be due to the older facilities in the original store or other factors. If you\u0026rsquo;re sensitive to the smoke, it might be better to choose another establishment.\nConclusion Arriving in Sapporo marks the final leg of this trip, and soon it will be time to board the plane back to Taiwan. Honestly, the most enjoyable meals have been the ones provided by the hotel. Perhaps it was due to not making restaurant reservations in advance that I missed out on trying the local delicacies. However, it\u0026rsquo;s also possible that my taste buds have become too picky from the food back in Taiwan üòÇ. If I have the opportunity to visit again, I\u0026rsquo;ll definitely plan my itinerary better and try the everyday local cuisine that the locals enjoy. Experiencing the local way of life is what truly makes a trip worthwhile.\nI must say, Japanese health supplements are really amazing. The digestive medicine I bought this time has significantly improved my digestive system. After returning to Taiwan, I can feel a noticeable difference. Looks like I\u0026rsquo;ll have to allocate some extra budget for regular purchases of Japanese products in the future üòé.\nReference\nDormy inn sapporo annex Sapporo Kani Honke Sapporo Genghis Khan (Main Shop) ",
	  "pubDate": "2023-05-10T08:29:57+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/032.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/hokkaido/hakodate-attractions-lodging/",
	  "title": "Recommended attractions in Hakodate, Hokkaido and review of Hakodate Hotel Banso",
	  "summary": "\u003cp\u003eRecommendations for attractions in Hakodate, Hokkaido, and the lodging experience at the Hotel Banso.\u003c/p\u003e",
	  "content": "Recommendations for attractions in Hakodate, Hokkaido, and the lodging experience at the Hotel Banso.\nForeword After leaving Noboribetsu, the next destination is Hakodate. This is my second time visiting Hakodate. During my previous visit, I experienced the Hakodate Morning Market, the night view from Mount Hakodate, Goryokaku Tower, and the Kanemori Red Brick Warehouse. These are all excellent attractions, and I highly recommend them to first-time visitors to Hakodate. However, this time, due to a tight schedule, I will only be staying in Hakodate for one night, so I didn\u0026rsquo;t plan many activities. My main plan is to relax in the hotel\u0026rsquo;s hot springs, take a leisurely walk on Hachiman-zaka, and explore the nearby Hakodate Hokkoku Shrine.\nHachiman-zaka Hachiman-zaka is an ancient street located in Hakodate, Hokkaido, Japan, and it is one of the iconic tourist attractions in the city. The name of this slope comes from the nearby Hachiman Shrine, which is considered the guardian deity of Hakodate. The starting point of Hachiman-zaka is near Hakodate Station, and it has a steep incline, extending all the way up the slope. Along the street, cherry trees are planted on both sides, and during the spring season when the cherry blossoms are in full bloom, Hachiman-zaka becomes a popular spot for cherry blossom viewing, attracting a large number of visitors.\nDuring my previous visit, I took the opportunity to ride the cable car up the mountain and enjoy the night view.\rWalking up Hachiman-zaka, you can admire the beautiful scenery of Hakodate city. Along the slope, there are many old and well-preserved historical buildings, including traditional Japanese-style architecture and Western-style buildings, giving a nostalgic feeling.\nAt the top of Hachiman-zaka is the cable car station for Mount Hakodate, where visitors can take the cable car to the summit and enjoy even more breathtaking views, including the cityscape of Hakodate, the harbor, and the nearby mountains. The atmosphere differs between daytime and nighttime visits.\nThe street of Hachiman-zaka.\rHakodate Hokkoku Shrine While walking along Hachiman-zaka, you can also visit Hakodate Hokkoku Shrine. Established in 1879, it was built to honor the Japanese soldiers who died in battle. The shrine is considered the guardian shrine of Hakodate city and is an important attraction for local residents and tourists. During my previous visit, I didn\u0026rsquo;t have much of an impression, but this time, I felt a sense of unusual antiquity and neglect in its maintenance.\nTorii\rThe torii gate is located at the entrance of Hakodate Hokkoku Shrine and symbolizes the symbolic act of meeting with the divine spirits upon entering the shrine. It serves as a reminder to respect and revere the sacred, while also representing reverence and remembrance for the fallen soldiers. Additionally, the torii gate serves as a recognizable symbol of the Hokkoku Shrine, making it unique and eye-catching in the cityscape of Hakodate. When visitors see the red gate, they naturally associate it with the presence of a shrine and become intrigued.\nDuring this visit, I didn\u0026rsquo;t have much interest in exploring the shrine itself. I simply found the positioning of the torii gate, which conveniently separates the interior of the shrine from Hachiman-zaka, with its long staircase, a great photo opportunity.\nHakodate Hotel Banso I chose to stay at the Hakodate Hotel Banso this time, and I found it to be perfect in terms of its hot springs, dining options, location, and price.\nI noticed that many Japanese people, upon checking into a hotel, would immediately choose to go for hot springs. So, when it came to having dinner, they would typically be dressed in the yukata provided by the hotel. It felt a bit out of place at first because we were all dressed in casual attire for dinner.\nThe hotel\u0026#39;s dinner\rThe attire provided for bathing in Japanese hotels\u0026rsquo; hot springs is called a \u0026ldquo;yukata.\u0026rdquo; The yukata is a traditional Japanese garment typically made of lightweight cotton or linen fabric, perfect for use in hot springs or during the summer.\nYukatas are often single-colored or patterned and feature wide sleeves and cuffs. They are tied around the waist with a sash or belt called an \u0026ldquo;obi.\u0026rdquo; Wearing a yukata provides a comfortable and refreshing feeling while also allowing one to experience the traditional style of Japan.\nYukatas are not only provided in Japanese hot spring hotels but are also commonly seen during summer festivals, tourist attractions, and traditional events. Many people enjoy wearing yukatas to participate in activities or stroll through onsen towns, showcasing their love for Japanese culture.\nWhen staying in a Japanese hotel and enjoying the hot springs, yukatas are typically provided for use. You can put one on and experience Japan\u0026rsquo;s hot spring culture while immersing yourself in a unique atmosphere of leisure and relaxation. However, in the end, I personally prefer to wear the yukata only when going to soak in the hot springs.üòÇ\nThe hotel\u0026#39;s breakfast\rOverall, I genuinely feel that this accommodation experience was excellent, and I would recommend this hotel to anyone planning to visit Hakodate.\nHakodate Tropical Botanical Garden Located in the Yunokawa Onsen area, the Hakodate Tropical Botanical Garden is one of my personal favorite attractions. It primarily showcases a variety of tropical plants, which may not be particularly surprising for someone from Taiwan like me. The botanical garden also features a greenhouse that provides a constant temperature and humidity, allowing tropical plants to thrive in Hokkaido\u0026rsquo;s cold climate. Inside the greenhouse, visitors can explore a diverse range of tropical plant species, including cacti, orchids, and rainforest trees, creating an atmosphere reminiscent of being in a tropical region.\nHowever, one incredibly unique sight in the Hakodate Tropical Botanical Garden is the opportunity to observe Japanese macaques (snow monkeys) soaking in hot springs! The botanical garden houses many Japanese macaques, and during the winter, the water pools in their enclosures are filled with hot springs. It is quite amusing to witness these monkeys, just like humans, enjoying themselves in the warm water. It has become one of the fascinating and rare scenes during the winter season. Since I visited in early May, I was still able to see the macaques. However, as the weather gradually becomes hotter, it becomes less likely to observe this phenomenon. If your intention is to witness the macaques bathing in the hot springs, it would be best to visit during the winter season.\nConclusion This visit to Hakodate was quite brief, with just a one-night stay, which didn\u0026rsquo;t allow for a thorough exploration of every attraction. Personally, I recommend staying for at least two nights to fully experience the morning market and enjoy the night view from Mount Hakodate. However, if you have the opportunity to choose a quality hot spring hotel, indulging in a luxurious seafood experience can also be a fantastic option.\nReference\nHakodate Tropical Botanical Garden Hotel Banso ",
	  "pubDate": "2023-05-07T08:29:57+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/031.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/hokkaido/noboribetsu-jigokudani/",
	  "title": "Self-driving attractions in Hokkaido: Noboribetsu Onsen Jigokudani, Izumi Park, Yunokawa Shrine, and Enmado",
	  "summary": "\u003cp\u003eIntroducing my favorite attraction, renowned for its spectacular volcanic landscape, geysers, hot springs, and the smell of sulfur - Noboribetsu Onsen Jigokudani.\u003c/p\u003e",
	  "content": "Introducing my favorite attraction, renowned for its spectacular volcanic landscape, geysers, hot springs, and the smell of sulfur - Noboribetsu Onsen Jigokudani.\nForeword It has been approximately 10 years since my last visit to Japan. Normally, when I have the opportunity to travel, I don\u0026rsquo;t usually consider going to Japan because I feel it\u0026rsquo;s too close to Taiwan, and I can always visit even when I\u0026rsquo;m older. Therefore, I always prefer to go to more distant places. However, for this family trip, considering everyone\u0026rsquo;s time and preferences, we decided to embark on a self-driving journey in Hokkaido. Interestingly, my previous visit to Japan was also to Hokkaido.\nItinerary planning This time, considering factors such as time constraints, car rental return fees, and personal preferences, we planned our itinerary starting from New Chitose Airport. After renting a car, we will head directly to Noboribetsu City (1 night) ‚ûû Hakodate City (1 night) ‚ûû and then return to New Chitose Airport to drop off the car before staying in Sapporo for two days and the end returning to Taiwan. I personally feel that the itinerary is quite tight. For those who enjoy leisurely exploration or visiting specific spots, it would be advisable to allocate more time. After all, most of the time on this trip will be spent driving, but our main purpose is to spend time with family and avoid exhausting the elderly with too much transportation. Additionally, we also want to enjoy the scenery along the way.\nArrival at New Chitose Airport ‚ûû Car rental Upon arrival at the airport, you can follow the signage to locate the car rental area. Perhaps due to the pandemic or staff reduction, there were no personnel present at each counter, but instead, there were phones provided for contacting the responsible individuals.\nWe had made a prior reservation, expecting to pick up the car directly at the airport. However, we had to wait for 15 minutes for a representative from the car rental company to arrive at the airport and drive us to their rental office to get the car.\nAfter collecting the car and completing a series of pre-check procedures, we were ready to hit the road. Our next destination was Noboribetsu City.\nWell, it\u0026#39;s time to embark on our first self-driving adventure in Hokkaido!\rAccommodation: Manseikaku Hotel, Noboribetsu. Upon arrival, it was already evening, so we checked into the hotel first. After having dinner, we enjoyed a relaxing soak in the hot springs. Tomorrow, we would explore the area.\nNoboribetsu Shopping Street\rRegarding the attractions in Noboribetsu, since the Jigokudani (Hell Valley) has always been at the top of my list, I chose to stay near the Jigokudani itself. This area is essentially a resort, and our itinerary revolves around enjoying the hot springs, exploring the Jigokudani, and strolling through the Noboribetsu Onsen Shopping Street. It\u0026rsquo;s also a great opportunity to buy some local souvenirs. It\u0026rsquo;s the perfect plan!\nSo cool room key.\rThe self-service breakfast was decent.\rEnmado On the way to the hot spring street, you can spot Enma-do Hall: a shrine or temple dedicated to Enma Daio, the Great King Enma. In Japanese Buddhism and Shinto beliefs, Enma Daio is regarded as the ruler and judge of Hell. He is believed to be responsible for determining people\u0026rsquo;s karma, reincarnation, and overseeing the operation of Hell. Enma-do Hall represents people\u0026rsquo;s faith and awe towards Hell, death, and karmic retribution, as well as their devout worship of Enma Daio. It seems that there are no such temples in Taiwan.üòÇ\nEnma-do Hall features a special performance called \u0026ldquo;Noh Mask Changing\u0026rdquo; at specific times.\nSchedule : 10:00„ÄÅ13:00„ÄÅ15:00„ÄÅ17:00„ÄÅ20:00„ÄÅ21:00\nIzumi Park On the way to Noboribetsu Jigokudani, you will pass through Izumi Park, located in the heart of the hot spring street. Although it is called a park, it is quite unique as it showcases the spectacle of hot water gushing out. It was built in 1996, costing approximately 80 million yen, to commemorate the 150th anniversary of the opening of the hot springs. The park stands on the former site of the \u0026ldquo;Noboribetsu Paradise Hotel\u0026rdquo; and is also known as the birthplace of Noboribetsu Onsen.\nInside the park, there is an intermittent geyser that, according to online sources, erupts approximately every 3 hours with hot springs reaching temperatures of about 80 degrees Celsius and reaching heights of up to 8 meters. The eruption lasts for about 50 minutes, during which around 2,000 liters of hot spring water gushes out. This unique experience can only be witnessed in Noboribetsu City. However, due to our tight schedule, we continued on our way to Jigokudani. Even without witnessing the eruption of hot springs, the sight of the intermittent geyser with its swirling white steam left a lasting impression.\nYunokawa Shrine Nearby Izumi Park, you can spot the entrance to Yunokawa Shrine. However, we decided to forgo visiting it due to the need to climb several flights of stairs.üòÇ\nPass by Yunokawa Shrine.\rContinuing on, we came across the entrance sign of Jigokudani, and I couldn\u0026rsquo;t help but appreciate how cute the Japanese monsters and spirits depicted on the sign were.\nNoboribetsu Onsen Jigokudani Noboribetsu Jigokudani is a volcanic hot spring area renowned for its unique geothermal landscapes and gushing springs. The terrain and thermal activities here create a mysterious and fiery ambiance, fitting well with the imagery of hell.\nCould see a sign indicating that we were getting closer and closer!\rThis sign must be joking!?\rConclusion In reality, exploring the entire Jigokudani could easily take half a day or more. If you venture deeper into the area, you will encounter attractions such as \u0026ldquo;Yunohana Hell,\u0026rdquo; \u0026ldquo;Sanzu no Kawa,\u0026rdquo; \u0026ldquo;Nanakamado Hiroba,\u0026rdquo; and \u0026ldquo;Oyunuma.\u0026rdquo; These spots are essentially within the hiking range of the entire Jigokudani area. However, due to our limited time and insufficient warm clothing, we decided not to linger outside for too long and made our way back.üòÇ\nFirst-time visitors to Hokkaido might not choose to come here, considering its distance from New Chitose Airport. Typically, for first-time visitors, it\u0026rsquo;s common to explore famous attractions like Sapporo, Otaru, and Hakodate. However, I highly recommend the itinerary in Noboribetsu. In fact, this trip to Hokkaido was primarily planned to visit this area. If you have more time, it\u0026rsquo;s recommended to stay for two nights, allocating one day for Jigokudani and the shopping street, while fully enjoying the natural hot springs here.\nThe wild fox we encountered on the way to Noboribetsu Onsen, it should be a fox, right?!üòé\nReference\nÈñªÈ≠îÂ†ÇÔºàEnmadoÔºâ - ÁôªÂà•ÂõΩÈöõË¶≥ÂÖâ„Ç≥„É≥„Éô„É≥„Ç∑„Éß„É≥Âçî‰ºö ",
	  "pubDate": "2023-05-06T19:02:59+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/030.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/taiwan/kenting-caesarpark/",
	  "title": "The Experience of Staying at Caesar Park Hotel Kenting - Kenting Street, Attractions, and Bars",
	  "summary": "\u003cp\u003eA Relaxing Journey to Kenting Street, Famous Attractions, and Bars - My Experience and Thoughts on a Luxurious Accommodation.\u003c/p\u003e",
	  "content": "A Relaxing Journey to Kenting Street, Famous Attractions, and Bars - My Experience and Thoughts on a Luxurious Accommodation.\nForeword This past long weekend, I was dragged by my girlfriend to travel with her large family to Kenting. The main purpose was to celebrate the first visit of elder brother and his wife Sophie from Australia to Taiwan and to let them experience the famous vacation destination in southern Taiwan. Unfortunately, I was suffering from stomach flu throughout the entire trip, and I was very cautious about everything. I didn\u0026rsquo;t take many photos of the scenery, didn\u0026rsquo;t eat much of the local street food, and only had a sip or two of alcohol. However, Kenting is still a novelty for me, as a person from southern Taiwan, since it\u0026rsquo;s relatively close geographically, and I had been too lazy to visit before, resulting in no memory of having been there üòÇ. So, I am recording my first Kenting vacation experience.\nCaesar Park Kenting After having lunch, we drove directly to Caesar Park Kenting,The room type was booked by my girlfriend\u0026rsquo;s father and it was a Deluxe Guest Room, featuring exotic Balinese-style decor with lush foliage and a romantic atmosphere.\nActually, it was a room for four people, but I only took pictures of half of it.\rStepping out of the floor-to-ceiling windows, there was a place for the hot springs.\rWhat\u0026rsquo;s cool is that the room\u0026rsquo;s perimeter can directly connect to the outdoor Coconut Grove Pool.\nAfter go to a hot spring, you can also go for a swim.\rThe view from the second floor allows you to see the entire coconut grove swimming pool.\rJust exploring the entire Caesar Hotel would keep you entertained for a long time, you wouldn\u0026rsquo;t even need any other schedules. ü§£\nKenting Street - Dinner Basically, for those who come to Kenting, dinner is usually taken care of on this street.\nIt\u0026#39;s quite similar to the night markets in Taiwan in general, but it seems like they sell different things from the night market near my home.ü§î\rLittle Bay Bar After finishing dinner, we came to this stall. I was actually quite tired at this point, so I just took some casual photos. The scenery and atmosphere were really nice though, and you can come here to relax both during the day and at night.\nWhen We arrived around ten o\u0026rsquo;clock, there were still a lot of people, so I noticed some of them just sat directly on the beach. I was kind of tempted to try drinking until I got drunk and sleeping until tomorrow right there on the beach, but my stomach really wouldn\u0026rsquo;t allow it.ü•¥\nThe bar was open until eleven o\u0026rsquo;clock, I was very tired when I got back, so I just washed up and went straight to bed.\nAs I was staying in such a top hotel, I thought that I shouldn\u0026rsquo;t stay out too late, especially since I hadn\u0026rsquo;t even had the chance to go in the hot spring yet.\nHotel buffet breakfast In the morning, I had the hotel\u0026rsquo;s buffet breakfast, which was of the same quality as the top hotels in Taiwan, so I didn\u0026rsquo;t take any photos. The dining environment was spacious, so it didn\u0026rsquo;t feel crowded, but I\u0026rsquo;m not sure if it was because I woke up too late.üòÖ Next, I go in the hot spring, and checked out at exactly 11:00am. Enjoy the scenery inside the hotel while eating.\rGelato Day by Day As my brother had to rush back to Taipei, our afternoon schedule was kept simple. We chose to visit the Daylily Ice Shop to take some photos and enjoy some refreshing ice desserts before heading directly to the high-speed rail station.\nHave a leisurely swing.\rConclusion It\u0026rsquo;s been a long time since I last visited Kenting, or maybe I\u0026rsquo;ve never been here before, I\u0026rsquo;m not sure.üòé Just in case I forget again, I\u0026rsquo;ll jot it down. I experienced the top-tier hotel, the beach, hot springs, and scenic views. If you choose Caesar Park Hotel, your itinerary should be able to be slightly more relaxed, after all, the hotel facilities need to be used well. Coming to the southernmost point of Taiwan, unlike living in the city, one must experience the slow pace. Next time, I must sit on the beach and have a drink.\n",
	  "pubDate": "2023-04-30T08:56:07+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/028kenting.caesarpark_view.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/mongodb-atlas-compass-tips/",
	  "title": "MongoDB Atlas Basic Concepts \u0026 Using Compass to Connect MongoDB",
	  "summary": "\u003cp\u003eRecord the key concepts and detailed connection operation tutorial of learning MongoDB Atlas.\u003c/p\u003e",
	  "content": "Record the key concepts and detailed connection operation tutorial of learning MongoDB Atlas.\n1. Foreword I\u0026rsquo;m used to working with RDBMS (relational databases), but when working on a personal project, deploying a database online always incurs significant costs. I stumbled upon MongoDB\u0026rsquo;s cloud service, Atlas, which surprisingly provides a free learning version. Therefore, I decided to learn how to use MongoDB, which is a document database belonging to the NoSQL (Not Only SQL) category.\nHere is the official introduction:\nMongoDB Atlas is a fully-managed cloud database service provided by MongoDB that offers a simple, secure, and reliable way to run MongoDB without any configuration or infrastructure management. MongoDB Atlas runs on cloud platforms such as AWS, Azure, and Google Cloud, enabling users to easily run, scale, and manage MongoDB globally. It provides automated deployment and management, including backup and recovery, monitoring, performance optimization, and security. With MongoDB Atlas, you can easily configure, deploy, and manage MongoDB clusters and use its various tools and services to simplify MongoDB development and operations. For example, it offers a user-friendly management console that allows you to easily manage your MongoDB clusters, users, and permissions. In addition, MongoDB Atlas provides advanced features such as full-text search, graph search, views, aggregation pipelines, and more, which can help you handle massive data and complex queries more easily.\n2. When to use NoSQL Here are some scenarios where using MongoDB is appropriate:\nUnstructured data: If the data format to be stored is uncertain, MongoDB can be a suitable choice, such as logs, multimedia, web content, social media data, etc.\nScalability: MongoDB can easily scale to handle more data by adding new nodes or shards.\nHigh performance: MongoDB is a high-performance database that can handle fast read/write operations, usually faster than relational databases.\nFlexible data model: MongoDB\u0026rsquo;s document data model is very flexible and can be changed according to the needs of the application without changing the structure of the entire database.\nBased on these points, MongoDB is a good option when data structure is uncertain, scalability is important, and there is no need to use joins between tables. It is also a good choice when fast read/write operations are necessary.\n3. Atlas sign-up As it is a database service provided directly through cloud services, the process is very simple. After registering an account on the MongoDB official website, simply select Atlas in the product section and follow the steps to create basic information and connection permissions, allowing you to use it directly.\n4. Authority Management In MongoDB Atlas, you can create multiple organizations, each of which can have multiple projects, and each project can have multiple clusters. Here are the differences between them:\nOrganization: An organization is the highest level in MongoDB Atlas. You can register and create an organization with an email address and invite other users to join. You can assign different projects to different teams or users, and manage all resources and settings in the organization.\nProject: A project is a collection of one or more MongoDB clusters. In a project, you can configure related security, monitoring, and automation features, as well as access permissions and notification rules. A project can be owned and managed by a team or a user.\nCluster: A cluster is an instance of MongoDB, which is a distributed system composed of a group of MongoDB servers. Each cluster has its own storage space, computing resources, and settings, and is managed and monitored by MongoDB Atlas. You can configure various options in each cluster, such as available regions, database versions, storage engines, and security settings.\nIn summary, these three levels are used to manage and regulate the usage permissions of each database, from top to bottom.\n5. Database Connection I prefer using a GUI to operate databases, and MongoDB also provides a GUI for users to operate databases. You can go to MongoDB Compass Download (GUI) and download MongoDB Compass in the Tools section.\nAfter installation, when you run it, you will be prompted to enter the string used to connect to the database. The connection string can be found in the Atlas Cluster management interface on the right side, where you can click on \u0026ldquo;Connect\u0026rdquo; to find various connection options. Since we are using Compass to connect, we click on Compass and the screen will display the connection string for Compass.\nmongodb+srv://\u0026lt;user\u0026gt;:\u0026lt;password\u0026gt;@cluster0.swvkd15.mongodb.net/test \u0026lt;user\u0026gt; \u0026lt;password\u0026gt; refer to the account username and password created when setting up the Cluster initially.\nAfter copying the connection string and modifying \u0026lt;user\u0026gt; and \u0026lt;password\u0026gt;, remember to configure the allowed IP addresses in the Network Access settings.\nAfter setting up the IP address for network access, paste the connection string in MongoDB Compass to establish a successful connection.\n6. Database Structure In MongoDB, data is stored in the form of documents, and a collection is a collection of documents. In other words, a MongoDB database can contain multiple collections, and each collection contains multiple documents.\nCompared to relational databases, it like below\nMySQL MongoDB database database table collection per row document In MongoDB, data is stored in the form of documents. A collection is a collection of documents. In other words, a MongoDB database can contain multiple collections, and each collection contains multiple documents.\nUnlike relational databases, MongoDB documents can contain highly flexible structures and content, and there is no need to define fields beforehand.\nIn simple terms, a collection is where documents are stored, and a document is the container that actually stores the data. If MongoDB is viewed as a document database system, then a collection is the table in this system, and a document is the row in this table.\n7. Conclusion Have a basic understanding of MongoDB\u0026rsquo;s data structure and how to connect to an Atlas cloud database. The next step is to learn how to perform MongoDB read and write operations using node.js.\n",
	  "pubDate": "2023-04-24T15:33:44+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/026en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/database-optimization-index-tips/",
	  "title": "Database Query Performance Optimization Tips : Statement Optimization, Adding Indexes",
	  "summary": "\u003cp\u003eAs the system data grows increasingly large, it\u0026rsquo;s common to encounter situations where database queries take too long. This article provides several methods for optimizing database query performance.\u003c/p\u003e",
	  "content": "As the system data grows increasingly large, it\u0026rsquo;s common to encounter situations where database queries take too long. This article provides several methods for optimizing database query performance.\nMainly records several techniques that I have used, including optimizing query statements, optimizing indexes, and reducing the amount of data in the database.\n1. Optimizing query statements Here are several tips that we need to pay attention to when writing SQL statements.\nAvoid using * in SELECT queries\nPerformance: SELECT * will query all columns in the entire table, including unnecessary ones. This increases the workload on the database and reduces query efficiency. Only querying the required columns can reduce the amount of data queried, thereby improving query efficiency.\nReadability: SELECT * does not clearly indicate which columns are being queried, which can make the code difficult to understand and maintain. Explicitly specifying the required columns can make the code clearer and easier to understand.\nConflicts: Using SELECT * may lead to conflicts if there are columns with the same name in the table, such as when using JOIN. Using explicit column names can avoid such problems.\nAvoid using != or \u0026lt;\u0026gt; in the WHERE clause\nUsing this statement is equivalent to querying the entire table, which leads to a decrease in performance.\nAvoid using OR in the WHERE clause\nThe OR operator also requires searching the entire table to find data that matches either of the conditions, leading to a decrease in performance. Instead, you can use UNION ALL to retrieve data that matches each condition separately and then combine them.\nAvoid using expressions and functions in the WHERE clause\nEx:\nselect name from table where age-10 = 20 Basically, this kind of writing will first subtract 10 from the entire \u0026ldquo;age\u0026rdquo; column of the table and then compare whether it is equal to 20, so it should be changed to:\nselect name from table where age = 30 Of course, this example is a bit extreme and not normally written like this. The main point is to avoid using expressions or functions on the left side of the equal sign to modify column data, and instead modify it on the right side.\n2. Adding Indexes An index is a data structure used in database management systems to speed up data queries. By sorting a column or a combination of columns, an index creates a data structure that can accelerate the speed of queries and sorting. In a database, an index can be seen as a pointer or address that points to the actual data.\nSimply put, adding an index for a specific column (e.g., the column containing data A, B, C, D, etc.) can help speed up queries searching for data related to a specific value (e.g., querying for data where the column equals B). This is because the index allows the database to quickly locate the block of data related to the value B, rather than searching through all blocks containing A, B, C, D, etc., which can increase query speed.\n‚óè Recommendations situation for adding indexes Indexes can be created on a single column or multiple columns. The following are some situations where adding an index is recommended:\nThe primary key and foreign key : Both are one of the most important columns in a database table, and they are typically used as a linking condition for queries and table relationships. Creating indexes on these columns can make querying and relationship operations faster.\nFrequently queried columns: If a column is frequently used as a query condition, creating an index can significantly improve query efficiency.\n!!Be aware of uniqueness and distribution issues.\nEx : The gender column is not suitable for indexing because it only has two values, male and female, and does not have uniqueness, so it cannot be used as a unique key or primary key column, and indexing it will not speed up queries. The distribution of gender values in the column is relatively even and does not cause a large amount of data to be concentrated on a particular value, so not creating an index has a smaller impact on query efficiency.\nCombinatorial query conditions : When multiple columns are used together as query conditions, a composite index can be created. A composite index can help the database system locate data that matches multiple conditions more quickly.\nColumns used for sorting : If a column is frequently used for sorting, creating an index can significantly improve the efficiency of sorting operations.\nColumns used for grouping : An index on grouping columns can greatly improve the efficiency of grouping operations if a column is frequently used for grouping.\n‚óè Note considerations Although it may seem that many situations can benefit from adding indexes, more indexes do not necessarily mean better performance. Too many indexes can increase query complexity and result in lower efficiency. Therefore, determining which situations require adding indexes often requires experience and judgement.\nWhile indexes can speed up query performance, in reality, they trade space for time. In the example of a column with data such as A, B, C, D, etc., adding an index to this column means that there will be additional data tables storing parts of the data (such as A, B, C, D, etc.) for faster querying. Therefore, the server\u0026rsquo;s hardware environment and capacity to handle this additional data also need to be considered.\n3. Reducing the amount of data in the database Currently, the main methods for reducing the amount of data in a database through querying include normalization, compressing the database, and deleting data.\nNormalization : Is a process in database design that involves dividing data into smaller, more related parts to reduce data duplication, improve database efficiency, and enhance flexibility.\nDatabase compression : Compressing database files is a technique that can increase performance depending on the compression method and ratio used. Generally, if the compression ratio is high, it may take longer to decompress data, which could slow down queries. If the compression ratio is low, it can reduce disk I/O operations and improve query performance. It is recommended to seek assistance from a professional DBA for this task.\nDeleting data: Delete redundant or duplicate data as needed.\nI think the part about deleting data should be paid more attention to at the beginning, when designing the schema and deciding what data to store. It is better to consider it thoroughly beforehand, rather than waiting for a large amount of data to accumulate before figuring out how to delete it. (Of course, if there is a need for system merging, there may be no other option.)\nThe above are the techniques that I have actually implemented, and I will supplement the others once I have tried them.üòé\n",
	  "pubDate": "2023-03-29T09:38:14+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/025en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/dark-mode-tips-hugo/",
	  "title": "Designing and Implementing Dark Mode in Hugo",
	  "summary": "\u003cp\u003eRecording the hurdles encountered while adding dark mode functionality to a website, as well as frontend display tips and implementation on Hugo.\u003c/p\u003e",
	  "content": "Recording the hurdles encountered while adding dark mode functionality to a website, as well as frontend display tips and implementation on Hugo.\nForeword The switch between dark and light modes is now a must-have feature for both webpages and apps. When using certain mobile apps without the option to switch, it\u0026rsquo;s difficult for me not to wonder why the team didn\u0026rsquo;t consider user-friendliness. Unfortunately, the built-in theme of my website didn\u0026rsquo;t support this feature, so I had to create one myself. Initially, I thought it was simple, just add a dark mode-specific CSS and use class switching to modify the CSS used for the interface to achieve the goal of modifying it to a dark mode.\nAlthough the interface successfully switched to dark mode, I encountered a problem where there was a brief white flash when navigating to other pages within the same website. After trying various solutions, I found that using prefers-color-scheme: dark resolved the issue.\nPrinciple The main principle is to use CSS prefers-color-scheme media query and JavaScript to monitor the color mode settings of the operating system or browser and dynamically add or remove the dark-mode class on the page. This means that when navigating to other pages, the page will not immediately switch to the default light or dark mode, but instead will maintain the current color mode state, thus avoiding the brief white flash.\nSwitch Mode Button First, need to create a button that can be clicked to switch between modes. You can choose your own style for the button, and here I chose to use Unicode symbols for the sun and moon to serve as the appearance of the button. The location of the code:Root directory\\themes\\Mainroad\\layouts\\baseof.html\n1\u0026lt;span class=\u0026#34;sun-btn\u0026#34;\u0026gt;\u0026amp;#x2600;\u0026lt;/span\u0026gt; 2\u0026lt;span class=\u0026#34;moon-btn\u0026#34;\u0026gt;\u0026amp;#x263D;\u0026lt;/span\u0026gt; Next, you can add the style for the button and the dark mode in style.css.\nCSS variable Since the configuration of different websites varies, you will still need to set up the corresponding dark mode color scheme in CSS. To have better control over CSS, you can use CSS variables, which are values defined and used in CSS that can be repeated throughout the document. They are named by prefixing a variable name with a double hyphen (\u0026ndash;).\nExample:\n1:root { 2 --main-color: #ff0000; 3} 4 5h1 { 6 color: var(--main-color); 7} A CSS variable named --main-color is defined in the :root pseudo-class and set to red. Then, the var() function is used to reference this variable in the color property applied to the h1 element.\nOne of the benefits of CSS variables is that they can be shared across different elements and selectors, and their values can be dynamically changed through JavaScript. This makes CSS variables a very powerful and flexible tool that can be used to achieve dynamic and maintainable style sheets.\nHere is the CSS used in my theme. To facilitate management and add styles for dark mode, I first define various CSS variables in the :root pseudo-class. Then, I replace all the CSS styles used in the css.style with these CSS variables, so that the effect of a unified transition can be achieved by simply modifying the variable values in the dark mode class.\n1:root { 2\t--bodyBG: #f7f7f7; 3\t--wrapperBG:#ffffff; 4\t--wrapperBG2:#f5f5f5; 5\t--text-color: #000000; 6\t--codeBG:#f6f8fa; 7 } Then customize the style of the light/dark mode toggle button.\n1/* Light Mode */ 2.sun-btn { 3\tdisplay: block; 4\tfont-size: 2rem; 5\tcolor: #ffc533; 6 } 7 8.moon-btn { 9\tdisplay: none; 10 } 11/* Dark Mode */ 12 .dark-mode .sun-btn{ 13\tdisplay: none; 14 } 15 16 .dark-mode .moon-btn { 17\tdisplay: block; 18\tfont-size: 2rem; 19\tcolor: #fff9be; 20 } 21 Here is the CSS for the dark mode, and you can see that there are not many actual changes because most of them have been replaced with CSS variables.\n1 /*dark-mode styles*/ 2.dark-mode { 3\t--bodyBG:#2b2b2b; 4\t--wrapperBG: #212121; 5\t--wrapperBG2: #212121; 6\t--text-color: #C5C5C5; 7\t--codeBG:#424242; 8\tbox-shadow: none; 9 } 10.dark-mode span.nav-indicator{ 11\tbackground-color: #ACD6FF !important; 12 } 13.dark-mode li{ 14\tcolor: var(--text-color); 15 } Reduce the brightness of the white background If there are some media or images on the webpage, it may be too bright in dark mode if they have a white background. In this case, you can use the brightness CSS property to reduce the brightness in dark mode.\n1.dark-mode iframe, 2.dark-mode img, 3.dark-mode video { 4 filter: brightness(0.9); 5} Button monitoring, localStorage store user preference mode Next, we will use JavaScript to implement the following functionalities. The location of the code:Root directory\\themes\\Mainroad\\layouts\\baseof.html\nWhen the page is loaded, check if the \u0026ldquo;dark-mode\u0026rdquo; setting exists in the localStorage. If it does, enable or disable the dark mode based on the setting. Listen for changes in the system settings for switching to dark mode (using window.matchMedia to create a MediaQueryList object). When the user clicks the \u0026ldquo;sun\u0026rdquo; or \u0026ldquo;moon\u0026rdquo; button, toggle the state of the dark mode and update the localStorage value accordingly. 1let darkModeState = false; 2 3const sun = document.querySelector(\u0026#39;.sun-btn\u0026#39;); 4const moon = document.querySelector(\u0026#39;.moon-btn\u0026#39;); 5 6// MediaQueryList object 7const useDark = window.matchMedia(\u0026#34;(prefers-color-scheme: dark)\u0026#34;); 8 9// Toggles the \u0026#34;dark-mode\u0026#34; class 10function toggleDarkMode(state) { 11 document.documentElement.classList.toggle(\u0026#34;dark-mode\u0026#34;, state); 12 darkModeState = state; 13} 14 15// Sets localStorage state 16function setDarkModeLocalStorage(state) { 17 localStorage.setItem(\u0026#34;dark-mode\u0026#34;, state); 18} 19 20// Initial setting 21toggleDarkMode(localStorage.getItem(\u0026#34;dark-mode\u0026#34;) == \u0026#34;true\u0026#34;); 22 23// Listen for changes in the OS settings. 24// useDark.addListener((evt) =\u0026gt; toggleDarkMode(evt.matches)); 25 useDark.addEventListener(\u0026#39;change\u0026#39;, (evt) =\u0026gt; { 26 toggleDarkMode(evt.matches); 27 }); 28 29 30// Toggles the \u0026#34;dark-mode\u0026#34; class on click and sets localStorage state 31sun.addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { 32 darkModeState = !darkModeState; 33 34 toggleDarkMode(darkModeState); 35 setDarkModeLocalStorage(darkModeState); 36}); 37moon.addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { 38 darkModeState = !darkModeState; 39 40 toggleDarkMode(darkModeState); 41 setDarkModeLocalStorage(darkModeState); 42}); Conclusion I actually think that this method is similar to the approach I was originally using, which was dynamically adding or removing the dark-mode class on the page using JavaScript. I\u0026rsquo;m not sure why this method seems to solve the issue of a brief white flash on page transitions, and I would appreciate any input from frontend experts.\nReference\nDark Mode - The prefers-color-scheme Website Tutorial ",
	  "pubDate": "2023-03-22T11:34:07+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/024en.webp"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/sql-scopeidentity-identity-diff/",
	  "title": "SQL SCOPE_IDENTITY() and @@IDENTITY Differences in Retrieving Auto-Increment Values",
	  "summary": "\u003cp\u003eUsing SCOPE_IDENTITY() in SQL Server to Retrieve the Most Recently Insert Data and Comparing it with @@IDENTITY for Value Retrieval Differences.\u003c/p\u003e",
	  "content": "Using SCOPE_IDENTITY() in SQL Server to Retrieve the Most Recently Insert Data and Comparing it with @@IDENTITY for Value Retrieval Differences.\nRequirement In frontend development, it is common to require subsequent processing of newly added data, which often involves the use of the IDENTITY value of the added data. Similarly, in backend databases, triggers may be used to modify or add data, and the IDENTITY value of the newly added data may also be required for subsequent processing. This article will discuss the practical differences between SCOPE_IDENTITY() and @@IDENTITY in SQL Server. Principle SCOPE_IDENTITY() function returns the most recently inserted identity value within the current scope. This means that it only returns the identity value generated by the insert statement currently being executed, and is not affected by any identity values generated by any triggers.\n@@IDENTITY function returns the last identity value generated by an insert statement, regardless of the scope in which it was generated. Therefore, if the insert statement includes triggers, the @@IDENTITY function may return an unexpected identity value.\nIt may sound a bit complicated, so let\u0026rsquo;s provide an example to illustrate the differences between the two functions. Example 1.Create an example table\n1CREATE TABLE ExampleTable ( 2 ID INT PRIMARY KEY IDENTITY(1,1), 3 Name NVARCHAR(50) NOT NULL 4); 2.Create an trigger if there is a newly data inserted that copy the newly data to another row in the same table.\n1CREATE TRIGGER ExampleTrigger 2ON ExampleTable 3FOR INSERT 4AS 5BEGIN 6 INSERT INTO ExampleTable (Name) 7 SELECT Name FROM INSERTED 8END; 3.Insert a row with Name \u0026ldquo;Alvin\u0026rdquo; to the table, which will activate the trigger and create another row with the same data. As a result, the table will have two rows with Name \u0026ldquo;Alvin\u0026rdquo;.\n1INSERT INTO ExampleTable (Name) 2VALUES (\u0026#39;Alvin\u0026#39;); 3 4SELECT SCOPE_IDENTITY() 5SELECT @@IDENTITY 6 7DROP TABLE ExampleTable; The content of the table is as follows:\nID Name 1 Alvin 2 Alvin The result of SELECT SCOPE_IDENTITY() is 1.\nThe result of SELECT @@IDENTITY is 2.\nIt is clear that SCOPE_IDENTITY() retrieves the ID of the current insert, while @@IDENTITY retrieves the ID of the data inserted by the trigger.\nConclusion When retrieving the value of an auto-incremented field for subsequent processing of the newly inserted data, it is recommended to use SCOPE_IDENTITY() rather than @@IDENTITY. Using @@IDENTITY may result in obtaining an additional value that was inserted due to the current insertion, whereas SCOPE_IDENTITY() specifically retrieves the ID of the newly inserted data.\n",
	  "pubDate": "2023-03-20T10:44:19+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/023en.png"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/jwt-authentication-mechanism/",
	  "title": "JWT Authentication Mechanism Principles and Implementation Examples (C#) ",
	  "summary": "\u003cp\u003eJWT (JSON Web Token) is an open standard used for securely transmitting information between parties. JWT uses JSON objects to represent the messages to be transmitted and uses digital signatures or encryption to protect these messages.\u003c/p\u003e",
	  "content": "JWT (JSON Web Token) is an open standard used for securely transmitting information between parties. JWT uses JSON objects to represent the messages to be transmitted and uses digital signatures or encryption to protect these messages.\nJWT is commonly used for quickly setting up login authentication for lightweight websites. Here is a Record the usage of implementing JWT in C#. Introduction JWT\nEncoded\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\nConsisting of three parts, namely Header, Payload, and Signature.\nUsing the example provided on the official website directly:\nHeader part includes data such as the encryption algorithm and type used by JWT.\n{ \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } Header is a JSON object that contains the following two properties:\nalgÔºö Indicates the encryption algorithm used, such as HS256, RS256, etc. typÔºöIndicates the type, which is usually set to JWT. Payload part contains the information to be transmitted.\nPayload is generally also a JSON object and can contain custom properties. Some basic customer information, etc. are typically stored in the Payload part.\n{ \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;iat\u0026#34;: 1516239022 } Standard properties that can be included in the Payload are:\niss: The issuer of the JWT, indicating who issued the JWT. sub: The subject of the JWT, indicating the entity (usually the user) that the JWT represents. aud: The audience of the JWT, indicating who can use the JWT. exp: The expiration time of the JWT, indicating when the JWT expires and can no longer be used. nbf: The time when the JWT becomes valid, indicating when the JWT becomes effective. iat: The time when the JWT was issued, indicating when the JWT was issued. jti: The unique identifier of the JWT, used to prevent the JWT from being reused. The properties listed above are some standards defined in the JWT specification, but custom names can also be defined to represent specific information in the application. Signature part is the result of digitally signing the Header and Payload.\nHMACSHA256(\rbase64UrlEncode(header) + \u0026#34;.\u0026#34; +\rbase64UrlEncode(payload),\r\u0026#39;your-256-bit-secret\u0026#39;\r) The header and payload are concatenated using a period .as a delimiter, and the your-256-bit-secret is the custom string (private key) stored on the server. The resulting string, which combines these three parts, is then encrypted using a cryptographic algorithm for generating a signature.\nThe signature can ensure that the JWT has not been tampered with. The calculation method of the signature varies depending on the encryption algorithm used, with commonly used algorithms including HMAC and RSA.\nPrinciple In simple terms, a string is generated using the Header and Payload, with each string separated by a .. Then, a private key is used to sign the string, generating a JWT that includes the Header, Payload, and Signature.\nWhen verifying the JWT, the receiver can decode the Header and Payload, and then use the same key to verify the Signature to ensure that the JWT has not been tampered with. If the verification is successful, the information contained in the JWT can be trusted. Creating JWT Example (.NET 6) 1using System.IdentityModel.Tokens.Jwt; 2using System.Security.Claims; 3using System.Text; 4using Microsoft.IdentityModel.Tokens; 5 6// Set the signing key for JWT 7var securityKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(\u0026#34;MySuperSecretKey\u0026#34;)); 8var signingCredentials = new SigningCredentials(securityKey, SecurityAlgorithms.HmacSha256); 9 10var claims = new[] 11{ 12 new Claim(JwtRegisteredClaimNames.Sub, \u0026#34;user123\u0026#34;), // Set the username 13 new Claim(JwtRegisteredClaimNames.Jti, Guid.NewGuid().ToString()) // Set the JWT ID 14 }; 15 16// Set the expiration time of JWT to 1 hour 17var expires = DateTime.UtcNow.AddHours(1); 18 19// Create JWT object 20var token = new JwtSecurityToken( 21 issuer: \u0026#34;MyApp\u0026#34;, 22 audience: \u0026#34;MyClient\u0026#34;, 23 claims: claims, 24 expires: expires, 25 signingCredentials: signingCredentials); 26 27// Convert the object to a JWT string 28var tokenString = new JwtSecurityTokenHandler().WriteToken(token); 29 30// Print the JWT string 31Console.WriteLine(tokenString); Validate JWT Example (.NET 6) 1// Get the JWT carried from the front-end 2string tokenString = \u0026#34;{insert JWT string here}\u0026#34;; 3 4// Set the validation parameters for JWT 5var validationParameters = new TokenValidationParameters 6{ 7 // Set the issuer and audience of JWT 8 ValidIssuer = \u0026#34;MyApp\u0026#34;, 9 ValidAudience = \u0026#34;MyClient\u0026#34;, 10 // Set the signing key and encryption algorithm for JWT validation 11 IssuerSigningKey = new SymmetricSecurityKey(Encoding.UTF8.GetBytes(\u0026#34;MySuperSecretKey\u0026#34;)), 12 ValidateIssuerSigningKey = true, 13 ValidateLifetime = true, 14 ClockSkew = TimeSpan.Zero 15}; 16 17try 18{ 19 // Validate the JWT 20 var jwtHandler = new JwtSecurityTokenHandler(); 21 var principal = jwtHandler.ValidateToken(tokenString, validationParameters, out var validatedToken); 22 var jwtToken = validatedToken as JwtSecurityToken; 23 24 // Get the username and JWTID 25 var username = jwtToken.Claims.First(x =\u0026gt; x.Type == JwtRegisteredClaimNames.Sub).Value; 26 var jwtId = jwtToken.Claims.First(x =\u0026gt; x.Type == JwtRegisteredClaimNames.Jti).Value; 27 28 // Additional validation can be performed here, 29 //such as checking if the user exists in the database or 30 //if the user has permission to access resources, etc. 31 32 // Validation succeeded 33 Console.WriteLine($\u0026#34;Token validated. Username: {username}, JWT ID: {jwtId}\u0026#34;); 34} 35catch (SecurityTokenException e) 36{ 37 // Validation failed 38 Console.WriteLine($\u0026#34;Token validation failed: {e.Message}\u0026#34;); 39} The issuer and receiver of a JWT.\nIssuer (iss): Used to indicate who issued the JWT, typically the name or domain of the organization or application that identifies the JWT. When verifying the JWT, we can check if the issuer of the JWT matches the expected value to ensure that the JWT was issued by the correct organization or application.\nAudience (aud): Used to indicate who can use the JWT, typically the name or domain of the application or API that identifies the JWT user. When verifying the JWT, we can check if the audience of the JWT matches the expected value to ensure that the JWT can be used by the correct application or API. Try it by myself The above is an implementation of JWT-related functionalities using the .NET Core library directly. Recently, I received a requirement to generate one-time download URLs for files in order to prevent the leakage of file URLs. After considering the options, I plan to use JWT to implement this feature.\nThe thought process is to generate a one-time download URL without creating a burden on the database. To achieve this, a time-limited JWT is generated. Additionally, JWT verification is added to the API that provides file downloads. If the JWT has expired, the download is not allowed, ensuring the one-time usability of the URL.\nThe flow of this feature is as follows:\nGenerate a JWT using the file\u0026rsquo;s ID (GUID). Generate a one-time download URL that includes the JWT. When downloading via the URL, verify the expiration of the JWT. Determine whether to allow the download based on the JWT validation. Creating Objects for Header and Payload\npublic class oJWT_Header { public string alg { get; set; } public string typ { get; set; } } public class oJWT_Payload { public long exp { get; set; } public long iat { get; set; } public Guid id { get; set; } } For the payload, I primarily require the expiration time (exp) and the unique identifier (id) of the file.\nEncode JWT 1public static string JWT_Encoder(Guid fileID) 2 { 3 oJWT_Header header = new oJWT_Header(); 4 header.alg = \u0026#34;HS256\u0026#34;; 5 header.typ = \u0026#34;JWT\u0026#34;; 6 7 oJWT_Payload payload = new oJWT_Payload(); 8 DateTimeOffset now = DateTimeOffset.UtcNow; 9 long unixTimestamp = now.ToUnixTimeSeconds(); 10 long unixTimestamp_exp = now.AddSeconds(20).ToUnixTimeSeconds(); 11 payload.iat = unixTimestamp; 12 payload.exp = unixTimestamp_exp; 13 payload.id = fileID; 14 15 string json_header = JsonConvert.SerializeObject(header); 16 byte[] b_header = System.Text.UTF8Encoding.UTF8.GetBytes(json_header); 17 string b64_header = Convert.ToBase64String(b_header); 18 19 string json_payload = JsonConvert.SerializeObject(payload); 20 byte[] b_pl = System.Text.UTF8Encoding.UTF8.GetBytes(json_payload); 21 string b64_payload = Convert.ToBase64String(b_pl); 22 23 string encryptSignature = ComputeHMACSHA256(b64_header + b64_payload, b64_payload + payload.id); 24 string token = b64_header + \u0026#34;.\u0026#34; + b64_payload + \u0026#34;.\u0026#34; + encryptSignature; 25 26 return token; 27 } Header Object should include the algorithm (alg) and token type (typ) being used.\nPayload Object should include the JWT issuance time (iat), expiration time (exp), and file ID (id). The times should be converted to Unix time based on UTC. The expiration time should be the current time plus 20 seconds.\nNext, encode the Header and Payload separately using Base64.\nThen, encrypt the encoded Header and Payload using HMACSHA256. For the private key, I have chosen to use the Encoded Payload + fileID. This ensures that the private key for each URL is unique and does not need to be hardcoded on the server side.\nHMACSHA256 encryption 1public static string ComputeHMACSHA256(string data, string key) 2 { 3 var keyBytes = Encoding.UTF8.GetBytes(key); 4 using (var hmacSHA = new HMACSHA256(keyBytes)) 5 { 6 var dataBytes = Encoding.UTF8.GetBytes(data); 7 var hash = hmacSHA.ComputeHash(dataBytes, 0, dataBytes.Length); 8 return BitConverter.ToString(hash).Replace(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;).ToUpper(); 9 } 10 } After obtaining the encrypted signature using HMACSHA256, the JWT becomes: Encoded Header.Encoded payload.encrypted signature\nDecode JWT 1public static bool JWT_Decoder(string JWT) 2 { 3 string[] ary = JWT.Split(\u0026#39;.\u0026#39;); 4 if (ary.Length != 3) return false; 5 else 6 { 7 string b64_header = ary[0]; 8 string b64_payload = ary[1]; 9 string Signature = ary[2]; 10 11 Byte[] ary_header = Convert.FromBase64String(b64_header); 12 string json_header = System.Text.UTF8Encoding.UTF8.GetString(ary_header); 13 oJWT_Header header = JsonConvert.DeserializeObject\u0026lt;oJWT_Header\u0026gt;(json_header); 14 15 Byte[] ary_payload = Convert.FromBase64String(b64_payload); 16 string json_pl = System.Text.UTF8Encoding.UTF8.GetString(ary_payload); 17 oJWT_Payload payload = JsonConvert.DeserializeObject\u0026lt;oJWT_Payload\u0026gt;(json_pl); 18 19 string encryptSignature = ComputeHMACSHA256(b64_header + b64_payload, b64_payload + payload.id); 20 21 if (!Signature.Equals(encryptSignature)) return false; 22 23 long exp = payload.exp; 24 long now = DateTimeOffset.UtcNow.ToUnixTimeSeconds(); 25 if (now \u0026gt; exp) return false; 26 } 27 return true; 28 } The verification process is straightforward. Initially, since the JWT format uses periods . as separators, I first split the encoded token using .. If the resulting array length is not equal to 3, it means it doesn\u0026rsquo;t comply with the format, and then return false.\nNext, I divide the array into the base64-encoded header, base64-encoded payload, and the signature. I decode the base64-encoded header and payload to extract the relevant information. Then, I verify if the signature matches the decoded data. If it doesn\u0026rsquo;t match, it indicates that the JWT has been tampered with, and then return false.\nThe final step is to verify the token\u0026rsquo;s expiration. Since the token has a predefined expiration time of 20 seconds after its creation, I compare the \u0026ldquo;exp\u0026rdquo; (expiration) claim in the payload with the current timestamp. Returns false if the token has expired.\nThe above process completes the functionality of generating one-time URLs using JWT. The payload can be customized to include additional parameters for convenient usage, such as querying file paths for downloading files from an API.\nConclusion I currently used JWT only for lightweight website login authentication, but in fact, JWT has other uses such as resource authorization, single sign-on (SSO), timed services, inter-application communication, etc. By distributing time-limited JWTs and verifying them each time whenever resources are accessed, its application can be more widely extended.\n",
	  "pubDate": "2023-03-06T16:05:44+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/022en.png"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/hugo-related-posts-supports-carousel-display/",
	  "title": "Implementing Recommended Posts and Carousel Display in Hugo",
	  "summary": "\u003cp\u003eSimple implementation of recommended posts list displayed in a carousel format.\u003c/p\u003e",
	  "content": "Simple implementation of recommended posts list displayed in a carousel format.\nForeword I have been curious about the implementation of recommended posts for a long time, and I have come across some arguments that having a recommended posts feature on a website is not necessarily essential. However, for myself, when browsing a website, I am much more likely to click into an article that catches my interest in the recommended posts section. So, I wanted to record the implementation details and present recommended posts in a carousel format.\nImplementing a carousel can be a tricky process, as it can negatively impact user experience on mobile or tablet devices if not done properly. However, the carousel plugin used in this article does not have this issue.üëç„ÄÇ\nPosts Relatedness The content of recommended posts can be obtained directly through the \u0026rsquo;tags\u0026rsquo; parameter of Hugo\u0026rsquo;s own posts. The logic is to first search for posts with the same tags as the current post among all posts. If there are any, then the recommended post list will be displayed.\nTo avoid modifying the original theme of Hugo, the single.html file in the root directory\\layouts\\_default can be modified. If this file does not exist, it can be copied from\nroot directory\\themes\\theme\\layouts\\_default\\single.html.\nRecommended post lists are usually placed at the bottom of posts, but can be placed wherever preferred. I chose to place it above the comment section, so I added the code above {{ partial \u0026quot;comments.html\u0026quot; . }} in single.html.\n1{{- range first 1 (where (where .Site.Pages \u0026#34;.Params.tags\u0026#34; \u0026#34;intersect\u0026#34; .Params.tags) \u0026#34;Permalink\u0026#34; \u0026#34;!=\u0026#34; .Permalink) -}} 2 {{- $.Scratch.Set \u0026#34;has_related\u0026#34; true -}} 3{{- end -}} 4 5{{ if $.Scratch.Get \u0026#34;has_related\u0026#34; }} 6 \u0026lt;div class=\u0026#34;related-content\u0026#34;\u0026gt; 7 \u0026lt;h3\u0026gt;See Also\u0026lt;/h3\u0026gt; 8 \u0026lt;ul\u0026gt; 9 {{- $num_to_show := .Site.Params.related_content_limit | default 3 -}} 10 {{ range first $num_to_show (where (where .Site.Pages \u0026#34;.Params.tags\u0026#34; \u0026#34;intersect\u0026#34; .Params.tags) \u0026#34;Permalink\u0026#34; \u0026#34;!=\u0026#34; .Permalink) }} 11 \u0026lt;li\u0026gt; 12 \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; 13 \u0026amp;ndash; 14 \u0026lt;time datetime=\u0026#34;{{ .Date.UTC.Format \u0026#34;2006-01-02T15:04:05-0700\u0026#34; }}\u0026#34;\u0026gt; 15 {{ .Date.Format \u0026#34;Jan 2, 2006\u0026#34; }} 16 \u0026lt;/time\u0026gt; 17 \u0026lt;br\u0026gt; 18 \u0026lt;small\u0026gt;{{ .Summary | plainify | htmlUnescape }}\u0026lt;/small\u0026gt; 19 \u0026lt;/li\u0026gt; 20 {{ end }} 21 \u0026lt;/ul\u0026gt; 22 \u0026lt;/div\u0026gt; 23{{ end }} {{ .Title }}refers to the title of the recommended post.\n{{ .RelPermalink }}refers to the permalink of the recommended post.\n{{ .Summary | plainify | htmlUnescape }}refers to the summary of the recommended post.\nRecommended post list, showing three posts by default.\rCarousel display I felt that displaying only the post title, date, and summary was a bit monotonous, so I searched the internet for a carousel implementation. In the end, I chose to use Owl Carousel, I found its features to be powerful, including RWD, flexible parameter settings, and the ability to drag and slide images in the carousel.\nImporting JS and CSS.\n1\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt; 2\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.theme.default.min.css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt; 3\u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.5.1.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 4\u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; You can choose how many carousel items to display on the screen.\n1$(\u0026#34;.owl-carousel\u0026#34;).owlCarousel({ 2 loop: true, // Enable loop playback 3 margin: 10, // Set margin between items to 10px 4 nav: true, // Enable navigation dots 5 responsive: { 6 0: { 7 items: 2 // Display 2 items on screens from 0 to 600 8 }, 9 600: { 10 items: 4 // Display 4 items on screens from 600 to 1000 11 }, 12 1000: { 13 items: 4 // Display 4 items on screens larger than 1000 14 } 15 } 16}); I wanted to display the post thumbnail in the carousel with a hover effect, so I customized it.\n1.owl-theme .item { 2\theight: 15rem; 3\tbackground-color: rgba(245, 245, 245, 0.3); 4\tpadding: 0.5rem; 5\tbox-shadow: 0 0 20px 0 rgba(0, 0, 0, 0.1); 6\ttransform: scale(0.9, 0.9); 7\ttransition: all 0.3s ease-out; 8} 9.owl-theme .item:hover { 10\ttransform: scale(1.0, 1.0); 11} 12.owl-carousel .item h4 { 13\tcolor: #000000; 14\tfont-weight: 400; 15\tfont-size: 1rem; 16\tmargin-top: 0rem; 17} 18.owl-carousel .item img { 19\twidth: 100%; 20\theight: 7rem; 21\tborder-radius: 4px; 22} As some of my posts have thumbnail and some do not, I made some adjustments to the HTML to display the post title and summary if there is no thumbnail, and to display the thumbnail and title if there is one.\n1{{- range first 1 (where (where .Site.Pages \u0026#34;.Params.tags\u0026#34; \u0026#34;intersect\u0026#34; .Params.tags) \u0026#34;Permalink\u0026#34; \u0026#34;!=\u0026#34; .Permalink) -}} 2\t{{- $.Scratch.Set \u0026#34;has_related\u0026#34; true -}} 3{{- end -}} 4 5{{ if $.Scratch.Get \u0026#34;has_related\u0026#34; }} 6\u0026lt;div class=\u0026#34;related-content\u0026#34;\u0026gt; 7\u0026lt;h3\u0026gt;See Also\u0026lt;/h3\u0026gt; 8{{ end }} 9\u0026lt;div class=\u0026#34;owl-carousel owl-theme\u0026#34;\u0026gt; 10\t{{- $num_to_show := .Site.Params.related_content_limit | default 5 -}} 11\t{{ range first $num_to_show (where (where .Site.Pages \u0026#34;.Params.tags\u0026#34; \u0026#34;intersect\u0026#34; .Params.tags) \u0026#34;Permalink\u0026#34; \u0026#34;!=\u0026#34; .Permalink) }} 12\t\u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt; 13\t{{ if .Params.Thumbnail }} 14\t\u0026lt;div class=\u0026#34;related-post__image\u0026#34;\u0026gt; 15\t\u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt; 16\t\u0026lt;img src=\u0026#34;{{ absURL .Params.Thumbnail }}\u0026#34; alt=\u0026#34;{{ .Title }}\u0026#34;/\u0026gt; 17\t\u0026lt;/a\u0026gt; 18\t\u0026lt;h4\u0026gt;\u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }} 19\t\u0026amp;ndash; 20\t\u0026lt;time datetime=\u0026#34;{{ .Date.UTC.Format \u0026#34;2006-01-02T15:04:05-0700\u0026#34; }}\u0026#34;\u0026gt; 21\t{{ .Date.Format \u0026#34;Jan 2, 2006\u0026#34; }} 22\t\u0026lt;/time\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/h4\u0026gt;\u0026lt;/div\u0026gt; 23 24\t{{ else }} 25\t\u0026lt;h4\u0026gt;\u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }} 26\t\u0026amp;ndash; 27\t\u0026lt;time datetime=\u0026#34;{{ .Date.UTC.Format \u0026#34;2006-01-02T15:04:05-0700\u0026#34; }}\u0026#34;\u0026gt; 28\t{{ .Date.Format \u0026#34;Jan 2, 2006\u0026#34; }} 29\t\u0026lt;/time\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/h4\u0026gt; 30\t\u0026lt;hr\u0026gt; 31\t\u0026lt;small\u0026gt;{{ .Summary | plainify | htmlUnescape }}\u0026lt;/small\u0026gt; 32 33\t{{ end }}\t34\t\u0026lt;/div\u0026gt; 35 {{ end }} Finished product Recommended posts carousel\rReference\nShow Related Posts in Hugo\nÁ∞°ÂñÆÂ•Ω‰∏äÊâãÁöÑÂúñÁâáËº™Êí≠ jQuery - Owl Carousel\n",
	  "pubDate": "2023-02-22T11:02:15+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/021en.png"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/hugo-multiple-comments/",
	  "title": "Implementing Multiple Comment Sections in a Static Website Generated by Hugo",
	  "summary": "\u003cp\u003eA Simple Implementation of Adding Multiple Comment Sections, with a Menu for Users to Choose their Preferred Commenting Method.\u003c/p\u003e",
	  "content": "A Simple Implementation of Adding Multiple Comment Sections, with a Menu for Users to Choose their Preferred Commenting Method.\nForeword If you have the habit of running a personal website, you probably want to engage with your visitors. There are many third-party tools available on the internet that enable quick and easy integration of comment sections, such as Disqus, Gitalk, Utterances, Commento, FB, and so on.\nHowever, based on my own reading habits and experiences, sometimes I refrain from leaving comments on articles that use Facebook Comments Plugin because I don\u0026rsquo;t want to reveal too much personal information. Or, if a non-technical person wants to ask a question, but the website only provides commenting functionality that requires a Github account, like Utterances or Gitalk, or requires additional account registration, like Disqus, the desire to leave a comment is quickly diminished.\nTherefore, I decided to implement all the comment sections that I could think of on my website. That way, if still no one leaves a comment, I can only blame the quality of my writing!üòÇ\nIdea My approach is to provide multiple comment sections, but initially hide them using CSS. Above the comment sections, a menu is provided for users to select their preferred commenting method. When a user clicks on a specific comment section button, the CSS is modified to display that comment section, while the others are set to hidden.\nCode In the HTML section, a button is added for each comment section that will be used. For this example, Facebook, Github, and Disqus will be utilized, resulting in three buttons. To avoid modifying the original theme of Hugo, the single.html file in the root directory\\layouts\\_default can be modified. If this file does not exist, it can be copied from\nroot directory\\themes\\theme\\layouts\\_default\\single.html.\nThe HTML code should be added below {{ partial \u0026quot;comments.html\u0026quot; . }} in the single.html file.\n1\u0026lt;nav class=\u0026#34;nav\u0026#34;\u0026gt; 2\t\u0026lt;button id=\u0026#34;fb-btn\u0026#34; class=\u0026#34;nav-item is-active\u0026#34; active-color=\u0026#34;#ACD6FF\u0026#34;\u0026gt;Facebook\u0026lt;/button\u0026gt; 3\t\u0026lt;button id=\u0026#34;github-btn\u0026#34; class=\u0026#34;nav-item\u0026#34; active-color=\u0026#34;#ACD6FF\u0026#34;\u0026gt;Github\u0026lt;/button\u0026gt; 4\t\u0026lt;button id=\u0026#34;disqus-btn\u0026#34; class=\u0026#34;nav-item\u0026#34; active-color=\u0026#34;#ACD6FF\u0026#34;\u0026gt;Disqus\u0026lt;/button\u0026gt; 5\t\u0026lt;span class=\u0026#34;nav-indicator\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; 6\u0026lt;/nav\u0026gt; Add an ID for each button in the HTML, so that JavaScript can use it to determine which comment section should be displayed.\nThe following CSS and JS are some materials found online, which are used to decorate the buttons and add dynamic click effects. The appearance can be modified according to personal preferences.\n.nav { display: inline-flex; position: relative; overflow: hidden; max-width: 100%; background-color: #fff; padding: 0 20px; border-radius: 40px; box-shadow: 0 10px 40px rgba(255, 255, 255, 0.8); display: flex; justify-content: center; } .nav-item { color: #83818c; padding: 20px; text-decoration: none; transition: .3s; margin: 0 6px; z-index: 1; font-family: \u0026#39;DM Sans\u0026#39;, sans-serif; font-weight: 500; position: relative; \u0026amp;:before { content: \u0026#34;\u0026#34;; position: absolute; bottom: -6px; left: 0; width: 100%; height: 5px; background-color: #dfe2ea; border-radius: 8px 8px 0 0; opacity: 0; transition: .3s; } } .nav-item:not(.is-active):hover:before { opacity: 1; bottom: 0; } .nav-item:not(.is-active):hover { color: #333; } .nav-indicator { position: absolute; left: 0; bottom: 0; height: 4px; transition: .5s; height: 5px; z-index: 1; border-radius: 8px 8px 0 0; } .nav-item { border: none; outline: none; background-color: #fff; } @media (max-width: 580px) { .nav { overflow: auto; } } 1const indicator = document.querySelector(\u0026#39;.nav-indicator\u0026#39;); 2\tconst items = document.querySelectorAll(\u0026#39;.nav-item\u0026#39;); 3 4\tfunction handleIndicator(el) { 5\titems.forEach(item =\u0026gt; { 6\titem.classList.remove(\u0026#39;is-active\u0026#39;); 7\titem.removeAttribute(\u0026#39;style\u0026#39;); 8\t}); 9 10\tindicator.style.width = `${el.offsetWidth}px`; 11\tindicator.style.left = `${el.offsetLeft}px`; 12\tindicator.style.backgroundColor = el.getAttribute(\u0026#39;active-color\u0026#39;); 13 14\tel.classList.add(\u0026#39;is-active\u0026#39;); 15\tel.style.color = el.getAttribute(\u0026#39;active-color\u0026#39;); 16\t} 17 18 19\titems.forEach((item, index) =\u0026gt; { 20\titem.addEventListener(\u0026#39;click\u0026#39;, (e) =\u0026gt; { handleIndicator(e.target) }); 21\titem.classList.contains(\u0026#39;is-active\u0026#39;) \u0026amp;\u0026amp; handleIndicator(item); 22\t}); At this point, you will see a menu with no functionality yet. Below the menu is the section to display the comment section. Since I prefer the default is the Facebook comment function, added style=\u0026quot;display: block;\u0026quot; to the Facebook section, added style=\u0026quot;display: none;\u0026quot;to the other comment sections.\n(Note that this article does not provide a tutorial for how to add various comment sections.)\n1 2\u0026lt;div id=\u0026#34;disqus-comments\u0026#34; class=\u0026#34;comments markdown\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt; 3\t{{ partial \u0026#34;disqus.html\u0026#34; . }} 4\u0026lt;/div\u0026gt; 5 6\u0026lt;div id=\u0026#34;utterances-comments\u0026#34; class=\u0026#34;comments markdown\u0026#34; style=\u0026#34;display: none;\u0026#34;\u0026gt; 7\t\u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;your repo\u0026#34; issue-term=\u0026#34;pathname\u0026#34; 8\ttheme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; 9\t\u0026lt;/script\u0026gt; 10\u0026lt;/div\u0026gt; 11 12\u0026lt;div id=\u0026#34;facebook-comments\u0026#34; class=\u0026#34;fb-comments\u0026#34; data-href=\u0026#34;your url\u0026#34; data-width=\u0026#34;100%\u0026#34; data-numposts=\u0026#34;5\u0026#34; style=\u0026#34;display: block;\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; Next, use JavaScript to modify the CSS when a button is clicked, achieving the effect of hiding or displaying the comment sections.\n1 // get button element 2\tconst githubBtn = document.getElementById(\u0026#34;github-btn\u0026#34;); 3\tconst disqusBtn = document.getElementById(\u0026#34;disqus-btn\u0026#34;); 4\tconst fbBtn = document.getElementById(\u0026#34;fb-btn\u0026#34;); 5 6\t// get message board element 7\tconst githubBoard = document.getElementById(\u0026#34;utterances-comments\u0026#34;); 8\tconst disqusBoard = document.getElementById(\u0026#34;disqus-comments\u0026#34;); 9\tconst fbBoard = document.getElementById(\u0026#34;facebook-comments\u0026#34;); 10\t// Add click event listeners to each button 11\tgithubBtn.addEventListener(\u0026#34;click\u0026#34;, function () { 12\tgithubBoard.style.display = \u0026#34;block\u0026#34;; 13\tdisqusBoard.style.display = \u0026#34;none\u0026#34;; 14\tfbBoard.style.display = \u0026#34;none\u0026#34;; 15\t}); 16\tdisqusBtn.addEventListener(\u0026#34;click\u0026#34;, function () { 17\tgithubBoard.style.display = \u0026#34;none\u0026#34;; 18\tdisqusBoard.style.display = \u0026#34;block\u0026#34;; 19\tfbBoard.style.display = \u0026#34;none\u0026#34;; 20\t}); 21\tfbBtn.addEventListener(\u0026#34;click\u0026#34;, function () { 22\tgithubBoard.style.display = \u0026#34;none\u0026#34;; 23\tdisqusBoard.style.display = \u0026#34;none\u0026#34;; 24\tfbBoard.style.display = \u0026#34;block\u0026#34;; 25\t}); Use the document.getElementById method to select each button element and then add a click event listener to it. When the button is clicked, modify the CSS to show or hide the corresponding comment section.\nConclusion Finished Product!!\nPart of the reason I had this idea was also because I have a hard time choosing between different third-party comment plugins, so I decided to use all of them. üòé\n",
	  "pubDate": "2023-02-16T10:11:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/020en.png"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/life/valentine-gift-marimo/",
	  "title": "Valentine's Day Gift Idea: Marimo Moss Ball (Basic Introduction, Care Instructions, and Symbolic Meaning)",
	  "summary": "\u003cp\u003eThe Marimo moss ball is truly a plant for lazy people. It\u0026rsquo;s easy to care for, extremely therapeutic mentally, and has symbolic significance.\u003c/p\u003e",
	  "content": "The Marimo moss ball is truly a plant for lazy people. It\u0026rsquo;s easy to care for, extremely therapeutic mentally, and has symbolic significance.\nHave you ever felt bored and wanted to have some pets or plants, but also worried that you don\u0026rsquo;t have the time to take care of them? If you\u0026rsquo;re really lazy or too busy to go out, then Marimo might be a perfect choice for you.\nMarimo is a very unique and precious plant, and I recently bought a few of them. üòé\nLet me introduce you to some basic knowledge about Marimo.\nüîéBasic Introduction Marimo is a very special plant, which is a type of green algae that grows in lakes. The name of this plant comes from the Japanese word \u0026lsquo;„Åæ„Çä„ÇÇ\u0026rsquo; which means \u0026lsquo;seaweed ball\u0026rsquo;, because of its unique spherical shape resembling a ball of fur. Marimo is a traditional plant in Northern Europe and Japan, and is believed to have special symbolic significance.\nNot only is it a beautiful decoration, but there are also many other advantages to Marimo. Marimo is very easy to care for, requiring only regular water changes, maintaining clean water quality, and ensuring that it receives sufficient sunlight (but be sure not to expose it to direct sunlight, as indoor artificial light sources are already sufficient).\nIn addition, Marimo has good air purifying effects, as it can absorb some harmful substances and maintain fresh indoor air (although a small ball may not have a significant effectüòÇ).\nüí°Types of Marimo Most of the ones you can see on the market are green spherical shapes, but in fact, Marimo not only in spherical shapes. The main body of the Marimo is composed of small filamentous algae that branch out from the center, and depending on their growing environment, they can be classified as epiphytic, floating, or colonial types. Despite the changes in shape, the contents are all the same.\nMarimo of different shapes\rüóæNative Habitat Marimo has only been found in Japan, Iceland, and Estonia, and the closest place to us where it originates from is Lake Akan in Hokkaido, Japan. Lake Akan is a beautiful lake located in the eastern part of Kushiro, Hokkaido, and is one of the main tourist attractions in the area. The water in the lake is clear and the ecological environment is rich, attracting many tourists to visit.\nThe discovery of marimo in Lake Akan can be traced back to the early 1970s, when a researcher conducting a lake survey found the existence of this microorganism. With further investigation, it was found that marimo\u0026rsquo;s quantity in Lake Akan was enormous, even becoming a major attraction in the area. The massive propagation of marimo is closely related to the unique ecological environment of Lake Akan. The surrounding area of Lake Akan has vast areas of primeval forest, which provides abundant organic matter for the growth of organisms in the lake. At the same time, the water quality of Lake Akan is clear and free of pollution, which also provides a good environment for the growth of marimo.\nDue to the threat of environmental changes and resource depletion, wild marimo have been protected and collecting and selling them is prohibited. Therefore, the marimo sold as souvenirs in Hokkaido are actually artificially cultured marimo.\nApart from Lake Akan, traces of marimo have also been found in Lake Biwa and the Fuji Five Lakes outside of Hokkaido. However, except for Lake Akan, no other lakes have been able to cultivate the ball-shaped Marimo in a colonial form.\nNative marimo at the bottom of Lake Akan\rü´∂Symbolic Meaning Marimo are an excellent choice as a Valentine\u0026rsquo;s Day gift because they have symbolic meaning. In Nordic and Japanese cultures, Marimo are regarded as symbols of love and loyalty, making them an ideal choice for a significant other. Additionally, the Marimo can represent a long-lasting relationship as it can grow for a considerable length of time while maintaining its beautiful appearance.\nIn summary, Marimo is a very special and symbolic plant. If you are looking for a unique gift, Marimo is a worthwhile choice. It not only represents your feelings, but also provides a lasting keepsake that can deepen your love. In addition to its symbolic meaning, Marimo also represents life and hope, with its green color symbolizing growth and prosperity. In busy and exhausting lives, Marimo can serve as a beautiful reminder of the important values and good things in life. It truly is a perfect gift!\ndidn\u0026#39;t know when my Marimo would start to grow fuzz at the beginning of raising it.ü´•\rAlthough I initially wanted to raise it by myself, I felt it would be boring, so I invited my partner to raise one together. One for each of us - so romantic of engineers.üòé\nüå±Care Instructions 1.Water I recommend using neutral mineral water that is rich in minerals for Marimo\u0026rsquo;s survival. Avoid using RO (reverse osmosis) water, which may be clean for humans but lacks nutrients and can be harmful to Marimo. If using tap water, note that it usually contains chlorine, which can be harmful to aquatic plants and animals. Therefore, it is best to let the tap water sit for one to two days before use to allow harmful substances to evaporate.\nGenerally, water should be changed every 1-2 weeks, but during the summer months, the water quality may deteriorate, so it is recommended to increase the frequency of water changes.\n2.Light\nAs a plant, Marimo requires light for photosynthesis to grow properly. However, Marimo lives at the bottom of lakes, and strong sunlight can cause great harm to them. Therefore, when keeping Marimo, it is important to avoid direct sunlight. Indoor lighting and artificial light are sufficient for their healthy growth.\n3.Temperature\nMarimo\u0026rsquo;s hometown is at the bottom of the cold lakes in Hokkaido, and it has a strong adaptability to low temperatures but cannot tolerate high temperatures at all. Marimo can grow healthily in water temperatures between 10-25 degrees Celsius.\nHowever, when the water temperature exceeds 30 degrees Celsius, Marimo may die due to high temperature. Therefore, in the summer, Marimo can be placed in a covered container and put in the refrigeration room to avoid heat. Of course, if you can change the water for Marimo on cool days in the summer and let them undergo photosynthesis under indoor lighting or artificial light, they will be even healthier!\nüü¢Conclusion At first, I only heared about this interesting creature because my brother help his colleague who was on a long vacation to changing the water . After doing some research, I realized that it\u0026rsquo;s suitable for someone like me who loves aquatic plants but is also super lazy. Although it may seem simple, there are still many details to pay attention to, and I hope it can grow up successfully.\nLater, I thought it would be a good idea to invite my parents to join in on the fun, so I bought a larger one and kept it at my place temporary. The 2cm marimo already feels fuzzy!üòÄ\nReference\nÊàëÊÑõmarimo‰∏≠ÊñáÊé®Âª£Á´ô Photo provided\nÈòøÂØíÊπñ„ÅÆ„Éû„É™„É¢‰øùÂÖ®Êé®ÈÄ≤ÂßîÂì°‰ºö ",
	  "pubDate": "2023-02-14T10:11:38+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/019mario.jpg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/sql-partition-by/",
	  "title": "SQL-implement data partitioning (partition by)",
	  "summary": "\u003cp\u003eImplement data partitioning using SQL partition by and compare the performance with LINQ.\u003c/p\u003e",
	  "content": "Implement data partitioning using SQL partition by and compare the performance with LINQ.\nForeword The data I received this time is the GPS positioning data of the trucks, including truck-related information, latitude and longitude, direction of travel, current speed, etc. The data volume is quite large because the data update frequency is once per 15 minutes. However, I only need to provide the latest information of all trucks, so that the front-end can display it on the map.\nUsing LINQ Intuitively, it\u0026rsquo;s just to directly retrieve all the data, use GroupBy to group the vehicles by license plate, sort the update time in descending order, and then choose the first item in each group.\nLINQ is very convenient!!üòÇ\n1var vehicles = dbContext.VehicleData.ToList(); 2 3var latestVehicleData = vehicles 4 .GroupBy(v =\u0026gt; v.LicensePlate) 5 .Select(g =\u0026gt; g.OrderByDescending(v =\u0026gt; v.Timestamp).First()); But the data volume is large, the performance of directly retrieving all the data is certainly not good.\nSo I have to use SQL queries to improve efficiency. To partition the data table, then use the partition by to group and use row_number() to get the number after sorting. By taking the first group, you can get the same result as LINQ.\nUsing partition by 1SELECT * 2-- Create a subquery to get the latest direction for each vehicle 3FROM ( 4 SELECT license_plate, direction, 5 -- Use the row_number() function to assign a unique row number to each row, 6 -- partitioned by license plate and ordered by timestamp in descending order 7 row_number() over (partition by license_plate order by timestamp desc) as row_num 8 FROM VehicleData 9) subquery 10-- Only return the rows where the row number is equal to 1, which corresponds to the latest data 11WHERE row_num = 1; Conclusion Although LINQ is easy to use, one must consider the overall data volume and what data will be used when initially retrieving the data. If other complex data operations are required, then consider using LINQ to increase efficiency.\nFun fact\nUpon seeing the data, I realized that if the vehicle is a trailer truck, it means that both the front and the back have their own separate license plates. üòé\n",
	  "pubDate": "2023-02-07T00:00:00Z",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/csharp-multithreading/",
	  "title": "C# Multi-threading principle, asynchronous usage and parameter with custom objects",
	  "summary": "\u003cp\u003eThe principle of C# multi-threading, asynchronous usage and parameter with custom objects records.\u003cbr\u003e\nNone of the projects encountered so far have a hard requirement to use multi-threading. This article is to record my own understanding and learning process.\u003c/p\u003e",
	  "content": "The principle of C# multi-threading, asynchronous usage and parameter with custom objects records.\nNone of the projects encountered so far have a hard requirement to use multi-threading. This article is to record my own understanding and learning process.\nForeword I had a project that has a file pre-store function, because there are new files to be updated every day, it needs to be executed pre-store function every day. The original way of programming is to download part A and then download part B. I would like to try new way with multi-threaded. The main reason why I want to try jsut it is too annoying when I test part B every time in the process, I always need to wait a lot of time (the amount of data is too large) until part A to finish.\nPrinciple First you must understand the difference between Program /Process/ Thread.\nProgram : Program code that has not yet been loaded into memory. Process : For a Program that has been executed and loaded into the memory, each line of code in the program may be executed by the CPU at any time. Thread : There will be many Threads in the same Process, and each Thread is responsible for a certain function. Take the factory as an example:\nProgram = Detailed manufacturing drawings and staffing of the factory\nProcess = factory already in operation\nThread = Every worker in the factory\nWhen programs are executed, they are executed line by line in order, similar to the way workers in a factory complete their tasks in order. This will cause the problem that the program is executed for too long and inefficiency. If the workers can perform their own tasks at the same time (multiple threads), the efficiency will be much faster, but the disadvantage is that when the hardware is insufficient (similar to factory space too small), too many workers performing tasks together will rob each other of resources and cause the program Deadlock.\nUse Situations where multithreading is commonly used include:\nMultiple tasks need to be executed at the same time, and there is no relationship between these tasks. For example, downloading multiple files at the same time or calculating multiple values. Need to wait for a long-running task, but don\u0026rsquo;t want to make the interface uninteractive. For example, executing a database query in another execution line running at the same time. Need to perform a task, but do not want to block other tasks because of it. For example, executing a network connection in another execution line running at the same time. Tasks need to be executed on multiple cores to take advantage of the performance of multi-core CPUs. Example 1using System.Threading; 2 3Thread thread1 = new Thread(Task1); 4thread1.Start(); 5 6Thread thread2 = new Thread(Task2); 7thread2.Start(); 8 9Console.WriteLine(\u0026#34;Completed\u0026#34;); 10Console.ReadKey(); 11 12static void Task1() 13{ 14 for (int i = 0; i \u0026lt; 5; i++) 15 { 16 Console.WriteLine(\u0026#34;Task 1\u0026#34;); 17 Thread.Sleep(1000); 18 } 19} 20 21static void Task2() 22{ 23 for (int i = 0; i \u0026lt; 5; i++) 24 { 25 Console.WriteLine(\u0026#34;Task 2\u0026#34;); 26 Thread.Sleep(1000); 27 } 28} Result:\nCompleted\rTask 1\rTask 2\rTask 1\rTask 2\rTask 1\rTask 2\rTask 1\rTask 2\rTask 2\rTask 1 Why does \u0026ldquo;Completed\u0026rdquo; appear first?\nThis is because in the above code, two threads are started at the same time, and the main thread does not wait for either thread to complete, but immediately writes \u0026ldquo;Completed\u0026rdquo; and ends the program. Therefore, no matter which thread completes first, the program will write \u0026ldquo;Completed\u0026rdquo; first.\nIf you want to wait for thread2 to complete, you can use thread2.Join() to make the main thread wait for thread2 to complete, and then execute Console.WriteLine(\u0026quot;Completed\u0026quot;);.\nPass parameters with custom objects In actual use, the thread often needs to be executed with parameters. You can use ParameterizedThreadStart. ParameterizedThreadStart is a delegate, which points to a method with an object parameter and no return value. So when using ParameterizedThreadStart delegation, you need to pass in an object type parameter.\nBut usually the data type is not object, most of them are JObject or self-defined types, the following is the implementation method.\nCreate corresponding objects for different execution threads\n1// Create an object to hold the list 2class ListContainer1 3 { 4 public List\u0026lt;dynamic\u0026gt; TheList { get; set; } 5 } 6class ListContainer2 7 { 8 public List\u0026lt;customize\u0026gt; TheList { get; set; } 9 } Pass List\u0026lt;dynamic\u0026gt; customize_obj to Task1, get customize_obj in Task1 function and execute\n1List\u0026lt;dynamic\u0026gt; customize_obj = new List\u0026lt;dynamic\u0026gt;(); 2var listContainer1 = new ListContainer1 { TheList = customize_obj }; 3// create new thread 4Thread t1 = new Thread(new ParameterizedThreadStart(Task1)); 5// pass parameters and start thread 6t1.Start(listContainer1); 7 8 9void Task1(Object listContainer) 10{ 11 // get customize_obj 12 ListContainer1 container = (ListContainer1)listContainer; 13 List\u0026lt;dynamic\u0026gt; customize_obj = container.TheList; 14 //... 15 //code 16 //... 17} Asynchronous programming with async and await(to be continued)",
	  "pubDate": "2023-01-19T11:49:15+08:00",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/csharp-enum/",
	  "title": "C# Enum Usage Record",
	  "summary": "\u003cp\u003eC#-Enum (enumeration) uses and uses the extension method to obtain detailed description records.\u003c/p\u003e",
	  "content": "C#-Enum (enumeration) uses and uses the extension method to obtain detailed description records.\nEnumeration is to name and classify some sets of constants. Taking the example of life as a week is a kind of enumeration.\nweek int Monday 1 Tuesday 2 Wednesday 3 Thursday 4 Friday 5 Saturday 6 Sunday 7 If there is such a fixed set that needs to be used frequently, it can be implemented by enumeration in the program. In addition to being easy to manage, if you want to use a string as a key in a program, you can clearly know the purpose, and there will be no trouble with capitalization or spelling mistakes.\nExample of use If there is no preset corresponding number, the value is given in order\nVeiw = 0\nAdd = 1\nEdit = 2\u0026hellip;\n1public enum LogBehavior 2 { 3 Veiw, Add, Edit, Delete, Search, Login, Send 4 } or\n1public enum Month 2 { 3 Jan = 1, Feb = 2, Mar = 3, Apr = 4, May = 5, Jun = 6, 4 Jul = 7, Aug = 8, Sep = 9, Oct = 10, Nov = 11, Dec = 12 5 } The way to get enum parameters\n1Console.WriteLine(LogBehavior.Veiw); 2\u0026gt;\u0026gt; Veiw 3 4Console.WriteLine(LogBehavior.Veiw.GetHashCode()); 5\u0026gt;\u0026gt; 0 6 7string month = Month.Jan.ToString(); 8Console.WriteLine(month); 9\u0026gt;\u0026gt; Jan 10 11int monthValue = (int)Month.Jan; 12Console.WriteLine(monthValue); 13\u0026gt;\u0026gt; 1 Practical application example The backend needs to add an extensibility object to each returned API(List\u0026lt;dynamic\u0026gt;), you can add objects according to the situation, but you need to add specific parameters that let front-end know properties of the objects.\nIt can be easily managed by using enumeration. If you want to return the specific parameter name of the object, other people can directly use it. There will be no artificial string misspelling or capitalization errors. You can also directly add extensions if you need other specific parameters later.\n1public enum prop 2{ 3 [Description(\u0026#34;Synchronize record information\u0026#34;)] 4 SyncInfo, 5 [Description(\u0026#34;Version information\u0026#34;)] 6 VersionInfo 7} If you want to get the content of Description, you can use this method to expand, if there is no content of Description, will return the original name.\n1public static string GetDescription(this Enum value) 2 { 3 var field = value.GetType().GetField(value.ToString()); 4 var attribute = field?.GetCustomAttributes(typeof(DescriptionAttribute), false).FirstOrDefault() as DescriptionAttribute; 5 return attribute == null ? value.ToString() : attribute.Description; 6 } 1Console.WriteLine(prop.SyncInfo.GetDescription()); 2\u0026gt;\u0026gt; Synchronize record information References\nExtension Methods\nC# enum ÂàóËàâ ÂêåÊôÇÂÑ≤Â≠òÂÖßÂÆπÂÄºÂíå‰∏≠ÊñáË™™Êòé ÊïôÂ≠∏\n",
	  "pubDate": "2023-01-15T08:49:07+08:00",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/sql-merge-into-statement/",
	  "title": "SQL-merge into statement",
	  "summary": "\u003cp\u003eI was assigned to implement the function of synchronizing from different databases.\u003cbr\u003e\nThe company currently has an ERP system.\u003cbr\u003e\nAll data sources are based on this standard, so that customers can create , update, write to ERP,\u003cbr\u003e\nand synchronize with ERP in their own systems\u0026hellip;\u003c/p\u003e",
	  "content": "I was assigned to implement the function of synchronizing from different databases.\nThe company currently has an ERP system.\nAll data sources are based on this standard, so that customers can create , update, write to ERP,\nand synchronize with ERP in their own systems\u0026hellip;\nwhatever ~ In short, after the data is written into the EPR, it is based on the ERP, and because the ERP will update the data regularly,\nThe data that has been written into the ERP in the customer\u0026rsquo;s own system needs to be synchronized frequently.\nSolution The original idea was to directly fetch the data from the two databases and compare them one by one. If there is a difference in the comparison, directly generate the SQL statement of the update in the project.\nFinal step executes the statement of update together. (I think it is quite intuitive, anyway, it is hard work)\nHowever, the colleague suggested that I can use the method of merge into and implement it directly in the database, so I check the usage method.\nCode Merge into statement is as followsÔºö\n1--sql 2MERGE INTO table_a target -- Data to be synchronized (own system) 3USING (select * from table_b ) source -- Original data (ERP) 4ON (target.A=source.A and target.B=source.B) -- join condition 5-- If the data is found, execute the following statement 6WHEN MATCHED THEN 7 UPDATE -- execute update 8 SET target.E = source.E, 9 target.D = source.D 10-- If no data, execute the following statement (can be omitted) 11WHEN NOT MATCHED THEN 12 INSERT (target.E,target.D) VALUES (source.E,source.D); Conclusion Concise statement is quite convenient, and there is no need to execute multiple update statement at the same time.\nHowever, there is no comparison of the actual program execution time difference between the two methods, but the Merge into method does not need to compare the fields one by one to check if there is a difference, Instead, as long as there is a comparison to the corresponding key, the update statement is directly executed, and it is executed directly in the database, which should be much faster.\nBut the disadvantages are also obvious, because no matter if data is changed, the update statement is executed directly. For the data table with log records, it should increase the load and data volume quite a lot.\n",
	  "pubDate": "2022-12-22T10:44:38+08:00",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/tutorials/leetcode-lru-cache-medium-python/",
	  "title": "Leetcode(Medium) 146. LRU Cache (Python)",
	  "summary": "\u003cp\u003eLRU Cache can be regarded as an interesting topic that I have encountered, and I feel that it is also a concept that will be used in many systems.\u003cbr\u003e\nUnfortunately, I never encountered this problem on my job yet\u0026hellip;\u003c/p\u003e",
	  "content": "LRU Cache can be regarded as an interesting topic that I have encountered, and I feel that it is also a concept that will be used in many systems.\nUnfortunately, I never encountered this problem on my job yet\u0026hellip;\nJust record my solving method of problem\nProblem Design a data structure that follows the constraints of a Least Recently Used (LRU) cache.\nConcept The main purpose of the cache mechanism is to keep the more frequently used data and eliminate the infrequently used data, so that users can obtain the information they want more quickly when executing the program.\nUse HashMap and DoubleLinkedList to implement, Each event represents the key and value of the HashMap, and the value is the node generated by DoubleLinkedList. This way you can know the actual order of the nodes.\nIn order to update the state of the node while calling functions get and put, two additional functions are required\ndef insertInToHead(self, node):\nInsert this node to the first position of LinkedList (behind the head, in front of the original first node)\ndef removeNode(self, node):\nRemove this node (connect the node of front and back nodes)\nWhether it is calling functions get or put, above of two functions are used when changing the state of the node.\nDetail The initial data structure of LRUCache needs to include dummyHead and dummyTail, so as to know the most frequently used position (the one after dummyHead) and the least frequently used position (the one before dummyTail) When calling functions put, if the final HashMap length is greater than capacity, you need to delete the value (key:value) in the HashMap first. The value of the key is the last node of the LinkedList (the node in front of dummyTail), and then delete this node. Code 1class LinkListed: 2 def __init__(self, key, val): 3 self.key = key 4 self.val = val 5 self.prev = None 6 self.next = None 7 8class LRUCache: 9 def __init__(self, capacity: int): 10 self.size = capacity 11 self.dic = {} 12 self.dummyHead = LinkListed(0, 0) 13 self.dummyTail = LinkListed(0, 0) 14 self.dummyHead.next = self.dummyTail 15 self.dummyTail.prev = self.dummyHead.next 16 17 def get(self, key: int) -\u0026gt; int: 18 if key not in self.dic: return -1 19 node = self.dic[key] 20 self.removeNode(node) 21 self.insertInToHead(node) 22 return node.val 23 24 def put(self, key: int, value: int) -\u0026gt; None: 25 if key in self.dic: 26 node = self.dic[key] 27 node.val = value 28 self.removeNode(node) 29 self.insertInToHead(node) 30 else: 31 node = LinkListed(key, value) 32 self.dic[key] = node 33 self.insertInToHead(node) 34 35 if len(self.dic) \u0026gt; self.size: 36 del self.dic[self.dummyTail.prev.key] 37 self.removeNode(self.dummyTail.prev) 38 39 def insertInToHead(self, node): 40 node.next = self.dummyHead.next 41 self.dummyHead.next.prev = node 42 self.dummyHead.next = node 43 node.prev = self.dummyHead 44 return 45 46 def removeNode(self, node): 47 node.next.prev = node.prev 48 node.prev.next = node.next 49 return ",
	  "pubDate": "2022-12-04T19:44:38+08:00",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/residential_area/",
	  "title": "Cebu Residential Area Strolling",
	  "summary": "\u003cp\u003eThe record accidentally entered a local residential area (Cabantan Street) that is usually not dared to enter.\u003c/p\u003e",
	  "content": "The record accidentally entered a local residential area (Cabantan Street) that is usually not dared to enter.\nDate : 2020.01.24\nForeword The traffic in downtown Cebu is usually the most congested on Fridays, because the locals always go out to eat and drink after a hard work week, and I chose to take Angkas in order to catch up with the dinner. Because the driver chose to take the side road instead of the main road in order to reduce the chance of traffic jam, I accidentally entered the local residential area (Cabantan Street) that I usually dare not enter.üòÜ\nCabantan Street Location Cabantan Street is located in the Luz district. You can see that the Luz district is next to Ayala Malls, so I usually take the Cardinal Rosales Ave from the Mabolo district where the school locate to Ayala Malls. I usually notice the street of Cabantan Street when passing by. I think it is a very stylish street, but because of safety considerations, I have never entered it.\nGo to an all-you-can-eat seafood restaurant (Isla Sugbo Seafood City) I had appointment with my friends at Isla Sugbo Seafood City that night, mainly because I haven‚Äôt tried all-you-can-eat restaurants outside of Taiwan, and particularly restaurants mainly serve seafood. I really wanted to experience the quality of the restaurant. Overall, the dining environment is perfect. All kinds of seafood are directly selected on the spot and then the cooking method is selected by yourself. In fact, I just started to catch up on all the English vocabulary related to cooking. üòÜThe only downside is that it takes a long time to serve the food, probably because there are too many customers. At present, it seems that it is permanently closed due to the epidemic situation, so I will not continue to introduce it in detail. Because the location of this store is just at the exit above Cabantan Street, if you take Grab, the approximate route is to go around the main roads like Mindanao Ave or Pope John Paul II Ave on both sides, because it happened to be in a hurry that day, I chose to take Angkas, the driver took me directly to the Cabantan Street path.\nResidential Area(Cabantan Street) Pass by often but never go in~ In Cebu, if the location is not far away, I often choose go out by walk, because of safety considerations, I basically take avenue, and I dare not go directly to the alleys. After all, it is quite conspicuous for Chinese to walk on the streets of Cebu.\nThis time, because I was in a hurry, I accidentally entered this small street that I think has a very Southeast Asian atmosphere, and particularly at night. üòé\nIt\u0026rsquo;s a pity that I didn\u0026rsquo;t have the opportunity to actually walk around, it was really full of people. Conclusion Although it was just passing by in a hurry, the cultural buildings in the local residential area are really special, and you must actually go by walk if you have the opportunity.üòÜ\nAttached is a daytime Street View ",
	  "pubDate": "2022-12-04T16:28:53+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/015Cabantan_Street.jpg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/bohol-island-day-tour-tarsier-chocolate-hills/",
	  "title": "Cebu-Bohol island Day Tour (Tarsier \u0026 Chocolate Hills)",
	  "summary": "\u003cp\u003eWhen I came to Cebu to study English, I didn‚Äôt plan to travel to other islands at first, but I wanted to buy some textbooks in order to prepare for IELTS. Since it is difficult to buy textbooks locally, I chose to order directly from the online platform. And my family will take the book for me from Taiwan and experience Cebu tourism by the way.\u003c/p\u003e",
	  "content": "When I came to Cebu to study English, I didn‚Äôt plan to travel to other islands at first, but I wanted to buy some textbooks in order to prepare for IELTS. Since it is difficult to buy textbooks locally, I chose to order directly from the online platform. And my family will take the book for me from Taiwan and experience Cebu tourism by the way.\nDate : 2020.01.26\nForeword At this time, I had been in Cebu for about three months. I originally just wanted to stay here for holiday, but after being influenced by the whole overseas atmosphere, I decided to continue to improve my English skill, so I chose to study for IELTS. The teaching at the Cebu language school is actually to let you have an environment to better adapt to the exam or basic oral speaking, but in terms of exam strategy, I still believe that directly referring to experienced people can improve the exam grade, so I decided to buy books about how to improve IELTS grade. Originally, my family wanted me to go back to Taiwan to pick it up or send it to me by delivery. But I thought it was a good opportunity to let my parents know what a good environment here is, so I asked them to bring the book to me and I will guide them for traveling.üòÜ\nItinerary Planning Traveling to Cebu is actually very convenient. Whether it is a local travel agency or directly through a travel platform, you can purchase the itinerary directly. Usually, there will be a driver who will drive and guide the entire travel itinerary. It can be said that there is no need for planning at all, as long as you prepare money. This time, I was traveling with my family, it\u0026rsquo;s enough as long as it was safe and convenient, I chose a one-day trip to Bohol Island directly. After taking a boat from the city of Cebu to the port and then to Bohol Island, it\u0026rsquo;s a happy day trip itinerary experience.üòé\nCebu‚ûùBohol Because it is a one-day trip, the itinerary is quite urgent, and we must arrive at the place where we board the boat early in the morning. There will be a driver to pick you up to the boarding point in Cebu City (the driver make an appointment with us when we come back he will on the waiting place for waitting us ). The next step is to buy a ticket and board the Ocean Fast Ferries to tagbilaran port.\nThe shift at six o\u0026rsquo;clock shows how early you need to get up and prepare Arrive at Tagbilaran port It takes about two hours during sailing. After disembarking, we have to find the exclusive driver who has agreed to wait at the port, and let him lead us the rest of the itinerary.\nTarsier Conservation At that time, I didn\u0026rsquo;t know much about this kind of monkey. I just heard that there is a very unique small tarsier on Bohol Island, so I was very much looking forward to seeing it. One of the itineraries when I came to Bohol Island was to come to the tarsier Conservation. I chatted with the driver on the way there. The driver said that it could be touched by people before, but because this kind of behavior made the tarsier very scared, it was so afraid that suicide due to insanity! So it\u0026rsquo;s banned now and can only be viewed from a distance.\nWhen I actually arrive at the conservation area, will be led into the park by a local guide, mainly to help find the tarsiers in the trees, because it is really small and the number is very rare, so it is really difficult to find. When I actually saw it, the body proportions really subverted my general imagination of monkeys, and tarsiers are nocturnal animals, so when you visit during the day, you will basically only see them sleeping. But what I photographed were those who opened their eyes but didn\u0026rsquo;t move. I really don\u0026rsquo;t know if they are awake or really sleeping.üòÇ Chocolate Hill After visiting the tarsiers, the next step is to go to the Chocolate Hills, which is one of the popular attractions in Bohol Island. During the rainy season, the Chocolate Hills are grassy green, and when summer comes, the hills will turn brown (chocolate color), so it is known as the Chocolate Hills. It\u0026rsquo;s a pity that the time we went happened to be the transition period between the rainy season and summer, and the temperature was not very hot, so the vegetation on the hills had not yet turned brown. However, the actual view is still very magnificent.\nYou have to climb a certain hill to see multiple Chocolate Hills.üòé Loboc River I think this should be an itinerary added by the travel agency XD. The itinerary is basically to take a boat and have lunch to enjoy the tropical jungle scenery. Halfway through, you will get off the boat to experience the culture of the local people, which is a bit similar to the aboriginal culture in Taiwan. I think the good point is that the whole environment is full the Southeast Asian, and it feels like returning to the virgin forest.\nThe entertainment of the local children is to climb to the top of the tree and then jump into the river, I really want to tryüòÜ Conclusion The rest of the itinerary includes visiting local wild plants, zoos, and some churches and historic site. In fact, the whole itinerary is quite rich, but because I didn‚Äôt do my homework beforehand moreover it‚Äôs a totally different culture with mine, I don‚Äôt have any special ideas about some historic site. Basically, it‚Äôs just give a hurried and cursory glance. But just to see Tarsier, I think this trip is worth it. During the whole trip, the driver was also very enthusiastic to explain the relevant attractions to us, but the accent was a bit heavy. It‚Äôs a good thing I‚Äôve been in Cebu for a while, so I can chat with him a little bit. After the end, the driver will send us back to the place where we boarded the boat. After we arrived at the main island of Cebu, the sun just went down, and when we went back to the place where I agreed with the driver. The driver was already waiting for us. A fulfilling day trip is end. ",
	  "pubDate": "2022-12-03T16:28:53+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/014IMG_1491.jpeg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/jeepney-anjo-world-theme-park-day-trip/",
	  "title": "Philippines Jeepney \u0026 Cebu Anjo World Theme Park Day Trip",
	  "summary": "\u003cp\u003eExperienced the long-awaited Cebu common transportation Jeepney and the largest and newest theme park in Cebu : \u003cstrong\u003eAnjo World Theme Park\u003c/strong\u003e.\u003c/p\u003e",
	  "content": "Experienced the long-awaited Cebu common transportation Jeepney and the largest and newest theme park in Cebu : Anjo World Theme Park.\nDate : 2020.01.11\nForeword There are basically two transportation options for foreigners in Cebu are taxis (Grab) and motorcycles (Angkas), or you can just walking like me. But what you can see the most common on the road is the jeepney (bus), which is also the most common means of transportation for locals. I have always wanted to try it, but seeing that the bus is always full of Filipinos, and they can stop at any time, without a bus stop sign, I don‚Äôt even know the entire driving line, and I am afraid that I will be taken to the wilderness I can\u0026rsquo;t go back to my home, so I still dare not experience alone. üòÇ\nFortunately, I beg my local friends to take me for a ride and she promised me, so we took the jeepney for the whole journey, including transfers with Anjo World Theme Park, the largest and newest theme park in Cebu, as my destination. (When I was writing this article, I searched for information by the way, and found that the route can now be queried through google map!!) Get on the jeepney at the departure point I live in the Mabolo area, which is some distance away from Anjo World Theme Park. You can see that I need to transfer other jeepney and then can arrive at the destination. My personality is when I go to a place, if someone already familiar this place, I will not deliberately find the way. So the whole trip, I was in a coma, anyway, I just believed in my friends. üòéThe starting location is tejero (villa gonzalo). When I came here, I realized that I had to wait for the driver to start the car. Because there was no one before the car started, we sat directly in the passenger seat. At first, I wondered why I came here to wait for the bus. It\u0026rsquo;s easy to get on the jeepney through waving on the road, maybe the reason is if you are at the departure point, you can easily find a seat.\nThe appearance and interior environment of each jeepney are actually very different. It depends entirely on driving preferences. This one is a bit old, but there are still many jeepneys that look good. A bit similar to the concept of refitted vehicle in Taiwan? I just thought it was quite cool to sit in the passenger front seat in a jeepney for the first time, so I took a picture. Then we hit the road. The signboard in front is the driving route, but for foreigners who are completely unfamiliar with the local area, it can be said that they have no idea where to go. Transfer to other jeepneys on the way Because of the route, We need to get off and take another jeepney. This time there is no place for the passenger front seat. In fact, the rear compartment is really crowded, and Filipinos are generally not tall, so the height of the compartment is quite low. I really took the ride with my head down all the wayüòÜ, and it was cool to watch the traffic behind while sitting. If there were more people, some people would even hang from the upper pole directly by hand, which is actually quite dangerous. On the way, I also met some Filipinos who were high school students who take rides and did not pay. Teenager in all the world are really same.üòé Arrive at Anjo World Theme Park After a lot of hard work, I finally arrived at Anjo World Theme Park (in fact, I just followed my friends üòé) The type of ticket we bought is Super Tres (Limited to Three Rides), which is cheaper but can only choose three rides, and the ticket price is Php 250. This kind of ticket price may be to attract groups with lower salaries or students. After entering the venue, you can clearly see the guidance of the amusement facilities, unlike Taiwan\u0026rsquo;s amusement parks, which are really big enough to get lost. Because we only have three facilities to take, so we are not in a hurry, anyway, just walk around and eat something in the park. Like other amusement parks, many buildings are suitable for taking pictures. In the end, we chose to take the free fall, carousel and Ferris wheel. The atmosphere in the amusement park at night was better than in the morning, and there were more people. The view from the Ferris wheel. Night view of the entire amusement park. Conclusion It is actually very difficult to experience this kind of itinerary when you just be in Cebu to travel. After all, you will worry about traffic safety on the road, or prefer save time and go directly to the chartered driving route. I am very grateful that someone can lead this journey, and go deeper into the transportation methods and life of the local people. Experienced memories that are hard to experience. Only found this funny sign when I on the way home.üòé ",
	  "pubDate": "2022-12-02T16:28:53+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/012IMG_1129.jpeg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/simala-church-day-trip/",
	  "title": "Cebu-Simala Church Day Trip",
	  "summary": "\u003cp\u003eI was fortunate enough to visit the Simala Church, the largest church in Cebu. This article mainly records the itinerary planning at that time, the mass participation and the majestic architecture.\u003c/p\u003e",
	  "content": "I was fortunate enough to visit the Simala Church, the largest church in Cebu. This article mainly records the itinerary planning at that time, the mass participation and the majestic architecture.\nDate : 2019.12.15\nForeword When you come to Cebu, you must have heard of the famous Simala Church, which is actually a monastery known for its castle-like church. Located on a hill in the village of Lindogon in Sibonga. However, this area is far away from the downtown area of Cebu, it is usually not very popular for foreigners. After all, there are many attractions in Cebu, so most of the people who come here are local Filipinos who hold the Catholic faith. Because I\u0026rsquo;m a atheist, Filipino friend invited me to experience this majestic church and Catholic culture together.\nCebu City‚ûùSimala Church Transportation Options If you have sufficient funds, I recommended to charter a car directly(private car), which is not only convenient and fast, but also can reach the destination directly. Because my friend is locals, and I prefered to take an in-depth tour, so I choose to take the bus.\nBy taxi (Grab): Using Grab app to book a car, it will cost about PHP 2,000 directly from downtown Cebu to Simala Church (depending on the time, you may not be able to get a car on the return journey).\nBy private vehicle: In fact, this is what I recommend the most. The method is to buy the itinerary directly on major travel platforms, and hire a driver to guide directly from the designated point to the Simala Church. If you meet a good driver, you can chat with him and experience different cultures and customs. After the end, there is no need to rush or worry about not being able to call a car too late.\nBy bus: The most cost-effective itinerary is to experience taking a bus with the locals (very crowded), because the place where the bus arrives is still some distance from the church, so you still need to take a habal habal motorcycle to get there.\nBus + habal habal(motorcycle) detailed information I chose this way, and the following is the detailed record at that time.\nYou need to go to Cebu South Bus Terminal to wait for the bus to the church. The destination of the bus is towards Oslob or Liloan, and you need to get off at Simala halfway. The fare is PHP 100 and the travel time takes about two hours. (It is recommended that whether you are at the station or on the bus, you can directly tell the station administrator or the driver that you are going to Simala Church, ask where to wait for the bus and ask the driver to remind you when to get off.) Ticket stub, there will be a ticket cutter in charge of punching the ticket on the way. In fact, I don‚Äôt understand the rules of punching.üòÜ After arriving, because there is still a distance from the church, it is recommended to take a habal-habal (motorcycle) directly to the church. When I arrived, there were quite a lot of motorcycle drivers waiting on the road. The price varies from person to person, about PHP 20 per person , a motorcycle can carry two people.\nMy friend negotiate the round-trip price with the driver. The advantage is that the price may be lower and the return trip is not afraid of not being able to find the driver. They use WhatsApp to communicate with each other. However, if you want to do this, you must consider your English level, because the countryside in Cebu. People have strong accents, so you may not be able to communicate with them.(or maybe it just my problemüòÇ) On the way to take the habal-habal, you can experience the primitive countryside at close range, and you can see a lot of sheep on the way. I was really scared and forgot to pick up the camera to record all of this because I didn\u0026rsquo;t wear a helmet at that time.üòÇ\nSimala Church After two hours of driving, I finally arrived, and I could see the church on the hill in the distance. After approaching, the first impression is that it is a very long churchüòÇ, unlike the tall churches in Europe, the church in the Philippines are very long. Could it be because this side is also near the earthquake plate, so if you want to build a majestic building, you can\u0026rsquo;t build it on the top, you have to build it on both sides? After more than two hours of driving, the first thing I did when I arrived at the church was to go to the toilet. sleeping cat next to toiletü•¥ Frescoes on the ceiling. Then I participated in the ongoing mass. This was the first time I participated in this kind of religious activity. I felt that everyone who came to participate was very pious. The atmosphere in the church is very solemn. And everyone is using Cebuano during the process, I can say that I can\u0026rsquo;t understand it at all, so I just follow what everyone does.üòé Mass schedule Monday ‚ûù Friday: 12:00\nSaturday ‚ûù Sunday: 12:00„ÄÅ15:00\nAfter the mass, you can visit the second floor of the church. Take off shoes are required to enter, and no entrance fee. In addition to displaying various types of the Virgin Mary, there are also some historical relics. It also provides believers to worship and fulfill their vows. Many people come here to petition. Many people use small notes to write their wishes, throw them into the petition box and pray. Being a non-religious person is actually a little awkward being here, but I try my best to fit in. After the end, continue to visit this gorgeous church around the periphery Most of the tourists I saw that day were Filipinos, and there were no foreign tourists. Maybe I was the only Taiwanese. When I came here, I found out that the original English name of the ËÅñÊØçÁë™Âà©‰∫û is MaMa Mary. The Taiwanese translation made me a little confused.üòÇ Back to Cebu City Because it is really a very rich itinerary, we stayed quite long near the church, and it was too late to return to the area waiting for the bus. In addition, there were a bunch of people waiting for the bus, and the bus shift interval was long and there was no obvious shift schedule. At that time, I was really afraid that I might miss the last train.üòÇ\nThe waiting area for the bus is really primitive. Fortunately, We got on the car in the end, but we had no seat and need to standing on the aisle for the return journey. Conclusion I am very happy to have the opportunity to come here. I even did not dare to try to travel alone with my language level at the time. Gorgeous and solemn buildings, rural scenery on the way, this tour is really very pleasant, and it allows me to experience different religions and cultures. Although I studied in a Catholic high school since I was a child, my family has no relevant religiou, I don\u0026rsquo;t have an in-depth understanding about Catholic, and I have never participated in various activities of the Catholic Church. It was my first time to attend mass in Cebu, and I felt very honored.\n",
	  "pubDate": "2022-12-01T16:28:53+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/013IMG_0563.jpeg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/christmas-in-cebu/",
	  "title": "Christmas in Cebu",
	  "summary": "\u003cp\u003eAfter December, the number of new students in language schools generally decreases, because most students need to go back to their hometown for New Year‚Äôs Eve, or come after the Lunar New Year. I am very happy to experience the local festive atmosphere during this time, but sometimes I still feel a little bit sad.üòÇ\u003c/p\u003e",
	  "content": "After December, the number of new students in language schools generally decreases, because most students need to go back to their hometown for New Year‚Äôs Eve, or come after the Lunar New Year. I am very happy to experience the local festive atmosphere during this time, but sometimes I still feel a little bit sad.üòÇ\nDate : 2019.12.25\nForeword I came to Cebu in mid-November, and plan to stay in Cebu for half a year, so I will definitely experience Christmas and New Year in Cebu. In fact, my life has not changed much. I just stay on campus and continue to improve my English on weekdays and only left the campus on the weekend, mainly recording the atmosphere I felt in Cebu.\nChristmas In Cebu, the atmosphere of Christmas is very different with Taiwan. In Taiwan, Christmas is a day that is hyped by businessmen and must prepare gifts and exchange with friends. Filipinos focus on reuniting with their families on this day. Basically, it is similar to our Lunar New Year, so people who are alone overseas at this time will feel inexplicably lonely, because friends have gone back to reunite with their families. However, although the purpose is different, the decorations that major shopping centers should have are also indispensable. The interior decoration of Ayala malls is also quite atmospheric, which is very suitable for a walk and tryst. I can feel it even when I order a delivery. This driver was the first person to say Merry Christmas to me.üòé When exchanging currency, some people even wear Christmas hats directly.üòÜ\nNew Year‚Äôs Eve After Christmas, the next are New Year\u0026rsquo;s Eve and Lunar New Year. In Cebu New Year\u0026rsquo;s Eve, there are fireworks activities like other areas. This year, the location where the fireworks are set off is located in Ayala malls. I went to dinner with my friends that night. There are some restaurant hsa no seats, next time we must make a reservation in advance.\nAfter that, we found a restaurant temporarily, and went in to eat when there was a seat. After the dinner, my friend suggested to drink some wine.\nSo we chose a French wine bar:La Vie Parisienne. This store has beautifully decorated, suitable for taking pictures or dating couples, but I didn‚Äôt take any pictures at that momentüòÜ Store information\nLa Vie Parisienne\nAddressÔºö371 Gorordo Avenue, Lahug, Cebu City, 6000 Cebu\nAfter the dinner, because I still have a curfew, and I didn\u0026rsquo;t want to spend the night outside, so I had to go back to the dormitory. There were actually some activities in the campus at that time, and most of the students gathered in the social area to chat. But I still wanted to focus on my studies and try to keep my schedule as normal as possible, I planned to stay in the dormitory and study until 12 o\u0026rsquo;clock before going to bed.\nWatching the New Year\u0026rsquo;s Eve fireworks alone in the dormitory is really a bit sentimental XD.\nConclusion It was the first time to experience Christmas, New Year\u0026rsquo;s Eve and Lunar New Year in a foreign country. Because of the local security and school curfew, I didn\u0026rsquo;t have a deep experience of the local people\u0026rsquo;s way of celebrating the festival. However, I still experienced the joy of celebrating with friends of different nationalities. Although I had to return to the dormitory on time in the end, it was a very different Christmas and New Year\u0026rsquo;s Eve.\n",
	  "pubDate": "2022-11-30T19:28:53+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/011IMG_0714.jpeg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/daily-life-part2/",
	  "title": "Study tour in Cebu - Daily life Part 2",
	  "summary": "\u003cp\u003eJsut want to record the daily life of the language school and the environment of city when I lived in Cebu. My purpose at that time was increase english skills for interview, So there is no itinerary such as island hopping or nightlife.\u003c/p\u003e",
	  "content": "Jsut want to record the daily life of the language school and the environment of city when I lived in Cebu. My purpose at that time was increase english skills for interview, So there is no itinerary such as island hopping or nightlife.\nDate : 2019.11.17‚ûù2020.03.21\nForeword I really didn\u0026rsquo;t experience various travel itineraries as much as possible when I studied in Cebu because of various factors such as financial and self-expectation. However I don‚Äôt regret it, because no matter what I choose, it is also an experience.üòé\nEntertainment Cebu is the second largest city in the Philippines, so there are actually quite a variety of activities that can be experienced. Even there are some activities that it is my first time to heard.üòÇ Students can also participate a travel group by the school or directly find a local travel agency. It\u0026rsquo;s quite convenient. There are quite a lot of one-day tour groups in Cebu, and you can hire a tour guide to take you directly for a whole day.\nBecause of my limited funds and a week of courses from morning to night, I always take the simple route or sometimes just wants to take a rest and at most go out for a meal and a drink.\nFull day of classes will really make you so tired that you will run back to the dormitory to sleep when you are free. First time to cinema in cebu This entertainment is that I always did in Taiwan and I often go to the movies by myself. Of course I must try it abroad. The audio-visual equipment in Cebu is not as good as in Taiwan and not subtitles on movie, so I need to train my listening first. Movie theaters in different malls will also have differences between old and new. It is quite a popular entertainment option for Filipinos!\nThere are so many people in the cinema here, it\u0026rsquo;s same as Taiwan. I think Robinsons Galleria Cebu cinema is newer than other. Watching a movie with my Filipino friends, it was the first time I saw someone who could eat fried chicken with a knife and fork very fluently. ü§£ Commemorate the first movie (Jumanji) I saw in the Philippines, the tickets are super cheap. If my listening is better, I will really see a movie more. XD What is very cultural shock is that watching movies in Taiwan basically does not make loud voices except for children. But in the Philippines, no matter how elder or younger, there will be all kinds of voice. My Filipino friends beside me also laughed at every funny scenes, It can be seen that the Filipinos have unrestrained souls from the heart.\nThe whole movie theater is very lively, I really curious to know what situation when watching a ghost movie.üòÇ\nRestaurants When you come to Cebu, you must try Filipino cuisine. The restaurants like Choobi choobi are so delicious. As for the cuisines of various countries, the Korean ethnic group accounts for a large proportion in Cebu, so you can see Korean barbecue restaurants everywhere. I already loved Korean cuisine before. After coming here, I realized that the Korean restaurants in Taiwan are a bit more Taiwanese-style. But the Korean-style barbecue in the Philippines is really authentic. The reason may be that basically most of the store owners are Koreans here!!\nMy most recommended Korean BBQ restaurant üëç, the name of the restaurant at that time was Su lat gan, for some reason it only has Korean name now.\nIf you really don\u0026rsquo;t know which restaurant is authentic, the best way is to ask Korean students directly.\nI don\u0026rsquo;t know why Koreans are so hyped. I also learned a lot of interesting ways of drinking.üòé\nIn short, what kind of restaurant is authentic, the fastest way to know is ask the students in that country.\nAs for Taiwan, there are quite a lot, because I only prefer Taiwan hot pot and wanted to concentrate on my studies, I tried to reduse go out that could be avoided as much as possible. But I still come here to miss the taste of my hometown. There is nothing to say about the taste, it is exactly the same as the small hot pot restaurant near my home.üò±\nRestaurant name:Red House Taiwan Sha-bu Sha-bu You can also order this kind of thing that doesn\u0026rsquo;t seem to appear in hot pot restaurants in Taiwan.üòÇ\nIT Park IT Park seems to be the commercial center of Cebu, and there are many job opportunities here. But I heard from the locals that most of them are customer service jobs. Customers are from other countries, so working hours are usually at night or in shifts. There are many bars and KTVs in here, the more night get more beauty!?\nBut I have curfew, I don\u0026rsquo;t really want to spend the night outside, so I go back to school early.üòÇ\nSugbo Mercado night market You can experience the Sugbo Mercado Night Market near IT Park. It is similar to night markets in Taiwan, of course the food is different. I think the overall environment is very clean.\nWhile eating, there were also people singing and performing on the front stage, but they all spoke Cebuano, I couldn‚Äôt understand it. What is very different is that Filipinos are so excited that they will actually go up and sing after being invited.üòé\n",
	  "pubDate": "2022-11-30T17:28:53+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/010.IMG_1727.jpeg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/daily-life-part1/",
	  "title": "Study tour in Cebu - Daily life Part 1",
	  "summary": "\u003cp\u003eJsut want to record the daily life of the language school and the environment of city when I lived in Cebu. My purpose at that time was increase english skills for interview, So there is no itinerary such as island hopping or nightlife.\u003c/p\u003e",
	  "content": "Jsut want to record the daily life of the language school and the environment of city when I lived in Cebu. My purpose at that time was increase english skills for interview, So there is no itinerary such as island hopping or nightlife.\nDate : 2019.11.17‚ûù2020.03.21\nForeword It has been a week since I came to Cebu. I lived in Sparta campus, so the school only permitted students to go out on the weekend. Sparta campus has strict curfew, minors who want to stay outside overnight must get confirmation from their parents before and they will be allowed. Before I came to Cebu, I had already heard about it from the agency, At that time, I thought the Philippines was quite dangerous as well , so I didn‚Äôt have any plans to go out of school.\nFirst time to go around outside Thinking back at that time, I was still a little afraid of going out of school, Moreover, I was a person who prefer stay at home. Fortunately, I met a good roommate who is a Korean, about 30 years old, who went on working holiday in Australia before. Because his goal is immigration, he came to Cebu with his girlfriend to study English. He came about a week earlier than me, and when I asked him about the environment outside the school, he graciously invited me to go out for a stroll.\nTaiwan bubble milk tea This is my first spot in Cebu. Formosa Taiwan Milk Tea Shop Because my roommate was focus on study so that didn\u0026rsquo;t go out very often, so this time we just took a walk within a walking distance. In fact, if we are same nationality, we had nothing to discuss about the name of this store. It really took me a long time to explain to this Korean couple what is relationship between Formosa and Taiwan.üòÇ The feeling of the whole store is a bit similar to the internet-famous shop in Taiwan. The decoration is gorgeous. As for the taste,the overall has not yet reached my standard. No matter what beverage shop in Taiwan, I always order bubble milk tea, it make me to be called the king of milk tea, I full the confidence of this.üòé However, if you drink a beverage with the shop name is about Taiwan on abroad, even if it is a fake, I will give it 100 score.üòÜ\nMassage The next step is to experience massage that you must experience when you come to Cebu. In fact, there are many massage massage shop in the whole urban area, and even a large shopping mall has an area that directly provides massage\u0026hellip; I had never been to a massage shop in Taiwan, after all, the consumption is really not cheap for me. But after seeing this price, I often came to here by myself when I felt tired. (just search casually on Google Maps, go in and try it outüòÇ) You can basically see Chinese, Korean and Japanese menus in Cebu, because the salary of the locals is very low, Most of the entertainment is provided to foreigners, and even KTV has Japanese, Korean and Chinese songs.\nThis is the itinerary of going out for the first time above. I am very grateful to my roommate for giving me this opportunity, which makes me always want to go outside later.üòé\nSchool life The environment in the language school is really rich, there is a swimming pool, a canteen, a sales department, a laundry department. In addition to the self-study room, there are also a lot of places suitable for sitting and chatting on campus. After all, language schools need an environment for students from different countries to practice speaking.\nYou can eat and read at the outdoor table, accompanied by cat~ If the school food is too unpalatable, many students decide to order delivery directly.\nJollibee, which is only available in the Philippines, I think is delicious and cheap, basically every set meal you can a pack of rice.üòÇ Basically, the school has provided all the necessities of life, and every week there will be a cleaner to help change the sheets and clean the dormitories.\nIt\u0026rsquo;s really possible to do not leave school at all.\nBecause we always ask her to clean our dormitory at the same time every week, and we become very familiar in the end.üòé Purchasing of necessary items Basically, at the beginning of the school, new students will be guide to Ayala Mall to exchange currency or buy some daily necessities. The days after you have to rely on yourself~! At first, I didn\u0026rsquo;t like going to Ayala Mall because it was so big that I often got lost.\nPlease ignore the map. I gave up knowing the way. Anyway, after walking for a long time, I will know how to get there.üòé\nWhen I got familiar with Ayala Mall, I often come here to eat dessert and drink afternoon tea~ Here I want to explain that there are guards at the entrances of various shopping malls in Cebu, and even individual stores in shopping malls. But usually it‚Äôs just doing things like opening the door for you. I really curious if the reason is that the public safety is really bad, or just the labor wages are too cheap. It is also very common to find that there are 3 or 4 employees in a storefront that is clearly empty, which is simply impossible in Taiwan.\nApart from Ayala Mall, there are actually quite a few shopping centers in downtown Cebu. If I usually just buy some snacks, I prefer a small mall that is closer to the school. (Because I usually walk directly from campus to Ayala Mall, I just like the feeling of walking on the street abroad XD) In some small shopping malls, the counter staff will take the initiative to pack the purchased things for you. I have never been treated like this in TaiwanüòÜ SM mall, distinctive staff uniforms Cut hair Most of the students I meet at the language school attend short-term course, most of which are two-three weeks or two months at most. But I signed up for the six-month course üòÇ because of the agency‚Äôs advice. Basically, it was difficult to find someone stay longer than me at the end hahaha. Also because I have to stay here for a long time, I have to cut my hair here.\nThere are quite a variety of barber shops in Cebu, because there are generally more Koreans, there are really quite a lot of Korean hairsty barber. I really don\u0026rsquo;t dare to try the local ones because the price are too cheap, moreover I prefer Korean hairstyles, so I always look for Korean stores.\nThis is my favorite barber shop. The owner is a Korean middle-aged woman, and other staffs who wash hair are all Filipinos. I think her skill is excellent. I had an experience that there is a Filipino asks me if I was Korean when I was walking on the road.ü§®\nThe point is the price is only NT$200!!! I have tried other barber shops that are more expensive and have full the decoration, but the skill is poor. One time after being cut to bad sitution, I went back and asked my Korean roommate what was going on, and he only told me that maybe hair barber might be not care. Because this kind of barber specially cut international students who only come once. After all, there are really many short-term students will choose them because don\u0026rsquo;t know this barber bad repution.\n",
	  "pubDate": "2022-11-30T16:28:53+08:00",
	  "cover": "https://sunnote.xyz/img/thumbnail/009.IMG_0720.jpeg"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/cebu/pre-departure-mood-record/",
	  "title": "Study tour in Cebu - pre-departure mood record",
	  "summary": "",
	  "content": "Date : 2019.11.17\nForeword Recently, I looked back at the photos and found that there was no record left.\nAfter all, I signed up for the famous Spartan course at that time, I knew that what awaited me was cram school hell,\nIn addition, the agency also mentioned factors such as public security, air pollution and really tiring courses in the Philippines. At that time, I felt depressed to the journey of Cebu.\nPrepare in Taoyuan Airport In order to get cheap air tickets, I will sleep at the airport the day before and take a flight that departs early in the morning.\nArrive at Mactan-Cebu International Airport My flight landed at the second terminal, and the first impression I felt about envirement was actually quite good, although the second terminal is small but quite new and very modern in appearance,\nCheck the info that I knew the second terminal was opened on July 1, 2018.\nIt obvious that it is newly built haha.\nThe next step is to go through the customs and pick up the luggage.\nBecause there is a certificate of language school attached visa,\nThe customs didn\u0026rsquo;t ask anything at all (maybe they also know that my English is not good and I just want to learn English) and let me go.\nMactan-Cebu International Airport ‚ûù Language School After leaving the airport, I have to find driver of language school who is responsible for picking up the students, usually holding a sign with the name of the language school you belong to. After successfully finding him, I got into his car and went straight to the language school.\nThe journey to the language school actually takes about 20-30 minutes,\nbecause Mactan Airport is actually located in Lapu-Lapu City.\nThe position of the plane as shown in the picture below is actually located on the right side of the entire Cebu Island. Language school is in the main island of Cebu\nOn the bridge to the main island of Cebu. I really didn‚Äôt leave any photos on the way, I only had one thought in mind,\nOh my god, why did the city look quite old after leaving the airport?üòÇ\nArrive at Language School It was Sunday when I arrived, so there were not many students in the school,\nIn fact, the school environment is not bad, there is a swimming pool, and many outdoor social places\nThere is quite a vacation atmosphere, and it was only later that I realized that many people came here to learn English while at vacation.\nThe next step is to register.\nThe school guards showed me around and get to know the campus. The guards are Filipinos. They are quite amiable.\nI really don\u0026rsquo;t know what he\u0026rsquo;s talking about XD, because of my poor listening at that time and unfamiliar with the Filipino accent.\nAfter the visit, I can go to my dormitory. Three people share a room, which is not bad for someone like me who has just finished compulsory military service.üòé\nFortunately, The roommates I met were very nice. I still remember when I first time entering the room.\nI just met my Korean roommate who was taking rest. After that, I could only squeeze vocabulary and grammar out of my head as much as possible.\nBoth of us are not good at English, but environment forced us to communicate with each other, it is really amazing that we can unexpectedly understand each other.\nThat night, I swear that I really spoke more English in one day than I have spoken in Taiwan for more than 20 years\u0026hellip;\nOf course, the following courses will let me speak more English then that night.\nLook at this full schedule, It\u0026rsquo;s tired than high school students. The street view near the language school is attached, it seems that it was raised by the neighbors nearby.\nI grow up in Taiwan, really think it\u0026rsquo;s so cool.\n",
	  "pubDate": "2022-11-29T16:28:53+08:00",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/about/",
	  "title": "About",
	  "summary": "About",
	  "content": "\rThis photo was taken in Hungary in 2018.\rAlvin, currently a software engineer responsible for backend system development.\nTechnologies: ASP.NET Core, Blazor, Node.js, Python, Azure, AWS, Docker\nHope that one day in the future, I can achieve the state where work is solely for a sense of accomplishment rather than just for survival.\nThis site mainly shares life experiences, travel tips, and a bit of technical notes.\nIf you have any questions, same interests, or want to exchange technical information, please feel free to contact me.\nI would be grateful for any contributions you might be able to make.\rYour support lets me know that these posts are helpful to someone in the world, and I really appreciate you letting me\rknow that.\rThere are several ways to support me.üí™\r‚ñ∂\rCryptocurrency\rNetwork:\rPolygon-usdt\rBSC-usdt\rArbitrum-usdt\rETH-usdt\r‚ñ∂\rJKOPAY\r‚ñ∂\rBuy Me a Coffee\r2023/12/20...\n",
	  "pubDate": "2022-11-20T00:00:00Z",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/archives/",
	  "title": "Archives",
	  "summary": "",
	  "content": "",
	  "pubDate": "2022-11-20T00:00:00Z",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/search/",
	  "title": "Site Search",
	  "summary": "search.",
	  "content": "",
	  "pubDate": "2022-11-20T00:00:00Z",
	  "cover": "https://sunnote.xyz"

	}, 
	{
	
		
	  "url": "http://sunnote.xyz/timeline/",
	  "title": "Timeline",
	  "summary": "timeline",
	  "content": " Backend Engineer (Formosa Plastics Corporation)\rDeveloped password authenticate and storage function (Bcrypt), automatic account distribution and worked with engineers to build a login system for electronic commerce.\nDeveloped database field information systemization module, automatically obtains data table information and updates the front-end display, increasing work efficiency. Developed automated inspection system for testing and formal database, increasing system stability.\nDeveloped notification system for electronic commerce. (internal group notification service)\nDeveloped automated script test API for electronic commerce.\nDeveloped material manage system for the factory, including automation-estimated inventory, suggested supplementary standards and notification service.\n2020-09-14~\nField Application Engineer (Micro Star International co. ltd)\r1 month Field Application Engineer in Micro Star International co. ltd\n2020-07-01~\n2020-08-26\r",
	  "pubDate": "2022-11-20T00:00:00Z",
	  "cover": "https://sunnote.xyz"

	}
  ]
  